{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "koml6g7p7y9e"
      },
      "outputs": [],
      "source": [
        "import yfinance as yf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM,Dense\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pk2U_Pi98s1w",
        "outputId": "9604a6d8-d33a-4c0b-dcc7-d59c1936db4e"
      },
      "outputs": [],
      "source": [
        "# Define the SP500 tickers and sector index ticker\n",
        "sp500_tickers = ['AAPL']  # Example tickers\n",
        "sector_index_ticker = 'XLC'  # Example sector index ticker for Technology\n",
        "\n",
        "# Fetch historical data for these tickers and the sector index\n",
        "def fetch_data(tickers, start_date, end_date):\n",
        "    data = {}\n",
        "    for ticker in tickers:\n",
        "        data[ticker] = yf.download(ticker, start=start_date, end=end_date)\n",
        "    return data\n",
        "\n",
        "data = fetch_data(sp500_tickers, '2012-08-12', '2022-08-12')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5VdMFj_h81I7",
        "outputId": "68ea2e00-b098-4694-8adc-cbc127860547"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Fetch the sector index data\n",
        "sector_index_data = yf.download(sector_index_ticker, start='2004-08-12', end='2024-08-12')\n",
        "\n",
        "def fetch_company_info(ticker):\n",
        "    company = yf.Ticker(ticker)\n",
        "    return company.info\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "qhCefEY48155"
      },
      "outputs": [],
      "source": [
        "def calculate_metrics(info):\n",
        "    metrics = {\n",
        "        'P/E Ratio': info.get('forwardEps') / info.get('previousClose') if info.get('forwardEps') and info.get('previousClose') else None,\n",
        "        # 'EPS': info.get('earningsPerShare'),\n",
        "        'P/B Ratio': info.get('priceToBook'),\n",
        "        'Dividend Yield': info.get('dividendYield'),\n",
        "        'Dividend Payout Ratio': info.get('payoutRatio'),\n",
        "        'ROE': info.get('returnOnEquity'),\n",
        "        'ROA': info.get('returnOnAssets'),\n",
        "        # 'ROI': info.get('returnOnInvestment'),\n",
        "        'Beta': info.get('beta'),\n",
        "        'Market Capitalization': info.get('marketCap'),\n",
        "        'Revenue Growth': info.get('revenueGrowth'),\n",
        "        # 'Net Profit Margin': info.get('netProfitMargin'),\n",
        "        # 'Gross Profit Margin': info.get('grossProfitMargin'),\n",
        "        # 'Operating Margin': info.get('operatingMargin'),\n",
        "        'Debt-to-Equity Ratio': info.get('debtToEquity'),\n",
        "        'Free Cash Flow': info.get('freeCashflow'),\n",
        "        'Current Ratio': info.get('currentRatio'),\n",
        "        'Quick Ratio': info.get('quickRatio'),\n",
        "        # 'Interest Coverage Ratio': info.get('interestCoverageRatio'),\n",
        "        # 'P/S Ratio': info.get('priceToSales'),\n",
        "        'PEG Ratio': info.get('pegRatio'),\n",
        "        # Additional metrics\n",
        "        # 'Alpha': None,\n",
        "        'Standard Deviation': None,\n",
        "        'Value at Risk (VaR)': None,\n",
        "        'Sharpe Ratio': None,\n",
        "        'Sortino Ratio': None,\n",
        "        'Maximum Drawdown': None,\n",
        "        'Downside Deviation': None,\n",
        "        'Tracking Error': None,\n",
        "        'R-squared': None,\n",
        "        'Treynor Ratio': None,\n",
        "        'Information Ratio': None,\n",
        "        'Conditional Value at Risk (CVaR)': None,\n",
        "        'Beta-adjusted Sharpe Ratio': None,\n",
        "        'Drawdown Duration': None,\n",
        "        'Ulcer Index': None,\n",
        "        'Jensenâ€™s Alpha': None\n",
        "    }\n",
        "    return metrics\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "AIWGRrcF9C0M"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Ensure company_data is defined\n",
        "company_data = data.get('AAPL')\n",
        "if company_data is None:\n",
        "    raise ValueError(\"Data for 'AAPL' not found in fetched data.\")\n",
        "\n",
        "# Ensure sector_index_data is defined\n",
        "if sector_index_data is None:\n",
        "    raise ValueError(f\"Data for sector index '{sector_index_ticker}' not found.\")\n",
        "\n",
        "company_info = fetch_company_info('AAPL')\n",
        "metrics = calculate_metrics(company_info)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Vet3_OQC9NrS"
      },
      "outputs": [],
      "source": [
        "\n",
        "def calculate_standard_deviation(price_data):\n",
        "    returns = price_data['Adj Close'].pct_change().dropna()\n",
        "    return returns.std()\n",
        "\n",
        "def calculate_var(price_data, confidence_level=0.95):\n",
        "    returns = price_data['Adj Close'].pct_change().dropna()\n",
        "    return returns.quantile(1 - confidence_level)\n",
        "\n",
        "def calculate_sharpe_ratio(price_data, risk_free_rate=0.01):\n",
        "    returns = price_data['Adj Close'].pct_change().dropna()\n",
        "    excess_returns = returns - risk_free_rate / 252\n",
        "    return excess_returns.mean() / excess_returns.std()\n",
        "\n",
        "def calculate_sortino_ratio(price_data, risk_free_rate=0.01):\n",
        "    returns = price_data['Adj Close'].pct_change().dropna()\n",
        "    excess_returns = returns - risk_free_rate / 252\n",
        "    downside_returns = excess_returns[excess_returns < 0]\n",
        "    return excess_returns.mean() / downside_returns.std()\n",
        "\n",
        "def calculate_max_drawdown(price_data):\n",
        "    rolling_max = price_data['Adj Close'].cummax()\n",
        "    drawdowns = (price_data['Adj Close'] - rolling_max) / rolling_max\n",
        "    return drawdowns.min()\n",
        "\n",
        "def calculate_downside_deviation(price_data, risk_free_rate=0.01):\n",
        "    returns = price_data['Adj Close'].pct_change().dropna()\n",
        "    downside_returns = returns[returns < risk_free_rate / 252]\n",
        "    return downside_returns.std()\n",
        "\n",
        "def calculate_tracking_error(company_data, index_data):\n",
        "    # Ensure both datasets have the same dates\n",
        "    common_dates = company_data.index.intersection(index_data.index)\n",
        "    company_returns = company_data.loc[common_dates, 'Adj Close'].pct_change().dropna()\n",
        "    index_returns = index_data.loc[common_dates, 'Adj Close'].pct_change().dropna()\n",
        "\n",
        "    # Check if there are any overlapping dates\n",
        "    if len(company_returns) == 0 or len(index_returns) == 0:\n",
        "        raise ValueError(\"No overlapping data points found.\")\n",
        "\n",
        "    # Ensure both series are aligned\n",
        "    aligned_data = pd.concat([company_returns, index_returns], axis=1).dropna()\n",
        "    company_returns_aligned = aligned_data.iloc[:, 0]\n",
        "    index_returns_aligned = aligned_data.iloc[:, 1]\n",
        "\n",
        "    # Compute Tracking Error\n",
        "    excess_returns = company_returns_aligned - index_returns_aligned\n",
        "    return excess_returns.std()\n",
        "\n",
        "def calculate_r_squared(company_data, index_data):\n",
        "    # Ensure both datasets have the same dates\n",
        "    common_dates = company_data.index.intersection(index_data.index)\n",
        "    company_returns = company_data.loc[common_dates, 'Adj Close'].pct_change().dropna()\n",
        "    index_returns = index_data.loc[common_dates, 'Adj Close'].pct_change().dropna()\n",
        "\n",
        "    # Check if there are any overlapping dates\n",
        "    if len(company_returns) == 0 or len(index_returns) == 0:\n",
        "        raise ValueError(\"No overlapping data points found.\")\n",
        "\n",
        "    # Ensure both series are aligned\n",
        "    aligned_data = pd.concat([company_returns, index_returns], axis=1).dropna()\n",
        "    company_returns_aligned = aligned_data.iloc[:, 0]\n",
        "    index_returns_aligned = aligned_data.iloc[:, 1]\n",
        "\n",
        "    # Fit the linear regression model\n",
        "    model = LinearRegression().fit(index_returns_aligned.values.reshape(-1, 1), company_returns_aligned.values)\n",
        "    return model.score(index_returns_aligned.values.reshape(-1, 1), company_returns_aligned.values)\n",
        "\n",
        "def calculate_treynor_ratio(price_data, risk_free_rate=0.01, beta=None):\n",
        "    returns = price_data['Adj Close'].pct_change().dropna()\n",
        "    excess_returns = returns.mean() - risk_free_rate / 252\n",
        "    return excess_returns / beta if beta else None\n",
        "\n",
        "def calculate_information_ratio(price_data, index_data):\n",
        "    # Ensure both datasets have the same dates\n",
        "    common_dates = price_data.index.intersection(index_data.index)\n",
        "    company_returns = price_data.loc[common_dates, 'Adj Close'].pct_change().dropna()\n",
        "    index_returns = index_data.loc[common_dates, 'Adj Close'].pct_change().dropna()\n",
        "\n",
        "    # Check if there are any overlapping dates\n",
        "    if len(company_returns) == 0 or len(index_returns) == 0:\n",
        "        raise ValueError(\"No overlapping data points found.\")\n",
        "\n",
        "    # Ensure both series are aligned\n",
        "    aligned_data = pd.concat([company_returns, index_returns], axis=1).dropna()\n",
        "    company_returns_aligned = aligned_data.iloc[:, 0]\n",
        "    index_returns_aligned = aligned_data.iloc[:, 1]\n",
        "\n",
        "    # Compute Information Ratio\n",
        "    excess_returns = company_returns_aligned - index_returns_aligned\n",
        "    return excess_returns.mean() / excess_returns.std()\n",
        "\n",
        "def calculate_cvar(price_data, confidence_level=0.95):\n",
        "    returns = price_data['Adj Close'].pct_change().dropna()\n",
        "    var = returns.quantile(1 - confidence_level)\n",
        "    cvar = returns[returns <= var].mean()\n",
        "    return cvar\n",
        "\n",
        "def calculate_beta(company_data, index_data):\n",
        "    # Ensure both datasets have the same dates\n",
        "    common_dates = company_data.index.intersection(index_data.index)\n",
        "    company_returns = company_data.loc[common_dates, 'Adj Close'].pct_change().dropna()\n",
        "    index_returns = index_data.loc[common_dates, 'Adj Close'].pct_change().dropna()\n",
        "\n",
        "    # Check if there are any overlapping dates\n",
        "    if len(company_returns) == 0 or len(index_returns) == 0:\n",
        "        raise ValueError(\"No overlapping data points found.\")\n",
        "\n",
        "    # Ensure both series are aligned\n",
        "    aligned_data = pd.concat([company_returns, index_returns], axis=1).dropna()\n",
        "    company_returns_aligned = aligned_data.iloc[:, 0]\n",
        "    index_returns_aligned = aligned_data.iloc[:, 1]\n",
        "\n",
        "    # Calculate Beta\n",
        "    covariance_matrix = np.cov(company_returns_aligned, index_returns_aligned)\n",
        "    beta = covariance_matrix[0, 1] / covariance_matrix[1, 1]\n",
        "    return beta\n",
        "\n",
        "def calculate_beta_adjusted_sharpe_ratio(price_data, index_data, risk_free_rate=0.01):\n",
        "    beta = calculate_beta(price_data, index_data)\n",
        "    sharpe_ratio = calculate_sharpe_ratio(price_data, risk_free_rate)\n",
        "    return sharpe_ratio / beta if beta else None\n",
        "\n",
        "def calculate_drawdown_duration(price_data):\n",
        "    rolling_max = price_data['Adj Close'].cummax()\n",
        "    drawdowns = (price_data['Adj Close'] - rolling_max) / rolling_max\n",
        "    drawdown_duration = (drawdowns < 0).astype(int).groupby((drawdowns >= 0).astype(int).cumsum()).cumsum().max()\n",
        "    return drawdown_duration\n",
        "\n",
        "def calculate_ulcer_index(price_data):\n",
        "    rolling_max = price_data['Adj Close'].cummax()\n",
        "    drawdowns = (price_data['Adj Close'] - rolling_max) / rolling_max\n",
        "    return (drawdowns ** 2).mean() ** 0.5\n",
        "\n",
        "def calculate_jensens_alpha(price_data, index_data, risk_free_rate=0.01):\n",
        "    # Ensure both datasets have the same dates\n",
        "    common_dates = price_data.index.intersection(index_data.index)\n",
        "    company_returns = price_data.loc[common_dates, 'Adj Close'].pct_change().dropna()\n",
        "    index_returns = index_data.loc[common_dates, 'Adj Close'].pct_change().dropna()\n",
        "\n",
        "    # Check if there are any overlapping dates\n",
        "    if len(company_returns) == 0 or len(index_returns) == 0:\n",
        "        raise ValueError(\"No overlapping data points found.\")\n",
        "\n",
        "    # Ensure both series are aligned\n",
        "    aligned_data = pd.concat([company_returns, index_returns], axis=1).dropna()\n",
        "    company_returns_aligned = aligned_data.iloc[:, 0]\n",
        "    index_returns_aligned = aligned_data.iloc[:, 1]\n",
        "\n",
        "    # Calculate Jensenâ€™s Alpha\n",
        "    beta = calculate_beta(price_data, index_data)\n",
        "    expected_return = risk_free_rate / 252 + beta * (index_returns_aligned.mean() - risk_free_rate / 252)\n",
        "    alpha = company_returns_aligned.mean() - expected_return\n",
        "    return alpha"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "wRAMjVtG9SSV"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Update metrics with calculations that require the sector index data\n",
        "metrics.update({\n",
        "    'Standard Deviation': calculate_standard_deviation(company_data),\n",
        "    'Value at Risk (VaR)': calculate_var(company_data),\n",
        "    'Sharpe Ratio': calculate_sharpe_ratio(company_data),\n",
        "    'Sortino Ratio': calculate_sortino_ratio(company_data),\n",
        "    'Maximum Drawdown': calculate_max_drawdown(company_data),\n",
        "    'Downside Deviation': calculate_downside_deviation(company_data),\n",
        "    'Tracking Error': calculate_tracking_error(company_data, sector_index_data),\n",
        "    'R-squared': calculate_r_squared(company_data, sector_index_data),\n",
        "    'Treynor Ratio': calculate_treynor_ratio(company_data, beta=company_info.get('beta')),\n",
        "    'Information Ratio': calculate_information_ratio(company_data, sector_index_data),\n",
        "    'Conditional Value at Risk (CVaR)': calculate_cvar(company_data),\n",
        "    'Beta-adjusted Sharpe Ratio': calculate_beta_adjusted_sharpe_ratio(company_data, sector_index_data),\n",
        "    'Drawdown Duration': calculate_drawdown_duration(company_data),\n",
        "    'Ulcer Index': calculate_ulcer_index(company_data),\n",
        "    'Jensenâ€™s Alpha': calculate_jensens_alpha(company_data, sector_index_data)\n",
        "})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qqCpbVs88Xt4",
        "outputId": "12b2afa2-6ca7-4861-f23f-4c4f58cd23a8"
      },
      "outputs": [],
      "source": [
        "metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ONg-9WAYa8W-",
        "outputId": "b936afdf-bd5e-4893-bb74-7d09261a7114"
      },
      "outputs": [],
      "source": [
        "X = np.array(list(metrics.values())).reshape(1, -1)  # Reshape to be a single sample\n",
        "\n",
        "# Extract the latest adjusted close price\n",
        "latest_date = company_data.index[-1]\n",
        "latest_adjusted_close_price = company_data.loc[latest_date, 'Adj Close']\n",
        "y = np.array([latest_adjusted_close_price])\n",
        "\n",
        "X,y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zH5pfBwhY1FG",
        "outputId": "427bb588-5ccd-4037-dc01-8d47ebf2584d"
      },
      "outputs": [],
      "source": [
        "X.shape,y.shape\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uf4ES5LCbNz_",
        "outputId": "5602477a-74f0-4486-da78-aec4360b8b8f"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Initialize ANN model\n",
        "model = Sequential()\n",
        "\n",
        "# Input layer with 64 neurons\n",
        "model.add(Dense(64, input_dim=X.shape[1], activation='relu'))\n",
        "\n",
        "# Hidden layers with 32, 16, and 8 neurons\n",
        "model.add(Dense(32, activation='relu'))\n",
        "model.add(Dense(16, activation='relu'))\n",
        "model.add(Dense(8, activation='relu'))\n",
        "\n",
        "# Output layer with 1 neuron (for regression)\n",
        "model.add(Dense(1, activation='linear'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "\n",
        "# Train model with 20 epochs\n",
        "model.fit(X, y, epochs=20, verbose=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "14cwT_kE2CkJ",
        "outputId": "f692bd6e-d77a-4a56-990b-58af97baf507"
      },
      "outputs": [],
      "source": [
        "def calculate_metrics_for_dates(data, index_data):\n",
        "    dates = data.index\n",
        "    metrics_list = []\n",
        "    prices = []\n",
        "\n",
        "    for date in dates:\n",
        "        try:\n",
        "            # Extract data up to the current date\n",
        "            subset_data = data.loc[:date]\n",
        "            subset_index_data = index_data.loc[:date]\n",
        "\n",
        "            # Ensure there is data available for metrics calculations\n",
        "            if subset_data.empty or subset_index_data.empty:\n",
        "                continue\n",
        "\n",
        "            # Calculate metrics\n",
        "            metrics = {\n",
        "                'P/E Ratio': calculate_metrics(fetch_company_info('AAPL'))['P/E Ratio'],\n",
        "                'P/B Ratio': calculate_metrics(fetch_company_info('AAPL'))['P/B Ratio'],\n",
        "                'Dividend Yield': calculate_metrics(fetch_company_info('AAPL'))['Dividend Yield'],\n",
        "                'Dividend Payout Ratio': calculate_metrics(fetch_company_info('AAPL'))['Dividend Payout Ratio'],\n",
        "                'ROE': calculate_metrics(fetch_company_info('AAPL'))['ROE'],\n",
        "                'ROA': calculate_metrics(fetch_company_info('AAPL'))['ROA'],\n",
        "                'Beta': calculate_metrics(fetch_company_info('AAPL'))['Beta'],\n",
        "                'Market Capitalization': calculate_metrics(fetch_company_info('AAPL'))['Market Capitalization'],\n",
        "                'Revenue Growth': calculate_metrics(fetch_company_info('AAPL'))['Revenue Growth'],\n",
        "                'Debt-to-Equity Ratio': calculate_metrics(fetch_company_info('AAPL'))['Debt-to-Equity Ratio'],\n",
        "                'Free Cash Flow': calculate_metrics(fetch_company_info('AAPL'))['Free Cash Flow'],\n",
        "                'Current Ratio': calculate_metrics(fetch_company_info('AAPL'))['Current Ratio'],\n",
        "                'Quick Ratio': calculate_metrics(fetch_company_info('AAPL'))['Quick Ratio'],\n",
        "                'PEG Ratio': calculate_metrics(fetch_company_info('AAPL'))['PEG Ratio'],\n",
        "                'Standard Deviation': calculate_standard_deviation(subset_data),\n",
        "                'Value at Risk (VaR)': calculate_var(subset_data),\n",
        "                'Sharpe Ratio': calculate_sharpe_ratio(subset_data),\n",
        "                'Sortino Ratio': calculate_sortino_ratio(subset_data),\n",
        "                'Maximum Drawdown': calculate_max_drawdown(subset_data),\n",
        "                'Downside Deviation': calculate_downside_deviation(subset_data),\n",
        "                'Tracking Error': calculate_tracking_error(subset_data, subset_index_data),\n",
        "                'R-squared': calculate_r_squared(subset_data, subset_index_data),\n",
        "                'Treynor Ratio': calculate_treynor_ratio(subset_data, beta=calculate_metrics(fetch_company_info('AAPL'))['Beta']),\n",
        "                'Information Ratio': calculate_information_ratio(subset_data, subset_index_data),\n",
        "                'Conditional Value at Risk (CVaR)': calculate_cvar(subset_data),\n",
        "                'Beta-adjusted Sharpe Ratio': calculate_beta_adjusted_sharpe_ratio(subset_data, subset_index_data),\n",
        "                'Drawdown Duration': calculate_drawdown_duration(subset_data),\n",
        "                'Ulcer Index': calculate_ulcer_index(subset_data),\n",
        "                'Jensenâ€™s Alpha': calculate_jensens_alpha(subset_data, subset_index_data)\n",
        "            }\n",
        "\n",
        "            # Store metrics and price\n",
        "            metrics_list.append(metrics)\n",
        "            prices.append(subset_data.loc[date, 'Adj Close'])\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing date {date}: {e}\")\n",
        "\n",
        "    return pd.DataFrame(metrics_list), np.array(prices)\n",
        "\n",
        "# Prepare the data\n",
        "metrics_df, true_prices = calculate_metrics_for_dates(data['AAPL'], sector_index_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 458
        },
        "id": "Ey9zSvG8rME8",
        "outputId": "2b924a9a-5e9d-475c-d1ac-4a2ba8616e00"
      },
      "outputs": [],
      "source": [
        "# Prepare data for the ANN\n",
        "X = metrics_df.values\n",
        "predictions = model.predict(X)\n",
        "\n",
        "# Plot the results\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Ensure predictions and true_prices are aligned\n",
        "plt.figure(figsize=(14, 7))\n",
        "# plt.plot(data['AAPL'].index[-len(true_prices):], true_prices, label='True Prices', color='blue')\n",
        "plt.plot(data['AAPL'].index[-len(predictions):], predictions, label='Predicted Prices', color='red', linestyle='--')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Stock Price')\n",
        "plt.title('True vs Predicted Stock Prices')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 451
        },
        "id": "dEisu6-QUBI0",
        "outputId": "c3822efe-26f9-4fab-b1ad-1c8397ec649c"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Assuming 'company_data' is your DataFrame containing the stock data\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(company_data.index, company_data['Adj Close'])\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Adjusted Close Price')\n",
        "plt.title('Adjusted Close Price Over Time')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y4VPi3Tl8Uf0"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fDNl9_h9Mfx7",
        "outputId": "ecaff781-aa03-4b16-afd3-7002b12be0c8"
      },
      "outputs": [],
      "source": [
        "import yfinance as yf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "# Define the tickers and dates\n",
        "sp500_tickers = ['AAPL']\n",
        "sector_index_ticker = 'XLC'\n",
        "start_date = '2012-08-12'\n",
        "end_date = '2024-08-12'\n",
        "\n",
        "# Fetch historical data\n",
        "def fetch_data(tickers, start_date, end_date):\n",
        "    data = {}\n",
        "    for ticker in tickers:\n",
        "        data[ticker] = yf.download(ticker, start=start_date, end=end_date)\n",
        "    return data\n",
        "\n",
        "data = fetch_data(sp500_tickers, start_date, end_date)\n",
        "sector_index_data = yf.download(sector_index_ticker, start=start_date, end=end_date)\n",
        "\n",
        "# Calculate metrics\n",
        "def fetch_company_info(ticker):\n",
        "    company = yf.Ticker(ticker)\n",
        "    return company.info\n",
        "\n",
        "def calculate_metrics(info):\n",
        "    metrics = {\n",
        "        'P/E Ratio': info.get('forwardEps') / info.get('previousClose') if info.get('forwardEps') and info.get('previousClose') else None,\n",
        "        'P/B Ratio': info.get('priceToBook'),\n",
        "        'Dividend Yield': info.get('dividendYield'),\n",
        "        'Dividend Payout Ratio': info.get('payoutRatio'),\n",
        "        'ROE': info.get('returnOnEquity'),\n",
        "        'ROA': info.get('returnOnAssets'),\n",
        "        'Beta': info.get('beta'),\n",
        "        'Market Capitalization': info.get('marketCap'),\n",
        "        'Revenue Growth': info.get('revenueGrowth'),\n",
        "        'Debt-to-Equity Ratio': info.get('debtToEquity'),\n",
        "        'Free Cash Flow': info.get('freeCashflow'),\n",
        "        'Current Ratio': info.get('currentRatio'),\n",
        "        'Quick Ratio': info.get('quickRatio'),\n",
        "        'PEG Ratio': info.get('pegRatio'),\n",
        "        'Standard Deviation': None,\n",
        "        'Value at Risk (VaR)': None,\n",
        "        'Sharpe Ratio': None,\n",
        "        'Sortino Ratio': None,\n",
        "        'Maximum Drawdown': None,\n",
        "        'Downside Deviation': None,\n",
        "        'Tracking Error': None,\n",
        "        'R-squared': None,\n",
        "        'Treynor Ratio': None,\n",
        "        'Information Ratio': None,\n",
        "        'Conditional Value at Risk (CVaR)': None,\n",
        "        'Beta-adjusted Sharpe Ratio': None,\n",
        "        'Drawdown Duration': None,\n",
        "        'Ulcer Index': None,\n",
        "        'Jensenâ€™s Alpha': None\n",
        "    }\n",
        "    return metrics\n",
        "\n",
        "company_info = fetch_company_info('AAPL')\n",
        "metrics = calculate_metrics(company_info)\n",
        "\n",
        "# Define functions for calculations\n",
        "def calculate_standard_deviation(price_data):\n",
        "    returns = price_data['Adj Close'].pct_change().dropna()\n",
        "    return returns.std()\n",
        "\n",
        "def calculate_var(price_data, confidence_level=0.95):\n",
        "    returns = price_data['Adj Close'].pct_change().dropna()\n",
        "    return returns.quantile(1 - confidence_level)\n",
        "\n",
        "def calculate_sharpe_ratio(price_data, risk_free_rate=0.01):\n",
        "    returns = price_data['Adj Close'].pct_change().dropna()\n",
        "    excess_returns = returns - risk_free_rate / 252\n",
        "    return excess_returns.mean() / excess_returns.std()\n",
        "\n",
        "def calculate_sortino_ratio(price_data, risk_free_rate=0.01):\n",
        "    returns = price_data['Adj Close'].pct_change().dropna()\n",
        "    excess_returns = returns - risk_free_rate / 252\n",
        "    downside_returns = excess_returns[excess_returns < 0]\n",
        "    return excess_returns.mean() / downside_returns.std()\n",
        "\n",
        "def calculate_max_drawdown(price_data):\n",
        "    rolling_max = price_data['Adj Close'].cummax()\n",
        "    drawdowns = (price_data['Adj Close'] - rolling_max) / rolling_max\n",
        "    return drawdowns.min()\n",
        "\n",
        "def calculate_downside_deviation(price_data, risk_free_rate=0.01):\n",
        "    returns = price_data['Adj Close'].pct_change().dropna()\n",
        "    downside_returns = returns[returns < risk_free_rate / 252]\n",
        "    return downside_returns.std()\n",
        "\n",
        "def calculate_tracking_error(company_data, index_data):\n",
        "    common_dates = company_data.index.intersection(index_data.index)\n",
        "    company_returns = company_data.loc[common_dates, 'Adj Close'].pct_change().dropna()\n",
        "    index_returns = index_data.loc[common_dates, 'Adj Close'].pct_change().dropna()\n",
        "    if len(company_returns) == 0 or len(index_returns) == 0:\n",
        "        raise ValueError(\"No overlapping data points found.\")\n",
        "    aligned_data = pd.concat([company_returns, index_returns], axis=1).dropna()\n",
        "    company_returns_aligned = aligned_data.iloc[:, 0]\n",
        "    index_returns_aligned = aligned_data.iloc[:, 1]\n",
        "    excess_returns = company_returns_aligned - index_returns_aligned\n",
        "    return excess_returns.std()\n",
        "\n",
        "def calculate_r_squared(company_data, index_data):\n",
        "    common_dates = company_data.index.intersection(index_data.index)\n",
        "    company_returns = company_data.loc[common_dates, 'Adj Close'].pct_change().dropna()\n",
        "    index_returns = index_data.loc[common_dates, 'Adj Close'].pct_change().dropna()\n",
        "    if len(company_returns) == 0 or len(index_returns) == 0:\n",
        "        raise ValueError(\"No overlapping data points found.\")\n",
        "    aligned_data = pd.concat([company_returns, index_returns], axis=1).dropna()\n",
        "    company_returns_aligned = aligned_data.iloc[:, 0]\n",
        "    index_returns_aligned = aligned_data.iloc[:, 1]\n",
        "    model = LinearRegression().fit(index_returns_aligned.values.reshape(-1, 1), company_returns_aligned.values)\n",
        "    return model.score(index_returns_aligned.values.reshape(-1, 1), company_returns_aligned.values)\n",
        "\n",
        "def calculate_treynor_ratio(price_data, risk_free_rate=0.01, beta=None):\n",
        "    returns = price_data['Adj Close'].pct_change().dropna()\n",
        "    excess_returns = returns.mean() - risk_free_rate / 252\n",
        "    return excess_returns / beta if beta else None\n",
        "\n",
        "def calculate_information_ratio(price_data, index_data):\n",
        "    common_dates = price_data.index.intersection(index_data.index)\n",
        "    company_returns = price_data.loc[common_dates, 'Adj Close'].pct_change().dropna()\n",
        "    index_returns = index_data.loc[common_dates, 'Adj Close'].pct_change().dropna()\n",
        "    if len(company_returns) == 0 or len(index_returns) == 0:\n",
        "        raise ValueError(\"No overlapping data points found.\")\n",
        "    aligned_data = pd.concat([company_returns, index_returns], axis=1).dropna()\n",
        "    company_returns_aligned = aligned_data.iloc[:, 0]\n",
        "    index_returns_aligned = aligned_data.iloc[:, 1]\n",
        "    excess_returns = company_returns_aligned - index_returns_aligned\n",
        "    return excess_returns.mean() / excess_returns.std()\n",
        "\n",
        "def calculate_cvar(price_data, confidence_level=0.95):\n",
        "    returns = price_data['Adj Close'].pct_change().dropna()\n",
        "    var = returns.quantile(1 - confidence_level)\n",
        "    cvar = returns[returns <= var].mean()\n",
        "    return cvar\n",
        "\n",
        "def calculate_beta(company_data, index_data):\n",
        "    common_dates = company_data.index.intersection(index_data.index)\n",
        "    company_returns = company_data.loc[common_dates, 'Adj Close'].pct_change().dropna()\n",
        "    index_returns = index_data.loc[common_dates, 'Adj Close'].pct_change().dropna()\n",
        "    if len(company_returns) == 0 or len(index_returns) == 0:\n",
        "        raise ValueError(\"No overlapping data points found.\")\n",
        "    aligned_data = pd.concat([company_returns, index_returns], axis=1).dropna()\n",
        "    company_returns_aligned = aligned_data.iloc[:, 0]\n",
        "    index_returns_aligned = aligned_data.iloc[:, 1]\n",
        "    covariance_matrix = np.cov(company_returns_aligned, index_returns_aligned)\n",
        "    beta = covariance_matrix[0, 1] / covariance_matrix[1, 1]\n",
        "    return beta\n",
        "\n",
        "def calculate_beta_adjusted_sharpe_ratio(price_data, index_data, risk_free_rate=0.01):\n",
        "    beta = calculate_beta(price_data, index_data)\n",
        "    sharpe_ratio = calculate_sharpe_ratio(price_data, risk_free_rate)\n",
        "    return sharpe_ratio / beta if beta else None\n",
        "\n",
        "def calculate_drawdown_duration(price_data):\n",
        "    rolling_max = price_data['Adj Close'].cummax()\n",
        "    drawdowns = (price_data['Adj Close'] - rolling_max) / rolling_max\n",
        "    drawdown_duration = (drawdowns < 0).astype(int).groupby((drawdowns >= 0).astype(int).cumsum()).cumsum().max()\n",
        "    return drawdown_duration\n",
        "\n",
        "def calculate_ulcer_index(price_data):\n",
        "    rolling_max = price_data['Adj Close'].cummax()\n",
        "    drawdowns = (price_data['Adj Close'] - rolling_max) / rolling_max\n",
        "    return (drawdowns ** 2).mean() ** 0.5\n",
        "\n",
        "def calculate_jensens_alpha(price_data, index_data, risk_free_rate=0.01):\n",
        "    common_dates = price_data.index.intersection(index_data.index)\n",
        "    company_returns = price_data.loc[common_dates, 'Adj Close'].pct_change().dropna()\n",
        "    index_returns = index_data.loc[common_dates, 'Adj Close'].pct_change().dropna()\n",
        "    if len(company_returns) == 0 or len(index_returns) == 0:\n",
        "        raise ValueError(\"No overlapping data points found.\")\n",
        "    aligned_data = pd.concat([company_returns, index_returns], axis=1).dropna()\n",
        "    company_returns_aligned = aligned_data.iloc[:, 0]\n",
        "    index_returns_aligned = aligned_data.iloc[:, 1]\n",
        "    beta = calculate_beta(price_data, index_data)\n",
        "    expected_return = risk_free_rate / 252 + beta * (index_returns_aligned.mean() - risk_free_rate / 252)\n",
        "    alpha = company_returns_aligned.mean() - expected_return\n",
        "    return alpha\n",
        "\n",
        "metrics.update({\n",
        "    'Standard Deviation': calculate_standard_deviation(data['AAPL']),\n",
        "    'Value at Risk (VaR)': calculate_var(data['AAPL']),\n",
        "    'Sharpe Ratio': calculate_sharpe_ratio(data['AAPL']),\n",
        "    'Sortino Ratio': calculate_sortino_ratio(data['AAPL']),\n",
        "    'Maximum Drawdown': calculate_max_drawdown(data['AAPL']),\n",
        "    'Downside Deviation': calculate_downside_deviation(data['AAPL']),\n",
        "    'Tracking Error': calculate_tracking_error(data['AAPL'], sector_index_data),\n",
        "    'R-squared': calculate_r_squared(data['AAPL'], sector_index_data),\n",
        "    'Treynor Ratio': calculate_treynor_ratio(data['AAPL'], beta=company_info.get('beta')),\n",
        "    'Information Ratio': calculate_information_ratio(data['AAPL'], sector_index_data),\n",
        "    'Conditional Value at Risk (CVaR)': calculate_cvar(data['AAPL']),\n",
        "    'Beta-adjusted Sharpe Ratio': calculate_beta_adjusted_sharpe_ratio(data['AAPL'], sector_index_data),\n",
        "    'Drawdown Duration': calculate_drawdown_duration(data['AAPL']),\n",
        "    'Ulcer Index': calculate_ulcer_index(data['AAPL']),\n",
        "    'Jensenâ€™s Alpha': calculate_jensens_alpha(data['AAPL'], sector_index_data)\n",
        "})\n",
        "\n",
        "# Prepare data for the ANN\n",
        "def prepare_data(data, metrics):\n",
        "    # Use metrics as input features\n",
        "    X = np.array(list(metrics.values())).reshape(1, -1)\n",
        "    # Extract adjusted close price as output\n",
        "    latest_adjusted_close_price = data['AAPL'].iloc[-1]['Adj Close']\n",
        "    y = np.array([latest_adjusted_close_price])\n",
        "    return X, y\n",
        "\n",
        "# Prepare data\n",
        "X, y = prepare_data(data, metrics)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "def split_data(data, train_years=10, test_years=2):\n",
        "    total_years = train_years + test_years\n",
        "    split_date = pd.Timestamp(data['AAPL'].index[-1] - pd.DateOffset(years=test_years))\n",
        "    train_data = data['AAPL'].loc[data['AAPL'].index <= split_date]\n",
        "    test_data = data['AAPL'].loc[data['AAPL'].index > split_date]\n",
        "    return train_data, test_data\n",
        "\n",
        "train_data, test_data = split_data(data)\n",
        "\n",
        "# Prepare the ANN model\n",
        "def create_ann_model(input_dim):\n",
        "    model = Sequential()\n",
        "    model.add(Dense(64, input_dim=input_dim, activation='relu'))\n",
        "    model.add(Dense(32, activation='relu'))\n",
        "    model.add(Dense(16, activation='relu'))\n",
        "    model.add(Dense(8, activation='relu'))\n",
        "    model.add(Dense(1))\n",
        "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "    return model\n",
        "\n",
        "# Create ANN model\n",
        "model = create_ann_model(len(metrics))\n",
        "\n",
        "# Train the model\n",
        "X_train = np.array([list(metrics.values())] * len(train_data)).reshape(len(train_data), -1)\n",
        "y_train = train_data['Adj Close'].values\n",
        "model.fit(X_train, y_train, epochs=20)\n",
        "\n",
        "# Test the model\n",
        "X_test = np.array([list(metrics.values())] * len(test_data)).reshape(len(test_data), -1)\n",
        "y_test = test_data['Adj Close'].values\n",
        "predictions = model.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "mse = np.mean((predictions.flatten() - y_test) ** 2)\n",
        "print(f\"Mean Squared Error on Test Data: {mse}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "TuPPBi2zkwE9",
        "outputId": "ac7fd3c4-db38-45fb-b23d-d7d7ae36a66d"
      },
      "outputs": [],
      "source": [
        "# Define functions (e.g., calculate_standard_deviation, calculate_var, etc.) here\n",
        "\n",
        "# Fetch and prepare data\n",
        "data = fetch_data(sp500_tickers, '2012-08-12', '2022-08-12')  # Adjust start date for 10 years\n",
        "sector_index_data = yf.download(sector_index_ticker, start='2012-08-12', end='2022-08-12')\n",
        "\n",
        "# Define model and scaler\n",
        "model = create_ann_model(len(metrics))\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Split data\n",
        "train_data, test_data = split_data(data)\n",
        "\n",
        "# Train model\n",
        "X_train = np.array([list(metrics.values())] * len(train_data)).reshape(len(train_data), -1)\n",
        "y_train = train_data['Adj Close'].values\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "model.fit(X_train_scaled, y_train, epochs=20)\n",
        "\n",
        "# Calculate metrics for the last 2 years\n",
        "last_2_years_metrics = calculate_last_2_years_metrics(data['AAPL'], sector_index_data)\n",
        "\n",
        "# Prepare the data for prediction\n",
        "X_last_2_years = np.array(list(last_2_years_metrics.values())).reshape(1, -1)\n",
        "X_last_2_years_scaled = scaler.transform(X_last_2_years)\n",
        "\n",
        "# Predict and plot\n",
        "predicted_prices = model.predict(X_last_2_years_scaled)\n",
        "start_date = data['AAPL'].index[-2*365]  # Approximate start date of the last 2 years\n",
        "end_date = data['AAPL'].index[-1]\n",
        "actual_prices = data['AAPL'].loc[start_date:end_date]['Adj Close'].values\n",
        "\n",
        "dates = data['AAPL'].loc[start_date:end_date].index\n",
        "\n",
        "plt.figure(figsize=(14, 7))\n",
        "plt.plot(dates, actual_prices, label='Actual Prices', color='blue')\n",
        "plt.plot(dates, predicted_prices.flatten(), label='Predicted Prices', color='red', linestyle='--')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Stock Price')\n",
        "plt.title('Actual vs. Predicted Stock Prices')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 455
        },
        "id": "bkgYNFnyl-3c",
        "outputId": "3d86f9f0-45a0-4497-9d76-629db5b281e8"
      },
      "outputs": [],
      "source": [
        "train_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load historical data from CSV\n",
        "company_data = pd.read_csv('AAPL_data.csv', index_col='Date', parse_dates=True)\n",
        "sector_index_data = pd.read_csv('XLC_data.csv', index_col='Date', parse_dates=True)\n",
        "\n",
        "# Define function to calculate metrics\n",
        "def calculate_metrics(info):\n",
        "    metrics = {\n",
        "        'P/E Ratio': info.get('forwardEps') / info.get('previousClose') if info.get('forwardEps') and info.get('previousClose') else None,\n",
        "        'P/B Ratio': info.get('priceToBook'),\n",
        "        'Dividend Yield': info.get('dividendYield'),\n",
        "        'Dividend Payout Ratio': info.get('payoutRatio'),\n",
        "        'ROE': info.get('returnOnEquity'),\n",
        "        'ROA': info.get('returnOnAssets'),\n",
        "        'Beta': info.get('beta'),\n",
        "        'Market Capitalization': info.get('marketCap'),\n",
        "        'Revenue Growth': info.get('revenueGrowth'),\n",
        "        'Debt-to-Equity Ratio': info.get('debtToEquity'),\n",
        "        'Free Cash Flow': info.get('freeCashflow'),\n",
        "        'Current Ratio': info.get('currentRatio'),\n",
        "        'Quick Ratio': info.get('quickRatio'),\n",
        "        'PEG Ratio': info.get('pegRatio'),\n",
        "        'Standard Deviation': None,\n",
        "        'Value at Risk (VaR)': None,\n",
        "        'Sharpe Ratio': None,\n",
        "        'Sortino Ratio': None,\n",
        "        'Maximum Drawdown': None,\n",
        "        'Downside Deviation': None,\n",
        "        'Tracking Error': None,\n",
        "        'R-squared': None,\n",
        "        'Treynor Ratio': None,\n",
        "        'Information Ratio': None,\n",
        "        'Conditional Value at Risk (CVaR)': None,\n",
        "        'Beta-adjusted Sharpe Ratio': None,\n",
        "        'Drawdown Duration': None,\n",
        "        'Ulcer Index': None,\n",
        "        'Jensenâ€™s Alpha': None\n",
        "    }\n",
        "    return metrics\n",
        "\n",
        "# Calculate financial metrics for the company\n",
        "def calculate_standard_deviation(price_data):\n",
        "    returns = price_data['Adj Close'].pct_change().dropna()\n",
        "    return returns.std()\n",
        "\n",
        "def calculate_var(price_data, confidence_level=0.95):\n",
        "    returns = price_data['Adj Close'].pct_change().dropna()\n",
        "    return returns.quantile(1 - confidence_level)\n",
        "\n",
        "def calculate_sharpe_ratio(price_data, risk_free_rate=0.01):\n",
        "    returns = price_data['Adj Close'].pct_change().dropna()\n",
        "    excess_returns = returns - risk_free_rate / 252\n",
        "    return excess_returns.mean() / excess_returns.std()\n",
        "\n",
        "def calculate_sortino_ratio(price_data, risk_free_rate=0.01):\n",
        "    returns = price_data['Adj Close'].pct_change().dropna()\n",
        "    excess_returns = returns - risk_free_rate / 252\n",
        "    downside_returns = excess_returns[excess_returns < 0]\n",
        "    return excess_returns.mean() / downside_returns.std()\n",
        "\n",
        "def calculate_max_drawdown(price_data):\n",
        "    rolling_max = price_data['Adj Close'].cummax()\n",
        "    drawdowns = (price_data['Adj Close'] - rolling_max) / rolling_max\n",
        "    return drawdowns.min()\n",
        "\n",
        "def calculate_downside_deviation(price_data, risk_free_rate=0.01):\n",
        "    returns = price_data['Adj Close'].pct_change().dropna()\n",
        "    downside_returns = returns[returns < risk_free_rate / 252]\n",
        "    return downside_returns.std()\n",
        "\n",
        "def calculate_tracking_error(company_data, index_data):\n",
        "    common_dates = company_data.index.intersection(index_data.index)\n",
        "    company_returns = company_data.loc[common_dates, 'Adj Close'].pct_change().dropna()\n",
        "    index_returns = index_data.loc[common_dates, 'Adj Close'].pct_change().dropna()\n",
        "    aligned_data = pd.concat([company_returns, index_returns], axis=1).dropna()\n",
        "    company_returns_aligned = aligned_data.iloc[:, 0]\n",
        "    index_returns_aligned = aligned_data.iloc[:, 1]\n",
        "    excess_returns = company_returns_aligned - index_returns_aligned\n",
        "    return excess_returns.std()\n",
        "\n",
        "def calculate_r_squared(company_data, index_data):\n",
        "    common_dates = company_data.index.intersection(index_data.index)\n",
        "    company_returns = company_data.loc[common_dates, 'Adj Close'].pct_change().dropna()\n",
        "    index_returns = index_data.loc[common_dates, 'Adj Close'].pct_change().dropna()\n",
        "    aligned_data = pd.concat([company_returns, index_returns], axis=1).dropna()\n",
        "    company_returns_aligned = aligned_data.iloc[:, 0]\n",
        "    index_returns_aligned = aligned_data.iloc[:, 1]\n",
        "    model = LinearRegression().fit(index_returns_aligned.values.reshape(-1, 1), company_returns_aligned.values)\n",
        "    return model.score(index_returns_aligned.values.reshape(-1, 1), company_returns_aligned.values)\n",
        "\n",
        "def calculate_treynor_ratio(price_data, risk_free_rate=0.01, beta=None):\n",
        "    returns = price_data['Adj Close'].pct_change().dropna()\n",
        "    excess_returns = returns.mean() - risk_free_rate / 252\n",
        "    return excess_returns / beta if beta else None\n",
        "\n",
        "def calculate_information_ratio(price_data, index_data):\n",
        "    common_dates = price_data.index.intersection(index_data.index)\n",
        "    company_returns = price_data.loc[common_dates, 'Adj Close'].pct_change().dropna()\n",
        "    index_returns = index_data.loc[common_dates, 'Adj Close'].pct_change().dropna()\n",
        "    aligned_data = pd.concat([company_returns, index_returns], axis=1).dropna()\n",
        "    company_returns_aligned = aligned_data.iloc[:, 0]\n",
        "    index_returns_aligned = aligned_data.iloc[:, 1]\n",
        "    excess_returns = company_returns_aligned - index_returns_aligned\n",
        "    return excess_returns.mean() / excess_returns.std()\n",
        "\n",
        "def calculate_cvar(price_data, confidence_level=0.95):\n",
        "    returns = price_data['Adj Close'].pct_change().dropna()\n",
        "    var = returns.quantile(1 - confidence_level)\n",
        "    cvar = returns[returns <= var].mean()\n",
        "    return cvar\n",
        "\n",
        "def calculate_beta(company_data, index_data):\n",
        "    common_dates = company_data.index.intersection(index_data.index)\n",
        "    company_returns = company_data.loc[common_dates, 'Adj Close'].pct_change().dropna()\n",
        "    index_returns = index_data.loc[common_dates, 'Adj Close'].pct_change().dropna()\n",
        "    aligned_data = pd.concat([company_returns, index_returns], axis=1).dropna()\n",
        "    company_returns_aligned = aligned_data.iloc[:, 0]\n",
        "    index_returns_aligned = aligned_data.iloc[:, 1]\n",
        "    covariance_matrix = np.cov(company_returns_aligned, index_returns_aligned)\n",
        "    beta = covariance_matrix[0, 1] / covariance_matrix[1, 1]\n",
        "    return beta\n",
        "\n",
        "def calculate_beta_adjusted_sharpe_ratio(price_data, index_data, risk_free_rate=0.01):\n",
        "    beta = calculate_beta(price_data, index_data)\n",
        "    sharpe_ratio = calculate_sharpe_ratio(price_data, risk_free_rate)\n",
        "    return sharpe_ratio / beta if beta else None\n",
        "\n",
        "def calculate_drawdown_duration(price_data):\n",
        "    rolling_max = price_data['Adj Close'].cummax()\n",
        "    drawdowns = (price_data['Adj Close'] - rolling_max) / rolling_max\n",
        "    drawdown_duration = (drawdowns < 0).astype(int).groupby((drawdowns >= 0).astype(int).cumsum()).cumsum().max()\n",
        "    return drawdown_duration\n",
        "\n",
        "def calculate_ulcer_index(price_data):\n",
        "    rolling_max = price_data['Adj Close'].cummax()\n",
        "    drawdowns = (price_data['Adj Close'] - rolling_max) / rolling_max\n",
        "    return (drawdowns ** 2).mean() ** 0.5\n",
        "\n",
        "def calculate_jensens_alpha(price_data, index_data, risk_free_rate=0.01):\n",
        "    common_dates = price_data.index.intersection(index_data.index)\n",
        "    company_returns = price_data.loc[common_dates, 'Adj Close'].pct_change().dropna()\n",
        "    index_returns = index_data.loc[common_dates, 'Adj Close'].pct_change().dropna()\n",
        "    aligned_data = pd.concat([company_returns, index_returns], axis=1).dropna()\n",
        "    company_returns_aligned = aligned_data.iloc[:, 0]\n",
        "    index_returns_aligned = aligned_data.iloc[:, 1]\n",
        "    beta = calculate_beta(price_data, index_data)\n",
        "    expected_return = risk_free_rate / 252 + beta * (index_returns_aligned.mean() - risk_free_rate / 252)\n",
        "    alpha = company_returns_aligned.mean() - expected_return\n",
        "    return alpha\n",
        "\n",
        "# Calculate financial metrics\n",
        "company_info = fetch_company_info('AAPL')\n",
        "metrics = calculate_metrics(company_info)\n",
        "\n",
        "# Update metrics with calculations that require the sector index data\n",
        "metrics.update({\n",
        "    'Standard Deviation': calculate_standard_deviation(company_data),\n",
        "    'Value at Risk (VaR)': calculate_var(company_data),\n",
        "    'Sharpe Ratio': calculate_sharpe_ratio(company_data),\n",
        "    'Sortino Ratio': calculate_sortino_ratio(company_data),\n",
        "    'Maximum Drawdown': calculate_max_drawdown(company_data),\n",
        "    'Downside Deviation': calculate_downside_deviation(company_data),\n",
        "    'Tracking Error': calculate_tracking_error(company_data, sector_index_data),\n",
        "    'R-squared': calculate_r_squared(company_data, sector_index_data),\n",
        "    'Treynor Ratio': calculate_treynor_ratio(company_data, beta=metrics.get('Beta')),\n",
        "    'Information Ratio': calculate_information_ratio(company_data, sector_index_data),\n",
        "    'Conditional Value at Risk (CVaR)': calculate_cvar(company_data),\n",
        "    'Beta-adjusted Sharpe Ratio': calculate_beta_adjusted_sharpe_ratio(company_data, sector_index_data),\n",
        "    'Drawdown Duration': calculate_drawdown_duration(company_data),\n",
        "    'Ulcer Index': calculate_ulcer_index(company_data),\n",
        "    'Jensenâ€™s Alpha': calculate_jensens_alpha(company_data, sector_index_data)\n",
        "})\n",
        "\n",
        "# Prepare data for ANN\n",
        "X = company_data[['P/E Ratio', 'P/B Ratio', 'Dividend Yield', 'Dividend Payout Ratio', 'ROE', 'ROA', 'Beta', 'Market Capitalization',\n",
        "                  'Revenue Growth', 'Debt-to-Equity Ratio', 'Free Cash Flow', 'Current Ratio', 'Quick Ratio', 'PEG Ratio',\n",
        "                  'Standard Deviation', 'Value at Risk (VaR)', 'Sharpe Ratio', 'Sortino Ratio', 'Maximum Drawdown',\n",
        "                  'Downside Deviation', 'Tracking Error', 'R-squared', 'Treynor Ratio', 'Information Ratio', \n",
        "                  'Conditional Value at Risk (CVaR)', 'Beta-adjusted Sharpe Ratio', 'Drawdown Duration', 'Ulcer Index', \n",
        "                  'Jensenâ€™s Alpha']]\n",
        "y = company_data['Adj Close']\n",
        "\n",
        "# Normalize data\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Build and train the ANN model\n",
        "model = Sequential()\n",
        "model.add(Dense(64, input_dim=X_train.shape[1], activation='relu'))\n",
        "model.add(Dense(32, activation='relu'))\n",
        "model.add(Dense(16, activation='relu'))\n",
        "model.add(Dense(8, activation='relu'))\n",
        "model.add(Dense(1, activation='linear'))\n",
        "\n",
        "model.compile(loss='mean_squared_error', optimizer='adam')\n",
        "\n",
        "history = model.fit(X_train, y_train, epochs=20, batch_size=32, validation_data=(X_test, y_test), verbose=1)\n",
        "\n",
        "# Evaluate the model\n",
        "loss = model.evaluate(X_test, y_test)\n",
        "print(f\"Mean Squared Error on test data: {loss}\")\n",
        "\n",
        "# Plot training history\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('Model Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend(['Train', 'Validation'], loc='upper right')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "import yfinance as yf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "from datetime import datetime, timedelta"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define the SP500 ticker and sector index ticker\n",
        "sp500_ticker = 'AAPL'  # Example ticker\n",
        "sector_index_ticker = 'XLC'  # Example sector index ticker for Technology\n",
        "\n",
        "# Calculate date range\n",
        "end_date = datetime.now()\n",
        "start_date = end_date - timedelta(days=365*12)  # 12 years ago\n",
        "\n",
        "# Fetch historical data\n",
        "def fetch_data(ticker, start_date, end_date):\n",
        "    return yf.download(ticker, start=start_date, end=end_date)\n",
        "\n",
        "company_data = fetch_data(sp500_ticker, start_date, end_date)\n",
        "sector_index_data = fetch_data(sector_index_ticker, start_date, end_date)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Function to calculate metrics\n",
        "def calculate_metrics(company_data, sector_index_data, company_info):\n",
        "    metrics = {\n",
        "        'P/E Ratio': company_info.get('forwardEps') / company_info.get('previousClose') if company_info.get('forwardEps') and company_info.get('previousClose') else None,\n",
        "        'P/B Ratio': company_info.get('priceToBook'),\n",
        "        'Dividend Yield': company_info.get('dividendYield'),\n",
        "        'Dividend Payout Ratio': company_info.get('payoutRatio'),\n",
        "        'ROE': company_info.get('returnOnEquity'),\n",
        "        'ROA': company_info.get('returnOnAssets'),\n",
        "        'Beta': company_info.get('beta'),\n",
        "        'Market Capitalization': company_info.get('marketCap'),\n",
        "        'Revenue Growth': company_info.get('revenueGrowth'),\n",
        "        'Debt-to-Equity Ratio': company_info.get('debtToEquity'),\n",
        "        'Free Cash Flow': company_info.get('freeCashflow'),\n",
        "        'Current Ratio': company_info.get('currentRatio'),\n",
        "        'Quick Ratio': company_info.get('quickRatio'),\n",
        "        'PEG Ratio': company_info.get('pegRatio'),\n",
        "        'Standard Deviation': calculate_standard_deviation(company_data),\n",
        "        'Value at Risk (VaR)': calculate_var(company_data),\n",
        "        'Sharpe Ratio': calculate_sharpe_ratio(company_data),\n",
        "        'Sortino Ratio': calculate_sortino_ratio(company_data),\n",
        "        'Maximum Drawdown': calculate_max_drawdown(company_data),\n",
        "        'Downside Deviation': calculate_downside_deviation(company_data),\n",
        "        'Tracking Error': calculate_tracking_error(company_data, sector_index_data),\n",
        "        'R-squared': calculate_r_squared(company_data, sector_index_data),\n",
        "        'Treynor Ratio': calculate_treynor_ratio(company_data, beta=company_info.get('beta')),\n",
        "        'Information Ratio': calculate_information_ratio(company_data, sector_index_data),\n",
        "        'Conditional Value at Risk (CVaR)': calculate_cvar(company_data),\n",
        "        'Beta-adjusted Sharpe Ratio': calculate_beta_adjusted_sharpe_ratio(company_data, sector_index_data),\n",
        "        'Drawdown Duration': calculate_drawdown_duration(company_data),\n",
        "        'Ulcer Index': calculate_ulcer_index(company_data),\n",
        "        'Jensens Alpha': calculate_jensens_alpha(company_data, sector_index_data)\n",
        "    }\n",
        "    return metrics\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Calculate financial metrics for the company\n",
        "def calculate_standard_deviation(price_data):\n",
        "    returns = price_data['Adj Close'].pct_change().dropna()\n",
        "    return returns.std()\n",
        "\n",
        "def calculate_var(price_data, confidence_level=0.95):\n",
        "    returns = price_data['Adj Close'].pct_change().dropna()\n",
        "    return returns.quantile(1 - confidence_level)\n",
        "\n",
        "def calculate_sharpe_ratio(price_data, risk_free_rate=0.01):\n",
        "    returns = price_data['Adj Close'].pct_change().dropna()\n",
        "    excess_returns = returns - risk_free_rate / 252\n",
        "    return excess_returns.mean() / excess_returns.std()\n",
        "\n",
        "def calculate_sortino_ratio(price_data, risk_free_rate=0.01):\n",
        "    returns = price_data['Adj Close'].pct_change().dropna()\n",
        "    excess_returns = returns - risk_free_rate / 252\n",
        "    downside_returns = excess_returns[excess_returns < 0]\n",
        "    return excess_returns.mean() / downside_returns.std()\n",
        "\n",
        "def calculate_max_drawdown(price_data):\n",
        "    rolling_max = price_data['Adj Close'].cummax()\n",
        "    drawdowns = (price_data['Adj Close'] - rolling_max) / rolling_max\n",
        "    return drawdowns.min()\n",
        "\n",
        "def calculate_downside_deviation(price_data, risk_free_rate=0.01):\n",
        "    returns = price_data['Adj Close'].pct_change().dropna()\n",
        "    downside_returns = returns[returns < risk_free_rate / 252]\n",
        "    return downside_returns.std()\n",
        "\n",
        "def calculate_tracking_error(company_data, index_data):\n",
        "    common_dates = company_data.index.intersection(index_data.index)\n",
        "    company_returns = company_data.loc[common_dates, 'Adj Close'].pct_change().dropna()\n",
        "    index_returns = index_data.loc[common_dates, 'Adj Close'].pct_change().dropna()\n",
        "    aligned_data = pd.concat([company_returns, index_returns], axis=1).dropna()\n",
        "    company_returns_aligned = aligned_data.iloc[:, 0]\n",
        "    index_returns_aligned = aligned_data.iloc[:, 1]\n",
        "    excess_returns = company_returns_aligned - index_returns_aligned\n",
        "    return excess_returns.std()\n",
        "\n",
        "def calculate_r_squared(company_data, index_data):\n",
        "    common_dates = company_data.index.intersection(index_data.index)\n",
        "    company_returns = company_data.loc[common_dates, 'Adj Close'].pct_change().dropna()\n",
        "    index_returns = index_data.loc[common_dates, 'Adj Close'].pct_change().dropna()\n",
        "    aligned_data = pd.concat([company_returns, index_returns], axis=1).dropna()\n",
        "    company_returns_aligned = aligned_data.iloc[:, 0]\n",
        "    index_returns_aligned = aligned_data.iloc[:, 1]\n",
        "    model = LinearRegression().fit(index_returns_aligned.values.reshape(-1, 1), company_returns_aligned.values)\n",
        "    return model.score(index_returns_aligned.values.reshape(-1, 1), company_returns_aligned.values)\n",
        "\n",
        "def calculate_treynor_ratio(price_data, risk_free_rate=0.01, beta=None):\n",
        "    returns = price_data['Adj Close'].pct_change().dropna()\n",
        "    excess_returns = returns.mean() - risk_free_rate / 252\n",
        "    return excess_returns / beta if beta else None\n",
        "\n",
        "def calculate_information_ratio(price_data, index_data):\n",
        "    common_dates = price_data.index.intersection(index_data.index)\n",
        "    company_returns = price_data.loc[common_dates, 'Adj Close'].pct_change().dropna()\n",
        "    index_returns = index_data.loc[common_dates, 'Adj Close'].pct_change().dropna()\n",
        "    aligned_data = pd.concat([company_returns, index_returns], axis=1).dropna()\n",
        "    company_returns_aligned = aligned_data.iloc[:, 0]\n",
        "    index_returns_aligned = aligned_data.iloc[:, 1]\n",
        "    excess_returns = company_returns_aligned - index_returns_aligned\n",
        "    return excess_returns.mean() / excess_returns.std()\n",
        "\n",
        "def calculate_cvar(price_data, confidence_level=0.95):\n",
        "    returns = price_data['Adj Close'].pct_change().dropna()\n",
        "    var = returns.quantile(1 - confidence_level)\n",
        "    cvar = returns[returns <= var].mean()\n",
        "    return cvar\n",
        "\n",
        "def calculate_beta(company_data, index_data):\n",
        "    common_dates = company_data.index.intersection(index_data.index)\n",
        "    company_returns = company_data.loc[common_dates, 'Adj Close'].pct_change().dropna()\n",
        "    index_returns = index_data.loc[common_dates, 'Adj Close'].pct_change().dropna()\n",
        "    aligned_data = pd.concat([company_returns, index_returns], axis=1).dropna()\n",
        "    company_returns_aligned = aligned_data.iloc[:, 0]\n",
        "    index_returns_aligned = aligned_data.iloc[:, 1]\n",
        "    covariance_matrix = np.cov(company_returns_aligned, index_returns_aligned)\n",
        "    beta = covariance_matrix[0, 1] / covariance_matrix[1, 1]\n",
        "    return beta\n",
        "\n",
        "def calculate_beta_adjusted_sharpe_ratio(price_data, index_data, risk_free_rate=0.01):\n",
        "    beta = calculate_beta(price_data, index_data)\n",
        "    sharpe_ratio = calculate_sharpe_ratio(price_data, risk_free_rate)\n",
        "    return sharpe_ratio / beta if beta else None\n",
        "\n",
        "def calculate_drawdown_duration(price_data):\n",
        "    rolling_max = price_data['Adj Close'].cummax()\n",
        "    drawdowns = (price_data['Adj Close'] - rolling_max) / rolling_max\n",
        "    drawdown_duration = (drawdowns < 0).astype(int).groupby((drawdowns >= 0).astype(int).cumsum()).cumsum().max()\n",
        "    return drawdown_duration\n",
        "\n",
        "def calculate_ulcer_index(price_data):\n",
        "    rolling_max = price_data['Adj Close'].cummax()\n",
        "    drawdowns = (price_data['Adj Close'] - rolling_max) / rolling_max\n",
        "    return (drawdowns ** 2).mean() ** 0.5\n",
        "\n",
        "def calculate_jensens_alpha(price_data, index_data, risk_free_rate=0.01):\n",
        "    common_dates = price_data.index.intersection(index_data.index)\n",
        "    company_returns = price_data.loc[common_dates, 'Adj Close'].pct_change().dropna()\n",
        "    index_returns = index_data.loc[common_dates, 'Adj Close'].pct_change().dropna()\n",
        "    aligned_data = pd.concat([company_returns, index_returns], axis=1).dropna()\n",
        "    company_returns_aligned = aligned_data.iloc[:, 0]\n",
        "    index_returns_aligned = aligned_data.iloc[:, 1]\n",
        "    beta = calculate_beta(price_data, index_data)\n",
        "    expected_return = risk_free_rate / 252 + beta * (index_returns_aligned.mean() - risk_free_rate / 252)\n",
        "    alpha = company_returns_aligned.mean() - expected_return\n",
        "    return alpha\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate metrics for each date\n",
        "def calculate_metrics_for_dates(company_data, sector_index_data):\n",
        "    dates = company_data.index\n",
        "    metrics_list = []\n",
        "    \n",
        "    company = yf.Ticker(sp500_ticker)\n",
        "    company_info = company.info\n",
        "\n",
        "    for date in dates:\n",
        "        subset_company_data = company_data.loc[:date]\n",
        "        subset_sector_index_data = sector_index_data.loc[:date]\n",
        "        \n",
        "        metrics = calculate_metrics(subset_company_data, subset_sector_index_data, company_info)\n",
        "        metrics['Adj Close'] = subset_company_data['Adj Close'].iloc[-1]\n",
        "        metrics_list.append(metrics)\n",
        "\n",
        "    return pd.DataFrame(metrics_list, index=dates)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Calculate metrics for all dates\n",
        "all_metrics_df = calculate_metrics_for_dates(company_data, sector_index_data)\n",
        "\n",
        "# Save the data to CSV\n",
        "all_metrics_df.to_csv(f'{sp500_ticker}_metrics.csv')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the data\n",
        "def load_data(file_path):\n",
        "    data = pd.read_csv(file_path, index_col=0, parse_dates=True)\n",
        "    return data[(data.index >= '2012-01-01') & (data.index <= '2020-12-31')]\n",
        "\n",
        "# Prepare data for LSTM\n",
        "def prepare_data(data, look_back=60):\n",
        "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "    scaled_data = scaler.fit_transform(data)\n",
        "    \n",
        "    X, y = [], []\n",
        "    for i in range(look_back, len(scaled_data)):\n",
        "        X.append(scaled_data[i-look_back:i])\n",
        "        y.append(scaled_data[i, 0])  # Predicting the 'Close' price\n",
        "    \n",
        "    return np.array(X), np.array(y), scaler\n",
        "\n",
        "# Build LSTM model\n",
        "def build_model(input_shape):\n",
        "    model = Sequential([\n",
        "        LSTM(units=50, return_sequences=True, input_shape=input_shape),\n",
        "        Dropout(0.2),\n",
        "        LSTM(units=50, return_sequences=True),\n",
        "        Dropout(0.2),\n",
        "        LSTM(units=50),\n",
        "        Dropout(0.2),\n",
        "        Dense(units=1)\n",
        "    ])\n",
        "    model.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error')\n",
        "    return model\n",
        "\n",
        "# Train model\n",
        "def train_model(model, X_train, y_train, epochs=100, batch_size=32, validation_split=0.2):\n",
        "    history = model.fit(\n",
        "        X_train, y_train,\n",
        "        epochs=epochs,\n",
        "        batch_size=batch_size,\n",
        "        validation_split=validation_split,\n",
        "        shuffle=False\n",
        "    )\n",
        "    return history\n",
        "\n",
        "# Make predictions\n",
        "def make_predictions(model, X_test, scaler):\n",
        "    predictions = model.predict(X_test)\n",
        "    return scaler.inverse_transform(predictions)\n",
        "\n",
        "# Evaluate model\n",
        "def evaluate_model(y_true, y_pred):\n",
        "    mse = np.mean((y_true - y_pred)**2)\n",
        "    rmse = np.sqrt(mse)\n",
        "    mae = np.mean(np.abs(y_true - y_pred))\n",
        "    return mse, rmse, mae\n",
        "\n",
        "# Plot results\n",
        "def plot_results(y_true, y_pred):\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.plot(y_true, label='Actual')\n",
        "    plt.plot(y_pred, label='Predicted')\n",
        "    plt.title('LSTM Model: Actual vs Predicted Stock Prices')\n",
        "    plt.xlabel('Time')\n",
        "    plt.ylabel('Stock Price')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "# Main function\n",
        "def main():\n",
        "    # Load data\n",
        "    file_path = 'AAPL_data_with_metrics.csv'  # Replace with your CSV file name\n",
        "    data = load_data(file_path)\n",
        "    \n",
        "    # Select features for training\n",
        "    features = ['Close', 'P/E Ratio', 'P/B Ratio', 'Dividend Yield', 'Dividend Payout Ratio', \n",
        "                'ROE', 'ROA', 'Beta', 'Market Capitalization', 'Revenue Growth', \n",
        "                'Debt-to-Equity Ratio', 'Free Cash Flow', 'Current Ratio', 'Quick Ratio', \n",
        "                'PEG Ratio', 'Standard Deviation', 'Value at Risk (VaR)', 'Sharpe Ratio', \n",
        "                'Sortino Ratio', 'Maximum Drawdown', 'Downside Deviation', 'Tracking Error', \n",
        "                'R-squared', 'Treynor Ratio', 'Information Ratio', 'Conditional Value at Risk (CVaR)', \n",
        "                'Beta-adjusted Sharpe Ratio', 'Drawdown Duration', 'Ulcer Index', 'Jensens Alpha']\n",
        "    \n",
        "    data = data[features]\n",
        "    \n",
        "    # Prepare data for LSTM\n",
        "    X, y, scaler = prepare_data(data)\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)\n",
        "    \n",
        "    # Build and train model\n",
        "    model = build_model(input_shape=(X_train.shape[1], X_train.shape[2]))\n",
        "    history = train_model(model, X_train, y_train)\n",
        "    \n",
        "    # Make predictions\n",
        "    y_pred = make_predictions(model, X_test, scaler)\n",
        "    y_test_inv = scaler.inverse_transform(y_test.reshape(-1, 1))\n",
        "    \n",
        "    # Evaluate model\n",
        "    mse, rmse, mae = evaluate_model(y_test_inv, y_pred)\n",
        "    print(f\"MSE: {mse:.4f}\")\n",
        "    print(f\"RMSE: {rmse:.4f}\")\n",
        "    print(f\"MAE: {mae:.4f}\")\n",
        "    \n",
        "    # Plot results\n",
        "    plot_results(y_test_inv, y_pred)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
