{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-15T20:17:07.496540Z",
     "start_time": "2024-11-15T20:17:07.472354Z"
    }
   },
   "outputs": [],
   "source": [
    "d=pd.read_csv('sector_mkt_cap_results/Aerospace & Defense_mkt_cap_quarter_end.csv')\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-15T20:17:33.561717Z",
     "start_time": "2024-11-15T20:17:07.669501Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def calculate_yoy_growth(df):\n",
    "    \"\"\"Calculate the Year-over-Year (YoY) growth for Market Cap.\"\"\"\n",
    "    df['YoY_Growth'] = df['MarketCap'].pct_change(periods=4) * 100  # YoY percentage change\n",
    "    df.dropna(subset=['YoY_Growth'], inplace=True)  # Drop rows with NaN YoY growth\n",
    "    return df\n",
    "\n",
    "def apply_log10_transformation(df):\n",
    "    \"\"\"Apply log10 transformation to the YoY growth.\"\"\"\n",
    "    df['Log_YoY_Growth'] = np.log10(df['YoY_Growth'] + 100)  # log10(1 + YoY_Growth) to handle negative growth\n",
    "    return df\n",
    "\n",
    "def plot_yoy_growth_with_log(df, sector):\n",
    "    \"\"\"Plot the YoY growth and log10-transformed YoY growth side by side.\"\"\"\n",
    "    unique_tickers = df['Ticker'].unique()\n",
    "\n",
    "    # Set up subplots\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "    # Plot 1: Percentage YoY Growth\n",
    "    for ticker in unique_tickers:\n",
    "        company_data = df[df['Ticker'] == ticker]\n",
    "        axs[0].plot(company_data['Date'], company_data['YoY_Growth'], label=ticker)\n",
    "\n",
    "    axs[0].set_title(f'Percentage YoY Growth for {sector} Sector')\n",
    "    axs[0].set_xlabel('Date')\n",
    "    axs[0].set_ylabel('YoY Growth (%)')\n",
    "    axs[0].legend(loc='best')\n",
    "    axs[0].grid(True)\n",
    "\n",
    "    # Plot 2: Log10 YoY Growth\n",
    "    for ticker in unique_tickers:\n",
    "        company_data = df[df['Ticker'] == ticker]\n",
    "        axs[1].plot(company_data['Date'], company_data['Log_YoY_Growth'], label=ticker)\n",
    "\n",
    "    axs[1].set_title(f'Log10 YoY Growth for {sector} Sector')\n",
    "    axs[1].set_xlabel('Date')\n",
    "    axs[1].set_ylabel('Log10 YoY Growth')\n",
    "    axs[1].legend(loc='best')\n",
    "    axs[1].grid(True)\n",
    "\n",
    "    plt.tight_layout()  # Adjust layout so plots don't overlap\n",
    "    plt.show()\n",
    "\n",
    "def main():\n",
    "    # Path to the directory where sector CSV files are saved\n",
    "    input_dir = \"sector_mkt_cap_results\"\n",
    "\n",
    "    # Process each sector CSV\n",
    "    for sector_file in os.listdir(input_dir):\n",
    "        if sector_file.endswith(\".csv\"):\n",
    "            sector = sector_file.replace(\"_mkt_cap_quarter_end.csv\", \"\")\n",
    "            print(f\"Processing {sector} sector...\")\n",
    "\n",
    "            # Load the CSV file\n",
    "            file_path = os.path.join(input_dir, sector_file)\n",
    "            df = pd.read_csv(file_path, parse_dates=['Date'])\n",
    "\n",
    "            # Calculate YoY growth and apply log10 transformation\n",
    "            df = calculate_yoy_growth(df)\n",
    "            df = apply_log10_transformation(df)\n",
    "\n",
    "            # Plot the YoY growth in percentage and log10\n",
    "            plot_yoy_growth_with_log(df, sector)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-15T20:18:02.273869Z",
     "start_time": "2024-11-15T20:17:36.608785Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def calculate_yoy_growth(df):\n",
    "    \"\"\"Calculate the Year-over-Year (YoY) growth for Market Cap.\"\"\"\n",
    "    df['YoY_Growth'] = df['MarketCap'].pct_change(periods=4) * 100  # YoY percentage change\n",
    "    df.dropna(subset=['YoY_Growth'], inplace=True)  # Drop rows with NaN YoY growth\n",
    "    return df\n",
    "\n",
    "def apply_log10_transformation(df):\n",
    "    \"\"\"Apply log10 transformation to the YoY growth.\"\"\"\n",
    "    df['Log_YoY_Growth'] = np.log10(df['YoY_Growth'] + 100)  # log10(1 + YoY_Growth) to handle negative growth\n",
    "    return df\n",
    "\n",
    "def calculate_sector_leader_and_rank(df, sector):\n",
    "    \"\"\"Calculate the leader and performance ranking for a sector.\"\"\"\n",
    "    # Calculate the average YoY growth of the sector per quarter\n",
    "    sector_avg = df.groupby('Date')['YoY_Growth'].mean()\n",
    "\n",
    "    # Count how many times each company outperforms the sector average\n",
    "    overperformance_counts = {}\n",
    "    \n",
    "    for ticker in df['Ticker'].unique():\n",
    "        company_data = df[df['Ticker'] == ticker]\n",
    "        company_data = company_data.set_index('Date')\n",
    "        company_data['Sector_Avg'] = sector_avg\n",
    "        \n",
    "        # Count number of times this company outperforms the sector average\n",
    "        overperformance_count = (company_data['YoY_Growth'] > company_data['Sector_Avg']).sum()\n",
    "        overperformance_counts[ticker] = overperformance_count\n",
    "\n",
    "    # Sort the companies based on the number of times they overperformed\n",
    "    sorted_companies = sorted(overperformance_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Identify the leader (company with the maximum overperformance count)\n",
    "    sector_leader = sorted_companies[0][0]  # The first company in the sorted list is the leader\n",
    "\n",
    "    # Print the leader and the ranking for the sector\n",
    "    print(f\"Leader for {sector} sector: {sector_leader}\")\n",
    "    print(f\"Descending order of companies by overperformance in {sector} sector:\")\n",
    "    for company, count in sorted_companies:\n",
    "        print(f\"{company}: {count} times overperformed\")\n",
    "    print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "    return sector_leader, sorted_companies\n",
    "\n",
    "def plot_yoy_growth_with_log(df, sector):\n",
    "    \"\"\"Plot the YoY growth and log10-transformed YoY growth side by side.\"\"\"\n",
    "    unique_tickers = df['Ticker'].unique()\n",
    "\n",
    "    # Set up subplots\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "    # Plot 1: Percentage YoY Growth\n",
    "    for ticker in unique_tickers:\n",
    "        company_data = df[df['Ticker'] == ticker]\n",
    "        axs[0].plot(company_data['Date'], company_data['YoY_Growth'], label=ticker)\n",
    "\n",
    "    axs[0].set_title(f'Percentage YoY Growth for {sector} Sector')\n",
    "    axs[0].set_xlabel('Date')\n",
    "    axs[0].set_ylabel('YoY Growth (%)')\n",
    "    axs[0].legend(loc='best')\n",
    "    axs[0].grid(True)\n",
    "\n",
    "    # Plot 2: Log10 YoY Growth\n",
    "    for ticker in unique_tickers:\n",
    "        company_data = df[df['Ticker'] == ticker]\n",
    "        axs[1].plot(company_data['Date'], company_data['Log_YoY_Growth'], label=ticker)\n",
    "\n",
    "    axs[1].set_title(f'Log10 YoY Growth for {sector} Sector')\n",
    "    axs[1].set_xlabel('Date')\n",
    "    axs[1].set_ylabel('Log10 YoY Growth')\n",
    "    axs[1].legend(loc='best')\n",
    "    axs[1].grid(True)\n",
    "\n",
    "    plt.tight_layout()  # Adjust layout so plots don't overlap\n",
    "    plt.show()\n",
    "\n",
    "def main():\n",
    "    # Path to the directory where sector CSV files are saved\n",
    "    input_dir = \"sector_mkt_cap_results\"\n",
    "\n",
    "    # Process each sector CSV\n",
    "    for sector_file in os.listdir(input_dir):\n",
    "        if sector_file.endswith(\".csv\"):\n",
    "            sector = sector_file.replace(\"_mkt_cap_quarter_end.csv\", \"\")\n",
    "            print(f\"Processing {sector} sector...\")\n",
    "\n",
    "            # Load the CSV file\n",
    "            file_path = os.path.join(input_dir, sector_file)\n",
    "            df = pd.read_csv(file_path, parse_dates=['Date'])\n",
    "\n",
    "            # Calculate YoY growth and apply log10 transformation\n",
    "            df = calculate_yoy_growth(df)\n",
    "            df = apply_log10_transformation(df)\n",
    "\n",
    "            # Calculate sector leader and rank\n",
    "            sector_leader, sorted_companies = calculate_sector_leader_and_rank(df, sector)\n",
    "\n",
    "            # Optionally, plot the YoY growth in percentage and log10\n",
    "            plot_yoy_growth_with_log(df, sector)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-15T20:18:23.377647Z",
     "start_time": "2024-11-15T20:18:04.660195Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def calculate_yoy_growth(df):\n",
    "    \"\"\"Calculate the Year-over-Year (YoY) growth for Market Cap.\"\"\"\n",
    "    df['YoY_Growth'] = df['MarketCap'].pct_change(periods=4) * 100  # YoY percentage change\n",
    "    df.dropna(subset=['YoY_Growth'], inplace=True)  # Drop rows with NaN YoY growth\n",
    "    return df\n",
    "\n",
    "def calculate_sector_leader_and_rank(df):\n",
    "    \"\"\"Calculate the leader and performance ranking for a sector.\"\"\"\n",
    "    sector_avg = df.groupby('Date')['YoY_Growth'].mean()\n",
    "    overperformance_counts = {}\n",
    "\n",
    "    for ticker in df['Ticker'].unique():\n",
    "        company_data = df[df['Ticker'] == ticker].set_index('Date')\n",
    "        company_data['Sector_Avg'] = sector_avg\n",
    "        overperformance_count = (company_data['YoY_Growth'] > company_data['Sector_Avg']).sum()\n",
    "        overperformance_counts[ticker] = overperformance_count\n",
    "\n",
    "    sorted_companies = sorted(overperformance_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "    return sorted_companies\n",
    "\n",
    "def calculate_weighted_average_growth(df, sorted_companies):\n",
    "    \"\"\"Calculate the weighted average growth for a sector based on company rankings.\"\"\"\n",
    "    # Assign weights to companies based on their ranking\n",
    "    total_weight = sum(range(1, len(sorted_companies) + 1))  # Total weight sum, e.g., 3+2+1\n",
    "    company_weights = {company: weight for company, weight in zip([sc[0] for sc in sorted_companies], range(len(sorted_companies), 0, -1))}\n",
    "\n",
    "    print(f\"Company Weights: {company_weights}\")  # Debugging: Print weights\n",
    "\n",
    "    # Initialize a DataFrame to store the weighted YoY growth\n",
    "    weighted_yoy_growth = pd.DataFrame()\n",
    "\n",
    "    # Process each company\n",
    "    for ticker in df['Ticker'].unique():\n",
    "        company_data = df[df['Ticker'] == ticker].copy()\n",
    "\n",
    "        # Check if the company exists in the company_weights dictionary\n",
    "        if ticker in company_weights:\n",
    "            company_data['Weight'] = company_weights[ticker]\n",
    "        else:\n",
    "            print(f\"Warning: No weight found for {ticker}, skipping.\")\n",
    "            continue\n",
    "\n",
    "        # Calculate the weighted YoY growth for the company\n",
    "        company_data['Weighted_YoY_Growth'] = company_data['YoY_Growth'] * company_data['Weight']\n",
    "\n",
    "        # Initialize weighted_yoy_growth DataFrame if it's empty\n",
    "        if weighted_yoy_growth.empty:\n",
    "            weighted_yoy_growth = company_data[['Date', 'Weighted_YoY_Growth']].copy()\n",
    "        else:\n",
    "            # Merge the company's weighted YoY growth into the existing DataFrame\n",
    "            weighted_yoy_growth = pd.merge(\n",
    "                weighted_yoy_growth,\n",
    "                company_data[['Date', 'Weighted_YoY_Growth']],\n",
    "                on='Date',\n",
    "                how='outer',\n",
    "                suffixes=('', f'_{ticker}')\n",
    "            )\n",
    "\n",
    "    # Sum across all companies for each date to get the total weighted growth\n",
    "    weighted_yoy_growth['Weighted_YoY_Growth_Total'] = weighted_yoy_growth.filter(like='Weighted_YoY_Growth').sum(axis=1)\n",
    "\n",
    "    # Normalize by the total weight\n",
    "    weighted_yoy_growth['Final_Weighted_YoY_Growth'] = weighted_yoy_growth['Weighted_YoY_Growth_Total'] / total_weight\n",
    "\n",
    "    # Debugging: print the first few rows to ensure data exists\n",
    "    print(\"First few rows of weighted YoY growth (after calculation):\")\n",
    "    print(weighted_yoy_growth.head())\n",
    "\n",
    "    return weighted_yoy_growth[['Date', 'Final_Weighted_YoY_Growth']]\n",
    "\n",
    "\n",
    "\n",
    "def plot_weighted_index(weighted_yoy_growth, sector, sorted_companies):\n",
    "    \"\"\"Plot the weighted YoY growth index for a given sector, along with individual company growth.\"\"\"\n",
    "    if weighted_yoy_growth.empty:\n",
    "        print(f\"No data to plot for {sector}.\")\n",
    "        return\n",
    "\n",
    "    plt.figure(figsize=(12, 8))\n",
    "\n",
    "    # Plot the sector's weighted index\n",
    "    plt.plot(weighted_yoy_growth['Date'], weighted_yoy_growth['Final_Weighted_YoY_Growth'],\n",
    "             label=f'{sector} Sector Index', color='blue', linewidth=2)\n",
    "\n",
    "    # Plot each company's YoY growth\n",
    "    for company, _ in sorted_companies:\n",
    "        company_col = f'Weighted_YoY_Growth_{company}'\n",
    "        if company_col in weighted_yoy_growth.  columns:\n",
    "            plt.plot(weighted_yoy_growth['Date'], weighted_yoy_growth[company_col],\n",
    "                     label=f'{company} YoY Growth', linestyle='--', alpha=0.8)\n",
    "\n",
    "    plt.title(f'Weighted YoY Growth Index and Company Growth for {sector} Sector')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('YoY Growth (%)')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Path to the directory where sector CSV files are saved\n",
    "    input_dir = \"sector_mkt_cap_results\"\n",
    "\n",
    "    # Process each sector CSV\n",
    "    for sector_file in os.listdir(input_dir):\n",
    "        if sector_file.endswith(\".csv\"):\n",
    "            sector = sector_file.replace(\"_mkt_cap_quarter_end.csv\", \"\")\n",
    "            print(f\"Processing {sector} sector...\")\n",
    "\n",
    "            # Load the CSV file\n",
    "            file_path = os.path.join(input_dir, sector_file)\n",
    "            df = pd.read_csv(file_path, parse_dates=['Date'])\n",
    "\n",
    "            # Calculate YoY growth\n",
    "            df = calculate_yoy_growth(df)\n",
    "\n",
    "            # Calculate sector leader and rank\n",
    "            sorted_companies = calculate_sector_leader_and_rank(df)\n",
    "\n",
    "            # Calculate weighted average YoY growth\n",
    "            weighted_yoy_growth = calculate_weighted_average_growth(df, sorted_companies)\n",
    "\n",
    "            # Plot the weighted index\n",
    "            plot_weighted_index(weighted_yoy_growth, sector, sorted_companies)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-15T20:18:38.691342Z",
     "start_time": "2024-11-15T20:18:23.824796Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def calculate_yoy_growth(df):\n",
    "    \"\"\"Calculate the Year-over-Year (YoY) growth for Market Cap.\"\"\"\n",
    "    df['YoY_Growth'] = df['MarketCap'].pct_change(periods=4) * 100  # YoY percentage change\n",
    "    df.dropna(subset=['YoY_Growth'], inplace=True)  # Drop rows with NaN YoY growth\n",
    "    return df\n",
    "\n",
    "def apply_log10_transformation(df):\n",
    "    \"\"\"Apply log10 transformation to the YoY growth.\"\"\"\n",
    "    df['Log_YoY_Growth'] = np.log10(df['YoY_Growth'] + 100)  # log10(1 + YoY_Growth) to handle negative growth\n",
    "    return df\n",
    "\n",
    "def calculate_sector_leader_and_rank(df, sector):\n",
    "    \"\"\"Calculate the leader and performance ranking for a sector.\"\"\"\n",
    "    sector_avg = df.groupby('Date')['YoY_Growth'].mean()\n",
    "    overperformance_counts = {}\n",
    "    \n",
    "    for ticker in df['Ticker'].unique():\n",
    "        company_data = df[df['Ticker'] == ticker]\n",
    "        company_data = company_data.set_index('Date')\n",
    "        company_data['Sector_Avg'] = sector_avg\n",
    "        \n",
    "        overperformance_count = (company_data['YoY_Growth'] > company_data['Sector_Avg']).sum()\n",
    "        overperformance_counts[ticker] = overperformance_count\n",
    "\n",
    "    sorted_companies = sorted(overperformance_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    sector_leader = sorted_companies[0][0]\n",
    "    \n",
    "    total_overperformance = sum([count for _, count in sorted_companies])\n",
    "\n",
    "    # Print sector leaders and overperformance counts\n",
    "    print(f\"Leader for {sector} sector: {sector_leader}\")\n",
    "    print(f\"Descending order of companies by overperformance in {sector} sector:\")\n",
    "    for company, count in sorted_companies:\n",
    "        print(f\"{company}: {count} times overperformed\")\n",
    "    \n",
    "    return sector_leader, sorted_companies, total_overperformance\n",
    "\n",
    "def calculate_sector_index(df, sorted_companies, total_overperformance):\n",
    "    \"\"\"Calculate the sector index based on fractional contribution of stocks.\"\"\"\n",
    "    # Create a dictionary of fractional contributions for each company\n",
    "    fractional_contribution = {company: count / total_overperformance for company, count in sorted_companies}\n",
    "    \n",
    "    # Initialize an empty DataFrame to store the sector index values\n",
    "    sector_index = pd.DataFrame()\n",
    "    \n",
    "    for ticker in df['Ticker'].unique():\n",
    "        company_data = df[df['Ticker'] == ticker].copy()\n",
    "        \n",
    "        # Assign the fractional contribution for the stock\n",
    "        if ticker in fractional_contribution:\n",
    "            contribution = fractional_contribution[ticker]\n",
    "            company_data['Weighted_YoY_Growth'] = company_data['YoY_Growth'] * contribution\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "        # Sum the weighted growth into the sector index\n",
    "        if sector_index.empty:\n",
    "            sector_index = company_data[['Date', 'Weighted_YoY_Growth']].copy()\n",
    "        else:\n",
    "            sector_index = pd.merge(sector_index, company_data[['Date', 'Weighted_YoY_Growth']],\n",
    "                                    on='Date', how='outer', suffixes=('', f'_{ticker}'))\n",
    "\n",
    "    # Sum across all companies for each date\n",
    "    sector_index['Sector_Index'] = sector_index.filter(like='Weighted_YoY_Growth').sum(axis=1)\n",
    "\n",
    "    return sector_index[['Date', 'Sector_Index']]\n",
    "\n",
    "def plot_sector_index(sector_index, sector):\n",
    "    \"\"\"Plot the sector index over time.\"\"\"\n",
    "    if sector_index.empty:\n",
    "        print(f\"No data to plot for {sector}.\")\n",
    "        return\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(sector_index['Date'], sector_index['Sector_Index'], label=f'{sector} Sector Index', color='blue', linewidth=2)\n",
    "    plt.title(f'Sector Index for {sector} Sector')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Index Value')\n",
    "    plt.grid(True)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def main():\n",
    "    # Path to the directory where sector CSV files are saved\n",
    "    input_dir = \"sector_mkt_cap_results\"\n",
    "\n",
    "    # Process each sector CSV\n",
    "    for sector_file in os.listdir(input_dir):\n",
    "        if sector_file.endswith(\".csv\"):\n",
    "            sector = sector_file.replace(\"_mkt_cap_quarter_end.csv\", \"\")\n",
    "            print(f\"Processing {sector} sector...\")\n",
    "\n",
    "            # Load the CSV file\n",
    "            file_path = os.path.join(input_dir, sector_file)\n",
    "            df = pd.read_csv(file_path, parse_dates=['Date'])\n",
    "\n",
    "            # Calculate YoY growth and apply log10 transformation\n",
    "            df = calculate_yoy_growth(df)\n",
    "            df = apply_log10_transformation(df)\n",
    "\n",
    "            # Calculate sector leader, ranking and total overperformance count\n",
    "            sector_leader, sorted_companies, total_overperformance = calculate_sector_leader_and_rank(df, sector)\n",
    "\n",
    "            # Calculate the sector index based on fractional contributions\n",
    "            sector_index = calculate_sector_index(df, sorted_companies, total_overperformance)\n",
    "\n",
    "            # Plot the sector index\n",
    "            plot_sector_index(sector_index, sector)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-15T20:18:58.197170Z",
     "start_time": "2024-11-15T20:18:41.209028Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def calculate_yoy_growth(df):\n",
    "    \"\"\"Calculate the Year-over-Year (YoY) growth for Market Cap.\"\"\"\n",
    "    df['YoY_Growth'] = df['MarketCap'].pct_change(periods=4) * 100  # YoY percentage change\n",
    "    df.dropna(subset=['YoY_Growth'], inplace=True)  # Drop rows with NaN YoY growth\n",
    "    return df\n",
    "\n",
    "def apply_log10_transformation(df):\n",
    "    \"\"\"Apply log10 transformation to the YoY growth.\"\"\"\n",
    "    df['Log_YoY_Growth'] = np.log10(df['YoY_Growth'] + 100)  # log10(1 + YoY_Growth) to handle negative growth\n",
    "    return df\n",
    "\n",
    "def calculate_sector_leader_and_rank(df, sector):\n",
    "    \"\"\"Calculate the leader and performance ranking for a sector.\"\"\"\n",
    "    sector_avg = df.groupby('Date')['YoY_Growth'].mean()\n",
    "    overperformance_counts = {}\n",
    "    \n",
    "    for ticker in df['Ticker'].unique():\n",
    "        company_data = df[df['Ticker'] == ticker]\n",
    "        company_data = company_data.set_index('Date')\n",
    "        company_data['Sector_Avg'] = sector_avg\n",
    "        \n",
    "        overperformance_count = (company_data['YoY_Growth'] > company_data['Sector_Avg']).sum()\n",
    "        overperformance_counts[ticker] = overperformance_count\n",
    "\n",
    "    sorted_companies = sorted(overperformance_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    sector_leader = sorted_companies[0][0]\n",
    "    \n",
    "    total_overperformance = sum([count**2 for _, count in sorted_companies])\n",
    "\n",
    "    print(f\"Leader for {sector} sector: {sector_leader}\")\n",
    "    print(f\"Descending order of companies by overperformance in {sector} sector:\")\n",
    "    for company, count in sorted_companies:\n",
    "        print(f\"{company}: {count} times overperformed\")\n",
    "    \n",
    "    return sector_leader, sorted_companies, total_overperformance\n",
    "\n",
    "def calculate_sector_index(df, sorted_companies, total_overperformance):\n",
    "    \"\"\"Calculate the sector index based on fractional contribution of stocks.\"\"\"\n",
    "    fractional_contribution = {company: count**2 / total_overperformance for company, count in sorted_companies}\n",
    "    \n",
    "    sector_index = pd.DataFrame()\n",
    "    \n",
    "    for ticker in df['Ticker'].unique():\n",
    "        company_data = df[df['Ticker'] == ticker].copy()\n",
    "        \n",
    "        if ticker in fractional_contribution:\n",
    "            contribution = fractional_contribution[ticker]\n",
    "            company_data['Weighted_YoY_Growth'] = company_data['YoY_Growth'] * contribution\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "        if sector_index.empty:\n",
    "            sector_index = company_data[['Date', 'Weighted_YoY_Growth']].copy()\n",
    "        else:\n",
    "            sector_index = pd.merge(sector_index, company_data[['Date', 'Weighted_YoY_Growth']],\n",
    "                                    on='Date', how='outer', suffixes=('', f'_{ticker}'))\n",
    "\n",
    "    sector_index['Sector_Index'] = sector_index.filter(like='Weighted_YoY_Growth').sum(axis=1)\n",
    "\n",
    "    return sector_index[['Date', 'Sector_Index']]\n",
    "\n",
    "def calculate_simple_average_index(df):\n",
    "    \"\"\"Calculate sector index using simple average of YoY growth.\"\"\"\n",
    "    simple_avg_index = df.groupby('Date')['YoY_Growth'].mean().reset_index()\n",
    "    simple_avg_index.rename(columns={'YoY_Growth': 'Simple_Avg_Index'}, inplace=True)\n",
    "    return simple_avg_index\n",
    "\n",
    "def plot_sector_index(sector_index, simple_avg_index, df, sector):\n",
    "    \"\"\"Plot the sector index (weighted and simple average) and stock values.\"\"\"\n",
    "    if sector_index.empty:\n",
    "        print(f\"No data to plot for {sector}.\")\n",
    "        return\n",
    "\n",
    "    # Set up the plot\n",
    "    plt.figure(figsize=(12, 8))\n",
    "\n",
    "    # Plot contributing stock YoY values\n",
    "    unique_tickers = df['Ticker'].unique()\n",
    "    for ticker in unique_tickers:\n",
    "        company_data = df[df['Ticker'] == ticker]\n",
    "        plt.plot(company_data['Date'], company_data['YoY_Growth'], label=f\"{ticker} YoY Growth\", linestyle='--')\n",
    "\n",
    "    # Plot weighted sector index\n",
    "    plt.plot(sector_index['Date'], sector_index['Sector_Index'], label=f'{sector} Weighted Sector Index', color='blue', linewidth=2)\n",
    "\n",
    "    # Plot simple average sector index\n",
    "    plt.plot(simple_avg_index['Date'], simple_avg_index['Simple_Avg_Index'], label=f'{sector} Simple Average Index', color='red', linewidth=2)\n",
    "\n",
    "    plt.title(f'Sector Index for {sector} Sector (Weighted vs Simple Average)')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('YoY Growth / Index Value')\n",
    "    plt.legend(loc='best')\n",
    "    plt.grid(True)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def main():\n",
    "    # Path to the directory where sector CSV files are saved\n",
    "    input_dir = \"sector_mkt_cap_results\"\n",
    "    output_dir = \"sector_wise_index\"  # Directory to save sector indices\n",
    "\n",
    "    # Create output directory if it doesn't exist\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    # Process each sector CSV\n",
    "    for sector_file in os.listdir(input_dir):\n",
    "        if sector_file.endswith(\".csv\"):\n",
    "            sector = sector_file.replace(\"_mkt_cap_quarter_end.csv\", \"\")\n",
    "            print(f\"Processing {sector} sector...\")\n",
    "\n",
    "            # Load the CSV file\n",
    "            file_path = os.path.join(input_dir, sector_file)\n",
    "            df = pd.read_csv(file_path, parse_dates=['Date'])\n",
    "\n",
    "            # Calculate YoY growth and apply log10 transformation\n",
    "            df = calculate_yoy_growth(df)\n",
    "            df = apply_log10_transformation(df)\n",
    "\n",
    "            # Calculate sector leader, ranking and total overperformance count\n",
    "            sector_leader, sorted_companies, total_overperformance = calculate_sector_leader_and_rank(df, sector)\n",
    "\n",
    "            # Calculate the sector index based on fractional contributions\n",
    "            sector_weighted_index = calculate_sector_index(df, sorted_companies, total_overperformance)\n",
    "\n",
    "            # Calculate the simple average index\n",
    "            simple_avg_index = calculate_simple_average_index(df)\n",
    "\n",
    "            # Merge sector_index and simple_avg_index for saving\n",
    "            combined_index = pd.merge(sector_weighted_index, simple_avg_index, on='Date', how='outer')\n",
    "\n",
    "            # Save the sector index data to a CSV file\n",
    "            output_file_path = os.path.join(output_dir, f\"{sector}_sector_index.csv\")\n",
    "            combined_index.to_csv(output_file_path, index=False)\n",
    "            print(f\"Saved sector index for {sector} to {output_file_path}\")\n",
    "\n",
    "            # Plot the sector index (both weighted and simple average) alongside stock values\n",
    "            plot_sector_index(sector_weighted_index, simple_avg_index, df, sector)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-15T20:19:06.050050Z",
     "start_time": "2024-11-15T20:19:05.156934Z"
    }
   },
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "\n",
    "# Fetch data for a specific ticker\n",
    "ticker = \"AAPL\"  # Example: Apple Inc.\n",
    "stock = yf.Ticker(ticker)\n",
    "\n",
    "# Get quarterly financials (revenue data is included)\n",
    "quarterly_financials = stock.quarterly_financials.T\n",
    "\n",
    "# Extract revenue data\n",
    "revenue_data = quarterly_financials[['Total Revenue']]\n",
    "\n",
    "print(revenue_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-15T20:19:06.455850Z",
     "start_time": "2024-11-15T20:19:06.446396Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Convert revenue to numeric values (if necessary)\n",
    "revenue_data['Total Revenue'] = pd.to_numeric(revenue_data['Total Revenue'], errors='coerce')\n",
    "\n",
    "# Sort data by date\n",
    "revenue_data = revenue_data.sort_index()\n",
    "\n",
    "# Shift revenue by 4 quarters (1 year) to calculate YoY\n",
    "revenue_data['YoY Growth'] = (revenue_data['Total Revenue'] - revenue_data['Total Revenue'].shift(4)) / revenue_data['Total Revenue'].shift(4) * 100\n",
    "\n",
    "# Drop rows with NaN values caused by the shift\n",
    "revenue_data = revenue_data.dropna()\n",
    "\n",
    "print(revenue_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-15T20:19:08.170810Z",
     "start_time": "2024-11-15T20:19:07.114520Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def calculate_yoy_growth(df):\n",
    "    \"\"\"Calculate the Year-over-Year (YoY) growth for Market Cap.\"\"\"\n",
    "    df['YoY_Growth'] = df['MarketCap'].pct_change(periods=4) * 100  # YoY percentage change\n",
    "    df.dropna(subset=['YoY_Growth'], inplace=True)  # Drop rows with NaN YoY growth\n",
    "    return df\n",
    "\n",
    "def calculate_sector_index_variations(df, sorted_companies, total_overperformance):\n",
    "    \"\"\"Calculate sector index and return variations between weighted and simple average index.\"\"\"\n",
    "    fractional_contribution = {company: count**2 / total_overperformance for company, count in sorted_companies}\n",
    "    \n",
    "    sector_index = pd.DataFrame()\n",
    "\n",
    "    for ticker in df['Ticker'].unique():\n",
    "        company_data = df[df['Ticker'] == ticker].copy()\n",
    "\n",
    "        if ticker in fractional_contribution:\n",
    "            contribution = fractional_contribution[ticker]\n",
    "            company_data['Weighted_YoY_Growth'] = company_data['YoY_Growth'] * contribution\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "        if sector_index.empty:\n",
    "            sector_index = company_data[['Date', 'Weighted_YoY_Growth']].copy()\n",
    "        else:\n",
    "            sector_index = pd.merge(sector_index, company_data[['Date', 'Weighted_YoY_Growth']],\n",
    "                                    on='Date', how='outer', suffixes=('', f'_{ticker}'))\n",
    "\n",
    "    sector_index['Weighted_Index'] = sector_index.filter(like='Weighted_YoY_Growth').sum(axis=1)\n",
    "\n",
    "    # Calculate simple average\n",
    "    simple_avg_index = df.groupby('Date')['YoY_Growth'].mean().reset_index()\n",
    "    sector_index = pd.merge(sector_index, simple_avg_index, on='Date', how='left')\n",
    "    sector_index.rename(columns={'YoY_Growth': 'Simple_Avg_Index'}, inplace=True)\n",
    "\n",
    "    # Calculate the difference between the weighted and simple averages\n",
    "    sector_index['Difference'] = (sector_index['Weighted_Index'] - sector_index['Simple_Avg_Index']).abs()\n",
    "\n",
    "    return sector_index[['Date', 'Weighted_Index', 'Simple_Avg_Index', 'Difference']]\n",
    "\n",
    "def calculate_variance_and_positive_ratio(df):\n",
    "    \"\"\"Calculate variance of YoY growth and the positive-to-negative growth ratio.\"\"\"\n",
    "    variance = df['YoY_Growth'].var()\n",
    "\n",
    "    positive_growth_count = (df['YoY_Growth'] > 0).sum()\n",
    "    negative_growth_count = (df['YoY_Growth'] < 0).sum()\n",
    "\n",
    "    if negative_growth_count > 0:\n",
    "        pos_to_neg_ratio = positive_growth_count / negative_growth_count\n",
    "    else:\n",
    "        pos_to_neg_ratio = float('inf')  # All positive growth\n",
    "\n",
    "    return variance, pos_to_neg_ratio\n",
    "\n",
    "def get_sector_rankings(input_dir, output_file='sector_rankings.csv'):\n",
    "    rankings_variation = {}\n",
    "    rankings_variance = {}\n",
    "    rankings_positive_negative_ratio = {}\n",
    "\n",
    "    for sector_file in os.listdir(input_dir):\n",
    "        if sector_file.endswith(\".csv\"):\n",
    "            sector = sector_file.replace(\"_mkt_cap_quarter_end.csv\", \"\")\n",
    "            print(f\"Processing {sector} sector...\")\n",
    "\n",
    "            # Load the CSV file and filter data for 2019-2024\n",
    "            file_path = os.path.join(input_dir, sector_file)\n",
    "            df = pd.read_csv(file_path, parse_dates=['Date'])\n",
    "            \n",
    "            # Convert 'Date' to UTC\n",
    "            df['Date'] = pd.to_datetime(df['Date'], utc=True)\n",
    "            \n",
    "            # Filter the data from 2019 onwards\n",
    "            df = df[df['Date'] >= pd.Timestamp('2019-01-01', tz='UTC')]\n",
    "\n",
    "            # Calculate YoY growth\n",
    "            df = calculate_yoy_growth(df)\n",
    "\n",
    "            # Calculate sector leader and fractional contributions\n",
    "            sector_leader, sorted_companies, total_overperformance = calculate_sector_leader_and_rank(df, sector)\n",
    "\n",
    "            # Calculate sector index variations\n",
    "            sector_index_variations = calculate_sector_index_variations(df, sorted_companies, total_overperformance)\n",
    "\n",
    "            # Calculate variance of YoY growth and positive/negative growth ratio\n",
    "            variance, pos_to_neg_ratio = calculate_variance_and_positive_ratio(df)\n",
    "\n",
    "            # Summarize the variation between weighted and simple averages\n",
    "            avg_difference = sector_index_variations['Difference'].mean()\n",
    "\n",
    "            # Save results for rankings\n",
    "            rankings_variation[sector] = avg_difference\n",
    "            rankings_variance[sector] = variance\n",
    "            # rankings_positive_negative_ratio[sector] = pos_to_neg_ratio\n",
    "\n",
    "    # Create a DataFrame from the rankings\n",
    "    rankings_df = pd.DataFrame({\n",
    "        'Sector': list(rankings_variation.keys()),\n",
    "        'Variation (Weighted vs Simple Avg)': list(rankings_variation.values()),\n",
    "        'Variance of YoY Growth': list(rankings_variance.values()),\n",
    "        # 'Positive to Negative Growth Ratio': list(rankings_positive_negative_ratio.values())\n",
    "    })\n",
    "\n",
    "    # Save the rankings DataFrame to CSV\n",
    "    rankings_df.to_csv(output_file, index=False)\n",
    "    print(f\"\\nRankings saved to {output_file}\")\n",
    "\n",
    "    # Optionally, print the rankings (as before)\n",
    "    print(\"\\nRanking of sectors based on variation between simple and weighted averages:\")\n",
    "    for sector, value in sorted(rankings_variation.items(), key=lambda x: x[1], reverse=True):\n",
    "        print(f\"{sector}: {value}\")\n",
    "\n",
    "    print(\"\\nRanking of sectors based on variance of YoY growth over time:\")\n",
    "    for sector, value in sorted(rankings_variance.items(), key=lambda x: x[1], reverse=True):\n",
    "        print(f\"{sector}: {value}\")\n",
    "\n",
    "    print(\"\\nRanking of sectors based on ratio of positive to negative growth over time:\")\n",
    "    for sector, value in sorted(rankings_positive_negative_ratio.items(), key=lambda x: x[1], reverse=True):\n",
    "        print(f\"{sector}: {value}\")\n",
    "        \n",
    "    rankings_df=rankings_df\n",
    "\n",
    "def main():\n",
    "    input_dir = \"sector_mkt_cap_results\"\n",
    "    get_sector_rankings(input_dir)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-15T20:19:08.772065Z",
     "start_time": "2024-11-15T20:19:08.755548Z"
    }
   },
   "outputs": [],
   "source": [
    "rankings_df=pd.read_csv('sector_rankings.csv')\n",
    "rankings_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-15T20:19:10.303361Z",
     "start_time": "2024-11-15T20:19:09.506329Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Step 1: Read the CSV file\n",
    "df = pd.read_csv('sector_rankings.csv')\n",
    "\n",
    "# Step 2: Normalize each column independently (excluding the 'Sector' column)\n",
    "scaler = StandardScaler()\n",
    "X = df.drop('Sector', axis=1)  # Keep only numerical data for normalization\n",
    "X_scaled = X.apply(lambda col: scaler.fit_transform(col.values.reshape(-1, 1)).flatten(), axis=0)\n",
    "\n",
    "# Step 3: Perform hierarchical clustering\n",
    "Z = linkage(X_scaled, method='ward')\n",
    "\n",
    "# Step 4: Plot the dendrogram\n",
    "plt.figure(figsize=(12, 20))\n",
    "dendrogram(Z, labels=df['Sector'].values, leaf_rotation=90, leaf_font_size=10)\n",
    "plt.title('Hierarchical Clustering of Sectors')\n",
    "plt.xlabel('Sectors')\n",
    "plt.ylabel('Euclidean Distance')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-15T20:19:13.372Z",
     "start_time": "2024-11-15T20:19:12.584555Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Step 1: Read the CSV file\n",
    "df = pd.read_csv('sector_rankings.csv')\n",
    "\n",
    "# Step 2: Normalize the data (excluding the 'Sector' column)\n",
    "scaler = StandardScaler()\n",
    "X = df.drop('Sector', axis=1)  # Keep only numerical data for normalization\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Step 3: Calculate inertia for different values of k\n",
    "inertia = []\n",
    "k_values = range(1, 11)\n",
    "\n",
    "for k in k_values:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "    kmeans.fit(X_scaled)\n",
    "    inertia.append(kmeans.inertia_)  # Sum of squared distances to closest cluster center\n",
    "\n",
    "# Step 4: Plot the Elbow Method graph\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(k_values, inertia, 'bo-', markersize=8)\n",
    "plt.title('Elbow Method For Optimal k')\n",
    "plt.xlabel('Number of clusters (k)')\n",
    "plt.ylabel('Inertia')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-15T20:19:14.236381Z",
     "start_time": "2024-11-15T20:19:13.991667Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Step 1: Read the CSV file\n",
    "df = pd.read_csv('sector_rankings.csv')\n",
    "\n",
    "# Step 2: Normalize the data (excluding the 'Sector' column)\n",
    "scaler = StandardScaler()\n",
    "X = df.drop('Sector', axis=1)  # Keep only numerical data for normalization\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Step 3: Perform K-means clustering with k=7\n",
    "kmeans = KMeans(n_clusters=4, random_state=42)\n",
    "df['Cluster'] = kmeans.fit_predict(X_scaled)  # Assign clusters to sectors\n",
    "\n",
    "# Step 4: Show the resulting clusters\n",
    "print(df[['Sector', 'Cluster']])\n",
    "\n",
    "# Step 5: Visualize the clusters using a scatter plot (based on the first two principal components for visualization)\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=2)  # Reduce to 2D for visualization\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "# Create a scatter plot with clusters\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.scatterplot(x=X_pca[:, 0], y=X_pca[:, 1], hue=df['Cluster'], palette='Set1', s=100, legend='full')\n",
    "plt.title('K-means Clustering of Sectors (k=7)')\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.legend(title='Cluster')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-15T20:19:14.732455Z",
     "start_time": "2024-11-15T20:19:14.711424Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Step 1: Read the CSV file\n",
    "df = pd.read_csv('sector_rankings.csv')\n",
    "\n",
    "# Step 2: Normalize the data (excluding the 'Sector' column)\n",
    "scaler = StandardScaler()\n",
    "X = df.drop('Sector', axis=1)  # Keep only numerical data for normalization\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Step 3: Perform K-means clustering with k=7\n",
    "kmeans = KMeans(n_clusters=7, random_state=42)\n",
    "df['Cluster'] = kmeans.fit_predict(X_scaled)  # Assign clusters to sectors\n",
    "\n",
    "# Step 4: Sort the dataframe by the 'Cluster' column\n",
    "df_sorted = df.sort_values(by='Cluster')\n",
    "\n",
    "# Step 5: Save the sorted dataframe to a new CSV file\n",
    "df_sorted.to_csv('sector_clusters_sorted.csv', index=False)\n",
    "\n",
    "# Step 6: Display a message indicating successful saving\n",
    "print(\"Cluster details saved to 'sector_clusters_sorted.csv'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-15T20:19:15.881757Z",
     "start_time": "2024-11-15T20:19:15.578343Z"
    }
   },
   "outputs": [],
   "source": [
    "def classify_growth(value):\n",
    "    \"\"\"Classify YoY growth into 1, 0, or -1 based on thresholds.\"\"\"\n",
    "    if value > 5:\n",
    "        return 1\n",
    "    elif value < -5:\n",
    "        return -1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def calculate_yoy_growth(df, column):\n",
    "    \"\"\"Calculate YoY growth and classify based on threshold.\"\"\"\n",
    "    df[f'{column}_YoY_Growth'] = df[column].pct_change(periods=4) * 100  # YoY percentage change\n",
    "    df[f'{column}_Growth_Class'] = df[f'{column}_YoY_Growth'].apply(classify_growth)\n",
    "    df.dropna(subset=[f'{column}_YoY_Growth'], inplace=True)\n",
    "    return df\n",
    "\n",
    "def calculate_beta_growth_covariance(df, period):\n",
    "    \"\"\"Calculate the covariance of beta growth over a specified period.\"\"\"\n",
    "    df[f'Beta_Growth_{period}_M'] = df['Beta'].pct_change(periods=period)\n",
    "    beta_covariance = df[f'Beta_Growth_{period}_M'].cov(df['Beta'])\n",
    "    return beta_covariance\n",
    "\n",
    "def calculate_sector_index_variations(df, sorted_companies, total_overperformance):\n",
    "    \"\"\"Calculate sector index and return variations between weighted and simple average index.\"\"\"\n",
    "    fractional_contribution = {company: count**2 / total_overperformance for company, count in sorted_companies}\n",
    "    \n",
    "    sector_index = pd.DataFrame()\n",
    "\n",
    "    for ticker in df['Ticker'].unique():\n",
    "        company_data = df[df['Ticker'] == ticker].copy()\n",
    "\n",
    "        if ticker in fractional_contribution:\n",
    "            contribution = fractional_contribution[ticker]\n",
    "            company_data['Weighted_YoY_Growth'] = company_data['MarketCap_Growth_Class'] * contribution\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "        if sector_index.empty:\n",
    "            sector_index = company_data[['Date', 'Weighted_YoY_Growth']].copy()\n",
    "        else:\n",
    "            sector_index = pd.merge(sector_index, company_data[['Date', 'Weighted_YoY_Growth']],\n",
    "                                    on='Date', how='outer', suffixes=('', f'_{ticker}'))\n",
    "\n",
    "    sector_index['Weighted_Index'] = sector_index.filter(like='Weighted_YoY_Growth').sum(axis=1)\n",
    "\n",
    "    # Calculate simple average\n",
    "    simple_avg_index = df.groupby('Date')['MarketCap_Growth_Class'].mean().reset_index()\n",
    "    sector_index = pd.merge(sector_index, simple_avg_index, on='Date', how='left')\n",
    "    sector_index.rename(columns={'MarketCap_Growth_Class': 'Simple_Avg_Index'}, inplace=True)\n",
    "\n",
    "    # Calculate the difference between the weighted and simple averages\n",
    "    sector_index['Difference'] = (sector_index['Weighted_Index'] - sector_index['Simple_Avg_Index']).abs()\n",
    "\n",
    "    return sector_index[['Date', 'Weighted_Index', 'Simple_Avg_Index', 'Difference']]\n",
    "\n",
    "def calculate_variance_and_covariances(df):\n",
    "    \"\"\"Calculate variance of YoY growth and the covariances of beta growth.\"\"\"\n",
    "    # Variance of YoY growth in market cap\n",
    "    variance = df['MarketCap_Growth_Class'].var()\n",
    "\n",
    "    # Covariance for 6 months and 5 years of beta growth\n",
    "    beta_cov_6m = calculate_beta_growth_covariance(df, period=6)\n",
    "    beta_cov_5y = calculate_beta_growth_covariance(df, period=20)  # Assuming 5 years corresponds to approx. 20 quarters\n",
    "\n",
    "    return variance, beta_cov_6m, beta_cov_5y\n",
    "\n",
    "def get_sector_rankings(input_dir, output_file='sector_rankings.csv'):\n",
    "    rankings_variation = {}\n",
    "    rankings_variance = {}\n",
    "    rankings_cov_6m_beta = {}\n",
    "    rankings_cov_5y_beta = {}\n",
    "\n",
    "    for sector_file in os.listdir(input_dir):\n",
    "        if sector_file.endswith(\".csv\"):\n",
    "            sector = sector_file.replace(\"_mkt_cap_quarter_end.csv\", \"\")\n",
    "            print(f\"Processing {sector} sector...\")\n",
    "\n",
    "            # Load the CSV file and filter data for 2019-2024\n",
    "            file_path = os.path.join(input_dir, sector_file)\n",
    "            df = pd.read_csv(file_path, parse_dates=['Date'])\n",
    "            \n",
    "            # Convert 'Date' to UTC\n",
    "            df['Date'] = pd.to_datetime(df['Date'], utc=True)\n",
    "            \n",
    "            # Filter the data from 2019 onwards\n",
    "            df = df[df['Date'] >= pd.Timestamp('2019-01-01', tz='UTC')]\n",
    "\n",
    "            # Calculate YoY growth for MarketCap and Revenue\n",
    "            df = calculate_yoy_growth(df, 'MarketCap')\n",
    "            df = calculate_yoy_growth(df, 'Revenue')\n",
    "\n",
    "            # Calculate sector leader and fractional contributions\n",
    "            sector_leader, sorted_companies, total_overperformance = calculate_sector_leader_and_rank(df, sector)\n",
    "\n",
    "            # Calculate sector index variations\n",
    "            sector_index_variations = calculate_sector_index_variations(df, sorted_companies, total_overperformance)\n",
    "\n",
    "            # Calculate variance of YoY growth and beta covariances\n",
    "            variance, beta_cov_6m, beta_cov_5y = calculate_variance_and_covariances(df)\n",
    "\n",
    "            # Summarize the variation between weighted and simple averages\n",
    "            avg_difference = sector_index_variations['Difference'].mean()\n",
    "\n",
    "            # Save results for rankings\n",
    "            rankings_variation[sector] = avg_difference\n",
    "            rankings_variance[sector] = variance\n",
    "            rankings_cov_6m_beta[sector] = beta_cov_6m\n",
    "            rankings_cov_5y_beta[sector] = beta_cov_5y\n",
    "\n",
    "    # Create a DataFrame from the rankings\n",
    "    rankings_df = pd.DataFrame({\n",
    "        'Sector': list(rankings_variation.keys()),\n",
    "        'Variation (Weighted vs Simple Avg)': list(rankings_variation.values()),\n",
    "        'Variance of YoY Growth': list(rankings_variance.values()),\n",
    "        'Covariance of 6M Beta Growth': list(rankings_cov_6m_beta.values()),\n",
    "        'Covariance of 5Y Beta Growth': list(rankings_cov_5y_beta.values())\n",
    "    })\n",
    "\n",
    "    # Save the rankings DataFrame to CSV\n",
    "    rankings_df.to_csv(output_file, index=False)\n",
    "    print(f\"\\nRankings saved to {output_file}\")\n",
    "\n",
    "    # Optionally, print the rankings (as before)\n",
    "    print(\"\\nRanking of sectors based on variation between simple and weighted averages:\")\n",
    "    for sector, value in sorted(rankings_variation.items(), key=lambda x: x[1], reverse=True):\n",
    "        print(f\"{sector}: {value}\")\n",
    "\n",
    "    print(\"\\nRanking of sectors based on variance of YoY growth over time:\")\n",
    "    for sector, value in sorted(rankings_variance.items(), key=lambda x: x[1], reverse=True):\n",
    "        print(f\"{sector}: {value}\")\n",
    "\n",
    "    print(\"\\nRanking of sectors based on covariance of 6M Beta Growth:\")\n",
    "    for sector, value in sorted(rankings_cov_6m_beta.items(), key=lambda x: x[1], reverse=True):\n",
    "        print(f\"{sector}: {value}\")\n",
    "\n",
    "    print(\"\\nRanking of sectors based on covariance of 5Y Beta Growth:\")\n",
    "    for sector, value in sorted(rankings_cov_5y_beta.items(), key=lambda x: x[1], reverse=True):\n",
    "        print(f\"{sector}: {value}\")\n",
    "\n",
    "def main():\n",
    "    input_dir = \"merged_sector_data\"\n",
    "    get_sector_rankings(input_dir)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-15T20:20:08.024427Z",
     "start_time": "2024-11-15T20:20:00.522259Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import seaborn as sns\n",
    "\n",
    "def merge_sector_data(mkt_cap_dir, revenue_dir, output_dir=\"merged_sector_data\"):\n",
    "    \"\"\"\n",
    "    Merge market cap and revenue data for sectors where both datasets are available.\n",
    "    Handles specific CSV structures with quarterly market cap and yearly revenue data.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    \n",
    "    merged_sectors = {}\n",
    "    \n",
    "    # Get list of sectors from both directories\n",
    "    mkt_cap_sectors = {f.split('_mkt_cap')[0] for f in os.listdir(mkt_cap_dir) if f.endswith('.csv')}\n",
    "    revenue_sectors = {f.split('_revenue')[0] for f in os.listdir(revenue_dir) if f.endswith('.csv')}\n",
    "    \n",
    "    # Find common sectors\n",
    "    common_sectors = mkt_cap_sectors.intersection(revenue_sectors)\n",
    "    \n",
    "    for sector in common_sectors:\n",
    "        print(f\"Processing sector: {sector}\")\n",
    "        \n",
    "        # Read market cap data\n",
    "        mkt_cap_file = f\"{sector}_mkt_cap_quarter_end.csv\"\n",
    "        mkt_cap_path = os.path.join(mkt_cap_dir, mkt_cap_file)\n",
    "        mkt_cap_df = pd.read_csv(mkt_cap_path)\n",
    "        \n",
    "        # Read revenue data\n",
    "        revenue_file = f\"{sector}_revenue.csv\"\n",
    "        revenue_path = os.path.join(revenue_dir, revenue_file)\n",
    "        revenue_df = pd.read_csv(revenue_path)\n",
    "        \n",
    "        # Clean and convert dates\n",
    "        try:\n",
    "            # Handle the specific date format in market cap data\n",
    "            mkt_cap_df['Date'] = pd.to_datetime(mkt_cap_df['Date'].str.split(' ').str[0])\n",
    "        except AttributeError:\n",
    "            mkt_cap_df['Date'] = pd.to_datetime(mkt_cap_df['Date'])\n",
    "            \n",
    "        revenue_df['date'] = pd.to_datetime(revenue_df['date'])\n",
    "        \n",
    "        # Extract year and quarter\n",
    "        mkt_cap_df['year'] = pd.DatetimeIndex(mkt_cap_df['Date']).year\n",
    "        mkt_cap_df['quarter'] = pd.DatetimeIndex(mkt_cap_df['Date']).quarter\n",
    "        \n",
    "        # Create a list to store merged data for each ticker\n",
    "        merged_data = []\n",
    "        \n",
    "        # Get unique tickers from both datasets\n",
    "        mkt_cap_df['Ticker'] = mkt_cap_df['Ticker'].str.upper()\n",
    "        revenue_df['ticker'] = revenue_df['ticker'].str.upper()\n",
    "        \n",
    "        common_tickers = set(mkt_cap_df['Ticker']).intersection(set(revenue_df['ticker']))\n",
    "        \n",
    "        print(f\"Found {len(common_tickers)} common tickers for {sector}\")\n",
    "        \n",
    "        for ticker in common_tickers:\n",
    "            ticker_mkt_cap = mkt_cap_df[mkt_cap_df['Ticker'] == ticker].copy()\n",
    "            ticker_revenue = revenue_df[revenue_df['ticker'] == ticker].copy()\n",
    "            \n",
    "            for _, mkt_cap_row in ticker_mkt_cap.iterrows():\n",
    "                matching_revenue = ticker_revenue[\n",
    "                    (ticker_revenue['year'] == mkt_cap_row['year']) &\n",
    "                    (ticker_revenue['quarter'] == mkt_cap_row['quarter'])\n",
    "                ]\n",
    "                \n",
    "                if matching_revenue.empty:\n",
    "                    yearly_revenue = ticker_revenue[\n",
    "                        ticker_revenue['year'] == mkt_cap_row['year']\n",
    "                    ]\n",
    "                    if not yearly_revenue.empty:\n",
    "                        revenue_value = yearly_revenue.iloc[-1]['revenue']\n",
    "                        revenue_growth = yearly_revenue.iloc[-1].get('revenue_yoy_growth', np.nan)\n",
    "                    else:\n",
    "                        continue\n",
    "                else:\n",
    "                    revenue_value = matching_revenue.iloc[0]['revenue']\n",
    "                    revenue_growth = matching_revenue.iloc[0].get('revenue_yoy_growth', np.nan)\n",
    "                \n",
    "                merged_row = {\n",
    "                    'Date': mkt_cap_row['Date'],\n",
    "                    'Year': mkt_cap_row['year'],\n",
    "                    'Quarter': mkt_cap_row['quarter'],\n",
    "                    'Ticker': ticker,\n",
    "                    'MarketCap': mkt_cap_row['MarketCap'],\n",
    "                    'Revenue': revenue_value,\n",
    "                    'Revenue_YoY_Growth': revenue_growth,\n",
    "                    'Company_Name': ticker_revenue.iloc[0].get('company_name', ticker)\n",
    "                }\n",
    "                merged_data.append(merged_row)\n",
    "        \n",
    "        if merged_data:\n",
    "            merged_df = pd.DataFrame(merged_data)\n",
    "            merged_df = merged_df.sort_values(['Date', 'Ticker'])\n",
    "            merged_sectors[sector] = merged_df\n",
    "            \n",
    "            # Save merged data to a CSV file\n",
    "            output_file_path = os.path.join(output_dir, f\"{sector}_merged_data.csv\")\n",
    "            merged_df.to_csv(output_file_path, index=False)\n",
    "            print(f\"Successfully merged data for {sector} and saved to {output_file_path}\")\n",
    "        else:\n",
    "            print(f\"No matching data found for {sector}\")\n",
    "        \n",
    "    return merged_sectors\n",
    "\n",
    "def calculate_growth_indicator(value):\n",
    "    \"\"\"Convert growth percentage to indicator: 1 (>5%), -1 (<-5%), 0 (between -5% and 5%)\"\"\"\n",
    "    if pd.isna(value):\n",
    "        return 0\n",
    "    if value > 5:\n",
    "        return 1\n",
    "    elif value < -5:\n",
    "        return -1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def calculate_beta_covariance(df, period_months):\n",
    "    \"\"\"\n",
    "    Calculate covariance of beta over a specified period with improved handling of time series.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = df.copy()\n",
    "        \n",
    "        # Ensure data is sorted by date\n",
    "        df = df.sort_values(['Date', 'Ticker'])\n",
    "        \n",
    "        # Calculate returns for each company\n",
    "        df['Returns'] = df.groupby('Ticker')['MarketCap'].pct_change()\n",
    "        \n",
    "        # Calculate market returns (using value-weighted market return)\n",
    "        df['Market_Value'] = df.groupby('Date')['MarketCap'].transform('sum')\n",
    "        df['Market_Weight'] = df['MarketCap'] / df['Market_Value']\n",
    "        df['Market_Returns'] = df.groupby('Date')['Returns'].transform(lambda x: (x * df.loc[x.index, 'Market_Weight']).sum())\n",
    "        \n",
    "        # Set minimum periods for rolling calculations\n",
    "        min_periods = max(2, period_months - 1)  # Ensure at least 2 periods for correlation\n",
    "        rolling_window = period_months * 3  # Convert months to quarters (assuming quarterly data)\n",
    "        \n",
    "        betas_by_date = []\n",
    "        \n",
    "        for ticker in df['Ticker'].unique():\n",
    "            ticker_data = df[df['Ticker'] == ticker].copy()\n",
    "            \n",
    "            if len(ticker_data) >= min_periods:\n",
    "                # Calculate rolling betas\n",
    "                rolling_cov = (\n",
    "                    ticker_data['Returns']\n",
    "                    .rolling(window=rolling_window, min_periods=min_periods)\n",
    "                    .cov(ticker_data['Market_Returns'])\n",
    "                )\n",
    "                \n",
    "                rolling_market_var = (\n",
    "                    ticker_data['Market_Returns']\n",
    "                    .rolling(window=rolling_window, min_periods=min_periods)\n",
    "                    .var()\n",
    "                )\n",
    "                \n",
    "                # To avoid dividing by zero, handle NaN or zero variance values\n",
    "                ticker_data['Beta'] = rolling_cov / rolling_market_var.replace(0, np.nan)\n",
    "                \n",
    "                # Store results\n",
    "                betas_by_date.append(ticker_data[['Date', 'Ticker', 'Beta']].dropna())\n",
    "        \n",
    "        if not betas_by_date:\n",
    "            return 0\n",
    "        \n",
    "        # Combine all beta calculations\n",
    "        all_betas = pd.concat(betas_by_date)\n",
    "        \n",
    "        # Create a pivot table of betas (companies x dates)\n",
    "        beta_matrix = all_betas.pivot_table(\n",
    "            index='Ticker',\n",
    "            columns='Date',\n",
    "            values='Beta',\n",
    "            aggfunc='first'\n",
    "        )\n",
    "        \n",
    "        # Remove companies with too many missing values\n",
    "        min_observations = beta_matrix.shape[1] * 0.5  # Require at least 50% of dates\n",
    "        beta_matrix = beta_matrix[beta_matrix.count(axis=1) >= min_observations]\n",
    "        \n",
    "        if beta_matrix.empty:\n",
    "            return 0\n",
    "        \n",
    "        # Fill remaining NaN values with forward fill then backward fill\n",
    "        beta_matrix = beta_matrix.fillna(method='ffill', axis=1).fillna(method='bfill', axis=1)\n",
    "        \n",
    "        # Calculate covariance between different dates\n",
    "        cov_matrix = beta_matrix.T.cov()\n",
    "        \n",
    "        # Calculate average absolute covariance (excluding diagonal)\n",
    "        mask = ~np.eye(cov_matrix.shape[0], dtype=bool)\n",
    "        avg_cov = np.abs(cov_matrix.where(mask)).mean().mean()\n",
    "        \n",
    "        return float(avg_cov) if not np.isnan(avg_cov) else 0\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in beta covariance calculation: {e}\")\n",
    "        return 0\n",
    "    \n",
    "def plot_covariance_heatmap(cov_matrix, title='Sector Covariance Heatmap'):\n",
    "    \"\"\"\n",
    "    Plot a heatmap based on the covariance matrix.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.heatmap(cov_matrix, annot=True, cmap=\"coolwarm\", fmt=\".2f\", cbar_kws={'label': 'Covariance'})\n",
    "    plt.title(title)\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.yticks(rotation=0)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def calculate_sector_rankings(merged_sectors, output_file='sector_rankings.csv'):\n",
    "    \"\"\"Calculate and rank sectors based on the five specified parameters\"\"\"\n",
    "    rankings = []\n",
    "    \n",
    "    for sector, df in merged_sectors.items():\n",
    "        print(f\"Processing sector: {sector}\")\n",
    "        try:\n",
    "            # Ensure data is sorted chronologically\n",
    "            df = df.sort_values('Date')\n",
    "            \n",
    "            # 1. Market Cap YoY Growth Indicator\n",
    "            df['MktCap_YoY_Change'] = df.groupby('Ticker')['MarketCap'].pct_change(periods=4) * 100\n",
    "            df['MktCap_Growth_Indicator'] = df['MktCap_YoY_Change'].apply(calculate_growth_indicator)\n",
    "            \n",
    "            # 2. Revenue YoY Growth Indicator\n",
    "            df['Revenue_Growth_Indicator'] = df['Revenue_YoY_Growth'].apply(calculate_growth_indicator)\n",
    "            \n",
    "            # 3. Variance between weighted and simple average\n",
    "            df['Weighted_MktCap_Change'] = (\n",
    "                df['MktCap_YoY_Change'] * \n",
    "                df['MarketCap'] / \n",
    "                df.groupby('Date')['MarketCap'].transform('sum')\n",
    "            )\n",
    "            \n",
    "            # Calculate averages only for non-NaN values\n",
    "            weighted_avg = df.groupby('Date')['Weighted_MktCap_Change'].sum().mean()\n",
    "            simple_avg = df['MktCap_YoY_Change'].mean()\n",
    "            variance_avg = abs(weighted_avg - simple_avg)\n",
    "            \n",
    "            # 4 & 5. Beta covariances\n",
    "            print(f\"Calculating 6-month beta covariance for {sector}\")\n",
    "            beta_6m_cov = calculate_beta_covariance(df, 2)\n",
    "            \n",
    "            print(f\"Calculating 5-year beta covariance for {sector}\")\n",
    "            beta_4y_cov = calculate_beta_covariance(df, 16)\n",
    "            \n",
    "            rankings.append({\n",
    "                'Sector': sector,\n",
    "                'MktCap_Growth_Score': df['MktCap_Growth_Indicator'].mean(),\n",
    "                'Revenue_Growth_Score': df['Revenue_Growth_Indicator'].mean(),\n",
    "                'Weighted_Simple_Variance': variance_avg,\n",
    "                'Beta_6M_Covariance': beta_6m_cov,\n",
    "                'Beta_4Y_Covariance': beta_4y_cov,\n",
    "                'Number_of_Companies': len(df['Ticker'].unique()),\n",
    "                'Date_Range': f\"{df['Date'].min().strftime('%Y-%m-%d')} to {df['Date'].max().strftime('%Y-%m-%d')}\"\n",
    "            })\n",
    "            \n",
    "            print(f\"Successfully processed {sector}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing sector {sector}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    # Create rankings DataFrame and save to CSV\n",
    "    rankings_df = pd.DataFrame(rankings)\n",
    "    rankings_df.to_csv(output_file, index=False)\n",
    "    return rankings_df\n",
    "\n",
    "def main():\n",
    "    mkt_cap_dir = \"sector_mkt_cap_results\"\n",
    "    revenue_dir = \"sector_revenue_results\"\n",
    "    \n",
    "    # Merge sector data\n",
    "    merged_sectors = merge_sector_data(mkt_cap_dir, revenue_dir)\n",
    "    \n",
    "    # Calculate rankings with new parameters\n",
    "    rankings_df = calculate_sector_rankings(merged_sectors)\n",
    "    \n",
    "    # Print rankings for each parameter\n",
    "    parameters = ['MktCap_Growth_Score', 'Revenue_Growth_Score', 'Weighted_Simple_Variance', \n",
    "                 'Beta_6M_Covariance', 'Beta_4Y_Covariance']\n",
    "    \n",
    "    for param in parameters:\n",
    "        print(f\"\\nRanking of sectors based on {param}:\")\n",
    "        sorted_rankings = rankings_df.sort_values(param, ascending=False)\n",
    "        for _, row in sorted_rankings.iterrows():\n",
    "            print(f\"{row['Sector']} ({row['Number_of_Companies']} companies): {row[param]:.4f}\")\n",
    "            print(f\"Date Range: {row['Date_Range']}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-15T20:21:06.014370Z",
     "start_time": "2024-11-15T20:21:05.936477Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('sector_rankings.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-15T20:21:09.035411Z",
     "start_time": "2024-11-15T20:21:08.999394Z"
    }
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-15T20:21:18.732731Z",
     "start_time": "2024-11-15T20:21:17.304697Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Step 1: Read the CSV file\n",
    "df = pd.read_csv('sector_rankings.csv')\n",
    "\n",
    "# Step 2: Normalize each column independently (excluding the 'Sector' column)\n",
    "scaler = StandardScaler()\n",
    "df = df.drop(columns=['Number_of_Companies', 'Date_Range'])\n",
    "X = df.drop('Sector', axis=1)  # Keep only numerical data for normalization\n",
    "print(df.shape)\n",
    "X_scaled = X.apply(lambda col: scaler.fit_transform(col.values.reshape(-1, 1)).flatten(), axis=0)\n",
    "\n",
    "# Step 3: Perform hierarchical clustering\n",
    "Z = linkage(X_scaled, method='ward')\n",
    "\n",
    "# Step 4: Plot the dendrogram\n",
    "plt.figure(figsize=(12, 20))\n",
    "dendrogram(Z, labels=df['Sector'].values, leaf_rotation=90, leaf_font_size=10)\n",
    "plt.title('Hierarchical Clustering of Sectors')\n",
    "plt.xlabel('Sectors')\n",
    "plt.ylabel('Euclidean Distance')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-15T20:21:28.897522Z",
     "start_time": "2024-11-15T20:21:24.897740Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from minisom import MiniSom\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import KLDivergence\n",
    "import tensorflow as tf\n",
    "from collections import Counter\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Load data\n",
    "data = pd.read_csv('sector_rankings.csv')\n",
    "params = data[['MktCap_Growth_Score', 'Revenue_Growth_Score', 'Weighted_Simple_Variance', 'Beta_6M_Covariance', 'Beta_4Y_Covariance']]  # Replace with actual column names\n",
    "\n",
    "# Scale the data\n",
    "scaler = StandardScaler()\n",
    "params_scaled = scaler.fit_transform(params)\n",
    "\n",
    "# Initialize lists to store results\n",
    "cluster_results = []\n",
    "clusters_range = list(range(3, 8))\n",
    "\n",
    "# Function to run Autoencoder + K-means\n",
    "def kmeans_clustering(n_clusters, params_scaled):\n",
    "    try:\n",
    "        # Define the autoencoder\n",
    "        input_dim = params_scaled.shape[1]\n",
    "        encoding_dim = 3\n",
    "        input_layer = Input(shape=(input_dim,))\n",
    "        encoder = Dense(encoding_dim, activation=\"relu\")(input_layer)\n",
    "        decoder = Dense(input_dim, activation=\"sigmoid\")(encoder)\n",
    "        autoencoder = Model(inputs=input_layer, outputs=decoder)\n",
    "        autoencoder.compile(optimizer=\"adam\", loss=\"mse\")\n",
    "        autoencoder.fit(params_scaled, params_scaled, epochs=50, batch_size=16, shuffle=True)\n",
    "\n",
    "        # Extract encoder and apply K-means clustering\n",
    "        encoder_model = Model(inputs=input_layer, outputs=encoder)\n",
    "        encoded_data = encoder_model.predict(params_scaled)\n",
    "        \n",
    "        kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "        clusters = kmeans.fit_predict(encoded_data)\n",
    "        \n",
    "        return clusters\n",
    "    except Exception as e:\n",
    "        print(f\"Error in KMeans clustering with {n_clusters} clusters: {e}\")\n",
    "        return None\n",
    "\n",
    "# Function to run Self-Organizing Map (SOM)\n",
    "def som_clustering(n_clusters, params_scaled):\n",
    "    try:\n",
    "        som_size = 10  # Grid size of SOM\n",
    "        som = MiniSom(x=som_size, y=som_size, input_len=params_scaled.shape[1], sigma=1.0, learning_rate=0.5)\n",
    "        som.random_weights_init(params_scaled)\n",
    "        som.train_random(params_scaled, 100)\n",
    "\n",
    "        # Assign clusters based on the winning neuron\n",
    "        clusters = []\n",
    "        for sample in params_scaled:\n",
    "            x, y = som.winner(sample)\n",
    "            clusters.append((x * som_size) + y)\n",
    "\n",
    "        return clusters\n",
    "    except Exception as e:\n",
    "        print(f\"Error in SOM clustering with {n_clusters} clusters: {e}\")\n",
    "        return None\n",
    "\n",
    "# Function to run Deep Embedded Clustering (DEC)\n",
    "def dec_clustering(n_clusters, params_scaled):\n",
    "    try:\n",
    "        # Define the autoencoder architecture for DEC\n",
    "        input_dim = params_scaled.shape[1]\n",
    "        encoding_dim = 3\n",
    "        input_layer = Input(shape=(input_dim,))\n",
    "        encoder = Dense(encoding_dim, activation='relu')(input_layer)\n",
    "        decoder = Dense(input_dim, activation='sigmoid')(encoder)\n",
    "        autoencoder = Model(inputs=input_layer, outputs=decoder)\n",
    "        autoencoder.compile(optimizer='adam', loss='mse')\n",
    "        autoencoder.fit(params_scaled, params_scaled, epochs=50, batch_size=16, shuffle=True)\n",
    "\n",
    "        encoder_model = Model(inputs=input_layer, outputs=encoder)\n",
    "        encoded_data = encoder_model.predict(params_scaled)\n",
    "\n",
    "        clustering_layer = Dense(n_clusters, activation='softmax', name='clustering')(encoder)\n",
    "        dec_model = Model(inputs=input_layer, outputs=clustering_layer)\n",
    "        dec_model.compile(optimizer=Adam(learning_rate=0.001), loss=KLDivergence())\n",
    "\n",
    "        initial_clusters = np.random.randint(0, n_clusters, size=params_scaled.shape[0])\n",
    "\n",
    "        # DEC training loop\n",
    "        for epoch in range(100):\n",
    "            cluster_probs = dec_model.predict(params_scaled)\n",
    "            soft_labels = cluster_probs\n",
    "            loss = dec_model.train_on_batch(params_scaled, soft_labels)\n",
    "            \n",
    "            if epoch % 10 == 0:\n",
    "                print(f'Epoch {epoch}/{100}, Loss: {loss}')\n",
    "\n",
    "        final_clusters = np.argmax(dec_model.predict(params_scaled), axis=1)\n",
    "        return final_clusters\n",
    "    except Exception as e:\n",
    "        print(f\"Error in DEC clustering with {n_clusters} clusters: {e}\")\n",
    "        return None\n",
    "\n",
    "# Run 50 times to collect clustering results\n",
    "for run in range(3):\n",
    "    print(f\"Running iteration {run+1}/50\")\n",
    "\n",
    "    run_results = {}\n",
    "\n",
    "    # For each number of clusters in the range, run KMeans, SOM, and DEC clustering\n",
    "    for n_clusters in clusters_range:\n",
    "        print(f\"Evaluating for {n_clusters} clusters:\")\n",
    "\n",
    "        # K-means clustering and get the clusters\n",
    "        kmeans_clusters = kmeans_clustering(n_clusters, params_scaled)\n",
    "        if kmeans_clusters is not None:\n",
    "            run_results['kmeans'] = tuple(kmeans_clusters)  # Store the cluster assignments\n",
    "\n",
    "        # SOM clustering and get the clusters\n",
    "        som_clusters = som_clustering(n_clusters, params_scaled)\n",
    "        if som_clusters is not None:\n",
    "            run_results['som'] = tuple(som_clusters)\n",
    "\n",
    "        # DEC clustering and get the clusters\n",
    "        dec_clusters = dec_clustering(n_clusters, params_scaled)\n",
    "        if dec_clusters is not None:\n",
    "            run_results['dec'] = tuple(dec_clusters)\n",
    "\n",
    "    # Append the results for this run to cluster_results\n",
    "    cluster_results.append(run_results)\n",
    "\n",
    "# Count the frequency of each unique cluster configuration\n",
    "cluster_frequencies = {}\n",
    "\n",
    "for result in cluster_results:\n",
    "    for method, clusters in result.items():\n",
    "        if clusters in cluster_frequencies:\n",
    "            cluster_frequencies[clusters] += 1\n",
    "        else:\n",
    "            cluster_frequencies[clusters] = 1\n",
    "\n",
    "# Find the most frequent configuration\n",
    "max_frequency = max(cluster_frequencies.values())\n",
    "winning_clusters = [clusters for clusters, freq in cluster_frequencies.items() if freq == max_frequency]\n",
    "\n",
    "# Calculate the percentage of times the winner configuration occurred\n",
    "winning_percentage = (max_frequency / 50) * 100\n",
    "\n",
    "print(f\"Winning clustering configuration occurred {max_frequency} times, i.e., {winning_percentage:.2f}% of the time.\")\n",
    "print(f\"Winner configurations: {winning_clusters}\")\n",
    "\n",
    "# Plot the results (silhouette score for each method)\n",
    "plt.figure(figsize=(10, 6))\n",
    "for n_clusters in clusters_range:\n",
    "    silhouette_kmeans = []\n",
    "    silhouette_som = []\n",
    "    silhouette_dec = []\n",
    "    for run in range(5):\n",
    "        result = cluster_results[run]\n",
    "        if 'kmeans' in result and len(result['kmeans']) > 0:\n",
    "            silhouette_kmeans.append(silhouette_score(params_scaled, result['kmeans']))\n",
    "        if 'som' in result and len(result['som']) > 0:\n",
    "            silhouette_som.append(silhouette_score(params_scaled, result['som']))\n",
    "        if 'dec' in result and len(result['dec']) > 0:\n",
    "            silhouette_dec.append(silhouette_score(params_scaled, result['dec']))\n",
    "\n",
    "    # Calculate average silhouette scores for each method\n",
    "    avg_silhouette_kmeans = np.mean(silhouette_kmeans) if silhouette_kmeans else 0\n",
    "    avg_silhouette_som = np.mean(silhouette_som) if silhouette_som else 0\n",
    "    avg_silhouette_dec = np.mean(silhouette_dec) if silhouette_dec else 0\n",
    "\n",
    "    # Plot the results\n",
    "    plt.plot(clusters_range, avg_silhouette_kmeans, label='Autoencoder + K-Means', marker='o')\n",
    "    plt.plot(clusters_range, avg_silhouette_som, label='SOM', marker='s')\n",
    "    plt.plot(clusters_range, avg_silhouette_dec, label='DEC', marker='^')\n",
    "\n",
    "plt.xlabel('Number of Clusters')\n",
    "plt.ylabel('Average Silhouette Score')\n",
    "plt.title('Average Silhouette Scores for Different Clustering Algorithms (50 Runs)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-15T20:19:17.151244Z",
     "start_time": "2024-11-11T16:28:57.604780Z"
    }
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Initialize a list to store the optimal cluster count for each run\n",
    "optimal_clusters = []\n",
    "\n",
    "# Number of runs\n",
    "num_runs = 20\n",
    "\n",
    "for i in range(num_runs):\n",
    "    silhouette_scores = {}\n",
    "\n",
    "    for n_clusters in clusters_range:\n",
    "        # Perform K-means clustering with Autoencoder and calculate silhouette score\n",
    "        score = kmeans_clustering(n_clusters, params_scaled)\n",
    "        if score is not None:\n",
    "            silhouette_scores[n_clusters] = score\n",
    "\n",
    "    # Find the number of clusters with the highest silhouette score for this run\n",
    "    optimal_cluster = max(silhouette_scores, key=silhouette_scores.get)\n",
    "    optimal_clusters.append(optimal_cluster)\n",
    "\n",
    "# Count the frequency of each optimal cluster count\n",
    "optimal_cluster_counts = Counter(optimal_clusters)\n",
    "\n",
    "# Plot a histogram of the optimal cluster counts\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(optimal_clusters, bins=len(clusters_range), edgecolor='black', alpha=0.7)\n",
    "plt.xlabel('Optimal Number of Clusters')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Optimal Cluster Counts (Autoencoder + K-Means)')\n",
    "plt.xticks(clusters_range)\n",
    "plt.grid(axis='y')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-15T20:19:17.152711Z",
     "start_time": "2024-11-11T17:06:59.355805Z"
    }
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Initialize a list to store the optimal cluster count for each run\n",
    "optimal_clusters = []\n",
    "\n",
    "# Number of runs\n",
    "num_runs = 50\n",
    "\n",
    "for i in range(num_runs):\n",
    "    silhouette_scores = {}\n",
    "\n",
    "    for n_clusters in clusters_range:\n",
    "        # Perform K-means clustering with Autoencoder and calculate silhouette score\n",
    "        score = kmeans_clustering(n_clusters, params_scaled)\n",
    "        if score is not None:\n",
    "            silhouette_scores[n_clusters] = score\n",
    "\n",
    "    # Find the number of clusters with the highest silhouette score for this run\n",
    "    optimal_cluster = max(silhouette_scores, key=silhouette_scores.get)\n",
    "    optimal_clusters.append(optimal_cluster)\n",
    "\n",
    "# Count the frequency of each optimal cluster count\n",
    "optimal_cluster_counts = Counter(optimal_clusters)\n",
    "\n",
    "# Plot a histogram of the optimal cluster counts\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(optimal_clusters, bins=len(clusters_range), edgecolor='black', alpha=0.7)\n",
    "plt.xlabel('Optimal Number of Clusters')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Optimal Cluster Counts (Autoencoder + K-Means)')\n",
    "plt.xticks(clusters_range)\n",
    "plt.grid(axis='y')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-15T20:19:17.154986Z",
     "start_time": "2024-11-11T17:24:58.587376Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "import numpy as np\n",
    "\n",
    "# Step 1: Load and preprocess the data\n",
    "df = pd.read_csv('sector_rankings.csv')\n",
    "df = df.drop(columns=['Number_of_Companies', 'Date_Range'])  # Drop unnecessary columns\n",
    "\n",
    "# Separate 'Sector' column for labeling\n",
    "sectors = df['Sector'].values\n",
    "X = df.drop('Sector', axis=1)\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Step 2: Build the autoencoder\n",
    "input_dim = X_scaled.shape[1]\n",
    "encoding_dim = 5  # Adjust based on desired compression level\n",
    "\n",
    "# Encoder\n",
    "input_layer = Input(shape=(input_dim,))\n",
    "encoded = Dense(encoding_dim, activation='relu')(input_layer)\n",
    "\n",
    "# Decoder\n",
    "decoded = Dense(input_dim, activation='linear')(encoded)\n",
    "\n",
    "# Autoencoder model\n",
    "autoencoder = Model(inputs=input_layer, outputs=decoded)\n",
    "autoencoder.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# Train the autoencoder\n",
    "autoencoder.fit(X_scaled, X_scaled, epochs=50, batch_size=8, shuffle=True, validation_split=0.2)\n",
    "\n",
    "# Encoder model to get the compressed representation\n",
    "encoder = Model(inputs=input_layer, outputs=encoded)\n",
    "\n",
    "# Step 3: Run clustering multiple times and record clusters\n",
    "num_iterations = 1000\n",
    "num_clusters = 5  # Set desired number of clusters\n",
    "all_clusterings = []\n",
    "\n",
    "for i in range(num_iterations):\n",
    "    # Get the encoded (compressed) data\n",
    "    X_encoded = encoder.predict(X_scaled)\n",
    "    \n",
    "    # Perform KMeans clustering on the encoded data\n",
    "    kmeans = KMeans(n_clusters=num_clusters, random_state=i)  # Use a different random state each time\n",
    "    clusters = kmeans.fit_predict(X_encoded)\n",
    "    \n",
    "    # Store the clustering result\n",
    "    all_clusterings.append(clusters)\n",
    "\n",
    "# Convert clustering results into a DataFrame for easier analysis\n",
    "cluster_df = pd.DataFrame(all_clusterings).T\n",
    "cluster_df.columns = [f'Iteration_{i+1}' for i in range(num_iterations)]\n",
    "cluster_df.index = sectors\n",
    "\n",
    "# Step 4: Analyze consistency of clusters across iterations\n",
    "# Find sectors that are clustered together in every iteration\n",
    "consistent_pairs = []\n",
    "for i in range(len(sectors)):\n",
    "    for j in range(i + 1, len(sectors)):\n",
    "        sector1, sector2 = sectors[i], sectors[j]\n",
    "        \n",
    "        # Check if the two sectors are clustered together in every iteration\n",
    "        consistently_clustered = all((cluster_df.iloc[i] == cluster_df.iloc[j]).values)\n",
    "        \n",
    "        if consistently_clustered:\n",
    "            consistent_pairs.append((sector1, sector2))\n",
    "\n",
    "# Count the number of unique sectors that were consistent in every clustering\n",
    "consistent_sectors = set([sector for pair in consistent_pairs for sector in pair])\n",
    "inconsistent_sectors = set(sectors) - consistent_sectors\n",
    "\n",
    "# Step 5: Output results\n",
    "print(\"Sectors that were consistently clustered together in all iterations:\")\n",
    "if consistent_sectors:\n",
    "    print(\", \".join(consistent_sectors))\n",
    "else:\n",
    "    print(\"No sectors were consistently clustered together every time.\")\n",
    "\n",
    "print(\"\\nNumber of sectors that were consistent:\", len(consistent_sectors))\n",
    "print(\"Number of sectors that were not consistent:\", len(inconsistent_sectors))\n",
    "print(\"Inconsistent sectors:\", \", \".join(inconsistent_sectors))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-15T20:19:17.155399Z",
     "start_time": "2024-11-12T11:12:36.009828Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from fcmeans import FCM\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")  # Ignore warnings for cleaner output\n",
    "\n",
    "# Step 1: Load and preprocess the data\n",
    "df = pd.read_csv('sector_rankings.csv')\n",
    "df = df.drop(columns=['Number_of_Companies', 'Date_Range'])  # Drop unnecessary columns\n",
    "\n",
    "# Separate 'Sector' column for labeling\n",
    "sectors = df['Sector'].values\n",
    "X = df.drop('Sector', axis=1)\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Step 2: Define the fuzzy clustering parameters\n",
    "num_clusters = 5  # Define the number of clusters you want\n",
    "num_iterations = 10\n",
    "membership_results = []\n",
    "\n",
    "# Step 3: Perform Fuzzy C-Means clustering multiple times\n",
    "for _ in range(num_iterations):\n",
    "    fcm = FCM(n_clusters=num_clusters, m=2)\n",
    "    fcm.fit(X_scaled)\n",
    "    membership_results.append(fcm.u)  # Membership matrix for each iteration\n",
    "\n",
    "# Step 4: Calculate the average membership matrix\n",
    "avg_membership = np.mean(np.array(membership_results), axis=0)\n",
    "\n",
    "# Step 5: Analyze consistent clusters\n",
    "# For each sector, find the cluster with the highest average membership\n",
    "sector_clusters = {}\n",
    "for idx, sector in enumerate(sectors):\n",
    "    max_cluster = np.argmax(avg_membership[idx])\n",
    "    max_membership = avg_membership[idx][max_cluster]\n",
    "    sector_clusters[sector] = (max_cluster, max_membership)\n",
    "\n",
    "# Identify sectors that consistently belong to the same cluster\n",
    "consistent_sectors = [sector for sector, (cluster, membership) in sector_clusters.items() if membership > 0.8]\n",
    "inconsistent_sectors = [sector for sector in sectors if sector not in consistent_sectors]\n",
    "\n",
    "# Step 6: Output results\n",
    "print(\"Sectors with high membership consistency in a single cluster:\")\n",
    "for sector in consistent_sectors:\n",
    "    cluster, membership = sector_clusters[sector]\n",
    "    print(f\"{sector} - Cluster: {cluster}, Membership: {membership:.2f}\")\n",
    "\n",
    "print(\"\\nSectors with mixed memberships across clusters (inconsistent):\")\n",
    "for sector in inconsistent_sectors:\n",
    "    memberships = avg_membership[sectors.tolist().index(sector)]\n",
    "    print(f\"{sector} - Cluster memberships: {[f'{m:.2f}' for m in memberships]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-15T20:19:17.155593Z",
     "start_time": "2024-11-12T16:08:35.722468Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import pairwise_distances_argmin_min\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")  # Ignore warnings for cleaner output\n",
    "\n",
    "# Step 1: Load and preprocess the data\n",
    "df = pd.read_csv('sector_rankings.csv')\n",
    "df = df.drop(columns=['Number_of_Companies', 'Date_Range'])  # Drop unnecessary columns\n",
    "\n",
    "# Separate 'Sector' column for labeling\n",
    "sectors = df['Sector'].values\n",
    "X = df.drop('Sector', axis=1)\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Step 2: Define seed sectors and calculate initial centroids\n",
    "seed_sectors = {\n",
    "    0: \"Internet Services & Infrastructure\",\n",
    "    1: \"Oil & Gas Exploration & Production\",\n",
    "    2: \"Interactive Media & Services\",\n",
    "    3: \"Broadcasting\",\n",
    "    4: \"Rail Transportation\"\n",
    "}\n",
    "\n",
    "# Find the rows corresponding to the seed sectors\n",
    "seed_indices = [list(sectors).index(seed) for seed in seed_sectors.values()]\n",
    "initial_centroids = X_scaled[seed_indices]\n",
    "\n",
    "# Step 3: Iteratively perform KMeans until convergence\n",
    "num_clusters = 5\n",
    "centroids = initial_centroids\n",
    "tolerance = 1e-4  # Convergence threshold\n",
    "max_iterations = 100  # Safety limit on iterations\n",
    "iteration = 0\n",
    "\n",
    "while iteration < max_iterations:\n",
    "    # Step 3a: Assign each point to the nearest centroid\n",
    "    labels, _ = pairwise_distances_argmin_min(X_scaled, centroids)\n",
    "    \n",
    "    # Step 3b: Calculate new centroids as the mean of points in each cluster\n",
    "    new_centroids = np.array([X_scaled[labels == k].mean(axis=0) for k in range(num_clusters)])\n",
    "    \n",
    "    # Step 3c: Check for convergence (if centroids do not change significantly)\n",
    "    centroid_shift = np.linalg.norm(new_centroids - centroids, axis=1).max()\n",
    "    print(f\"Iteration {iteration + 1}, centroid shift: {centroid_shift:.6f}\")\n",
    "    \n",
    "    if centroid_shift < tolerance:\n",
    "        print(\"Convergence reached.\")\n",
    "        break\n",
    "    \n",
    "    centroids = new_centroids\n",
    "    iteration += 1\n",
    "\n",
    "# Final labels after convergence\n",
    "final_labels = labels\n",
    "\n",
    "# Step 4: Reduce dimensions for plotting using PCA\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "centroids_pca = pca.transform(centroids)\n",
    "\n",
    "# Step 5: Plot the final clusters\n",
    "plt.figure(figsize=(10, 7))\n",
    "for cluster in range(num_clusters):\n",
    "    # Plot points in each cluster\n",
    "    cluster_points = X_pca[final_labels == cluster]\n",
    "    plt.scatter(cluster_points[:, 0], cluster_points[:, 1], label=f\"Cluster {cluster}\")\n",
    "\n",
    "print(cluster)\n",
    "# Plot centroids\n",
    "plt.scatter(centroids_pca[:, 0], centroids_pca[:, 1], s=200, c='black', marker='X', label='Centroids')\n",
    "\n",
    "# Add labels and title\n",
    "plt.title(\"Final Clusters after KMeans Convergence\")\n",
    "plt.xlabel(\"Principal Component 1\")\n",
    "plt.ylabel(\"Principal Component 2\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-15T20:21:55.392685Z",
     "start_time": "2024-11-15T20:21:51.951662Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Step 1: Load and preprocess the data\n",
    "df = pd.read_csv('sector_rankings.csv')\n",
    "df = df.drop(columns=['Number_of_Companies', 'Date_Range'])  # Drop unnecessary columns\n",
    "\n",
    "# Separate 'Sector' column for labeling and features\n",
    "sectors = df['Sector'].values\n",
    "X = df.drop('Sector', axis=1)\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Step 2: Generate cluster labels with KMeans\n",
    "num_clusters = 5\n",
    "seed_sectors = {\n",
    "    0: \"Internet Services & Infrastructure\",\n",
    "    1: \"Oil & Gas Exploration & Production\",\n",
    "    2: \"Interactive Media & Services\",\n",
    "    3: \"Broadcasting\",\n",
    "    4: \"Rail Transportation\"\n",
    "}\n",
    "\n",
    "# Find the rows corresponding to the seed sectors\n",
    "seed_indices = [list(sectors).index(seed) for seed in seed_sectors.values()]\n",
    "initial_centroids = X_scaled[seed_indices]\n",
    "\n",
    "# Initialize and fit KMeans\n",
    "kmeans = KMeans(n_clusters=num_clusters, init=initial_centroids, n_init=1)\n",
    "kmeans.fit(X_scaled)\n",
    "labels = kmeans.labels_  # Use these labels as the target for training\n",
    "\n",
    "# Step 3: Prepare data for the neural network\n",
    "# Convert labels to categorical (one-hot encoding)\n",
    "y = to_categorical(labels, num_clusters)\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.33, random_state=42)\n",
    "\n",
    "# Step 4: Build the neural network model\n",
    "model = Sequential([\n",
    "    Dense(64, input_shape=(X_train.shape[1],), activation='relu'),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(num_clusters, activation='softmax')  # Output layer with softmax for classification\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Step 5: Train the model\n",
    "history = model.fit(X_train, y_train, epochs=100, batch_size=8, validation_split=0.2, verbose=1)\n",
    "\n",
    "# Step 6: Evaluate the model\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"Test Accuracy: {test_accuracy:.2f}\")\n",
    "\n",
    "# Predict clusters for new data points\n",
    "predictions = model.predict(X_test)\n",
    "predicted_clusters = np.argmax(predictions, axis=1)\n",
    "\n",
    "# Output some test predictions\n",
    "for i in range(len(y_test)):  # Show first 5 predictions\n",
    "    print(f\"True cluster: {np.argmax(y_test[i])}, Predicted cluster: {predicted_clusters[i]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-15T20:19:17.155920Z",
     "start_time": "2024-11-12T13:30:32.136320Z"
    }
   },
   "outputs": [],
   "source": [
    "model.save(\"sector_classification_model.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-15T20:19:17.156108Z",
     "start_time": "2024-11-12T11:08:13.210835Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from fcmeans import FCM\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")  # Ignore warnings for cleaner output\n",
    "\n",
    "# Step 1: Load and preprocess the data\n",
    "df = pd.read_csv('sector_rankings.csv')\n",
    "df = df.drop(columns=['Number_of_Companies', 'Date_Range'])  # Drop unnecessary columns\n",
    "\n",
    "# Separate 'Sector' column for labeling\n",
    "sectors = df['Sector'].values\n",
    "X = df.drop('Sector', axis=1)\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Step 2: Define the fuzzy clustering parameters\n",
    "num_clusters = 5  # Define the number of clusters you want\n",
    "num_iterations = 10\n",
    "membership_results = []\n",
    "\n",
    "# Step 3: Perform Fuzzy C-Means clustering multiple times\n",
    "for _ in range(num_iterations):\n",
    "    fcm = FCM(n_clusters=num_clusters, m=2)\n",
    "    fcm.fit(X_scaled)\n",
    "    membership_results.append(fcm.u)  # Membership matrix for each iteration\n",
    "\n",
    "# Step 4: Calculate the average membership matrix\n",
    "avg_membership = np.mean(np.array(membership_results), axis=0)\n",
    "\n",
    "# Define cluster labels\n",
    "cluster_labels = {\n",
    "    0: \"Internet Service and Infrastructure\",\n",
    "    1: \"Oil & Gas Exploration & Production\",\n",
    "    2: \"Interactive Media & Services\",\n",
    "    3: \"Broadcasting\",\n",
    "    4: \"Rail Transportation\"\n",
    "}\n",
    "\n",
    "# Step 5: Analyze consistent clusters\n",
    "# For each sector, find the cluster with the highest average membership\n",
    "sector_clusters = {}\n",
    "for idx, sector in enumerate(sectors):\n",
    "    max_cluster = np.argmax(avg_membership[idx])\n",
    "    max_membership = avg_membership[idx][max_cluster]\n",
    "    sector_clusters[sector] = (max_cluster, max_membership)\n",
    "\n",
    "# Identify sectors that consistently belong to the same cluster\n",
    "consistent_sectors = [sector for sector, (cluster, membership) in sector_clusters.items() if membership > 0.8]\n",
    "inconsistent_sectors = [sector for sector in sectors if sector not in consistent_sectors]\n",
    "\n",
    "# Step 6: Output results with cluster labels\n",
    "print(\"Sectors with high membership consistency in a single cluster:\")\n",
    "for sector in consistent_sectors:\n",
    "    cluster, membership = sector_clusters[sector]\n",
    "    cluster_name = cluster_labels[cluster]\n",
    "    print(f\"{sector} - Cluster: {cluster_name} ({cluster}), Membership: {membership:.2f}\")\n",
    "\n",
    "print(\"\\nSectors with mixed memberships across clusters (inconsistent):\")\n",
    "for sector in inconsistent_sectors:\n",
    "    memberships = avg_membership[sectors.tolist().index(sector)]\n",
    "    membership_list = [f'{m:.2f}' for m in memberships]\n",
    "    print(f\"{sector} - Cluster memberships: {membership_list}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-15T20:19:17.156307Z",
     "start_time": "2024-11-12T11:13:03.628662Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from fcmeans import FCM\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")  # Ignore warnings for cleaner output\n",
    "\n",
    "# Step 1: Load and preprocess the data\n",
    "df = pd.read_csv('sector_rankings.csv')\n",
    "df = df.drop(columns=['Number_of_Companies', 'Date_Range'])  # Drop unnecessary columns\n",
    "\n",
    "# Separate 'Sector' column for labeling\n",
    "sectors = df['Sector'].values\n",
    "X = df.drop('Sector', axis=1)\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Step 2: Define the fuzzy clustering parameters\n",
    "num_clusters = 5  # Define the number of clusters you want\n",
    "num_iterations = 10\n",
    "membership_results = []\n",
    "\n",
    "# Step 3: Perform Fuzzy C-Means clustering multiple times\n",
    "for _ in range(num_iterations):\n",
    "    fcm = FCM(n_clusters=num_clusters, m=2)\n",
    "    fcm.fit(X_scaled)\n",
    "    membership_results.append(fcm.u)  # Membership matrix for each iteration\n",
    "\n",
    "# Step 4: Calculate the average membership matrix\n",
    "avg_membership = np.mean(np.array(membership_results), axis=0)\n",
    "\n",
    "# Step 5: Track sector consistency across clusters\n",
    "# For each sector, count how often it appears in the same cluster in different iterations\n",
    "consistent_clusters = {}\n",
    "for sector_idx in range(len(sectors)):\n",
    "    sector_memberships = [np.argmax(membership[sector_idx]) for membership in membership_results]\n",
    "    most_common_cluster = max(set(sector_memberships), key=sector_memberships.count)\n",
    "    consistency_ratio = sector_memberships.count(most_common_cluster) / num_iterations\n",
    "    consistent_clusters[sectors[sector_idx]] = (most_common_cluster, consistency_ratio)\n",
    "\n",
    "# Define cluster labels\n",
    "cluster_labels = {\n",
    "    0: \"Internet Service and Infrastructure\",\n",
    "    1: \"Oil & Gas Exploration & Production\",\n",
    "    2: \"Interactive Media & Services\",\n",
    "    3: \"Broadcasting\",\n",
    "    4: \"Rail Transportation\"\n",
    "}\n",
    "\n",
    "# Identify sectors that consistently belong to the same cluster\n",
    "consistent_sectors = [sector for sector, (_, ratio) in consistent_clusters.items() if ratio > 0.8]\n",
    "inconsistent_sectors = [sector for sector in sectors if sector not in consistent_sectors]\n",
    "\n",
    "# Step 6: Output results with cluster labels\n",
    "print(\"Sectors with high membership consistency in a single cluster:\")\n",
    "for sector in consistent_sectors:\n",
    "    cluster, ratio = consistent_clusters[sector]\n",
    "    cluster_name = cluster_labels[cluster]\n",
    "    print(f\"{sector} - Cluster: {cluster_name} ({cluster}), Consistency: {ratio:.2f}\")\n",
    "\n",
    "print(\"\\nSectors with mixed memberships across clusters (inconsistent):\")\n",
    "for sector in inconsistent_sectors:\n",
    "    memberships = avg_membership[sectors.tolist().index(sector)]\n",
    "    membership_list = [f'{m:.2f}' for m in memberships]\n",
    "    print(f\"{sector} - Cluster memberships: {membership_list}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from fcmeans import FCM\n",
    "import warnings\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")  # Ignore warnings for cleaner output\n",
    "\n",
    "# Step 1: Load and preprocess the data\n",
    "df = pd.read_csv('sector_rankings.csv')\n",
    "df = df.drop(columns=['Number_of_Companies', 'Date_Range'])  # Drop unnecessary columns\n",
    "\n",
    "# Separate 'Sector' column for labeling\n",
    "sectors = df['Sector'].values\n",
    "X = df.drop('Sector', axis=1)\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Step 2: Define the fuzzy clustering parameters\n",
    "num_clusters = 5  # Define the number of clusters you want\n",
    "num_iterations = 10\n",
    "membership_results = []\n",
    "\n",
    "# Step 3: Perform Fuzzy C-Means clustering and align clusters based on composition similarity\n",
    "for i in range(num_iterations):\n",
    "    fcm = FCM(n_clusters=num_clusters, m=2)\n",
    "    fcm.fit(X_scaled)\n",
    "    membership_matrix = fcm.u\n",
    "\n",
    "    # Align clusters based on the first iteration\n",
    "    if i == 0:\n",
    "        reference_matrix = membership_matrix.copy()\n",
    "    else:\n",
    "        # Use the Hungarian algorithm to match clusters based on maximum similarity\n",
    "        cost_matrix = -np.dot(reference_matrix.T, membership_matrix)  # Negative for maximum matching\n",
    "        row_ind, col_ind = linear_sum_assignment(cost_matrix)\n",
    "        membership_matrix = membership_matrix[:, col_ind]\n",
    "\n",
    "    membership_results.append(membership_matrix)  # Store aligned membership matrix\n",
    "\n",
    "# Step 4: Calculate the average membership matrix\n",
    "avg_membership = np.mean(np.array(membership_results), axis=0)\n",
    "\n",
    "# Step 5: Analyze consistent clusters\n",
    "# For each sector, find the cluster with the highest average membership\n",
    "sector_clusters = {}\n",
    "for idx, sector in enumerate(sectors):\n",
    "    max_cluster = np.argmax(avg_membership[idx])\n",
    "    max_membership = avg_membership[idx][max_cluster]\n",
    "    sector_clusters[sector] = (max_cluster, max_membership)\n",
    "\n",
    "# Identify sectors that consistently belong to the same cluster\n",
    "consistent_sectors = [sector for sector, (cluster, membership) in sector_clusters.items() if membership > 0.8]\n",
    "inconsistent_sectors = [sector for sector in sectors if sector not in consistent_sectors]\n",
    "\n",
    "# Step 6: Output results\n",
    "print(\"Sectors with high membership consistency in a single cluster:\")\n",
    "for sector in consistent_sectors:\n",
    "    cluster, membership = sector_clusters[sector]\n",
    "    print(f\"{sector} - Cluster: {cluster}, Membership: {membership:.2f}\")\n",
    "\n",
    "print(\"\\nSectors with mixed memberships across clusters (inconsistent):\")\n",
    "for sector in inconsistent_sectors:\n",
    "    memberships = avg_membership[sectors.tolist().index(sector)]\n",
    "    print(f\"{sector} - Cluster memberships: {[f'{m:.2f}' for m in memberships]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from fcmeans import FCM\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")  # Ignore warnings for cleaner output\n",
    "\n",
    "# Step 1: Load and preprocess the data\n",
    "df = pd.read_csv('sector_rankings.csv')\n",
    "df = df.drop(columns=['Number_of_Companies', 'Date_Range'])  # Drop unnecessary columns\n",
    "\n",
    "# Separate 'Sector' column for labeling\n",
    "sectors = df['Sector'].values\n",
    "X = df.drop('Sector', axis=1)\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Step 2: Define the fuzzy clustering parameters\n",
    "num_clusters = 2  # Define the number of clusters you want\n",
    "num_iterations = 10\n",
    "membership_results = []\n",
    "\n",
    "# Define your seeding clusters (e.g., manually assigning sectors to clusters)\n",
    "initial_seeds = {\n",
    "    \"Tech\": 0,  # Example: Place \"Tech\" sector in cluster 0\n",
    "    \"Finance\": 1,  # Place \"Finance\" sector in cluster 1\n",
    "    # Add more sectors as needed\n",
    "}\n",
    "\n",
    "# Step 3: Perform Fuzzy C-Means clustering multiple times\n",
    "for _ in range(num_iterations):\n",
    "    fcm = FCM(n_clusters=num_clusters, m=2)\n",
    "    fcm.fit(X_scaled)\n",
    "    \n",
    "    # Apply seeding to the membership matrix after initial fit\n",
    "    for sector, cluster in initial_seeds.items():\n",
    "        sector_idx = sectors.tolist().index(sector)  # Find index of the sector\n",
    "        fcm.u[sector_idx] = 0  # Set all memberships to 0 initially\n",
    "        fcm.u[sector_idx][cluster] = 1  # Assign full membership to the chosen cluster\n",
    "\n",
    "    membership_results.append(fcm.u)  # Membership matrix for each iteration\n",
    "\n",
    "# Step 4: Calculate the average membership matrix\n",
    "avg_membership = np.mean(np.array(membership_results), axis=0)\n",
    "\n",
    "# Step 5: Analyze consistent clusters\n",
    "# For each sector, find the cluster with the highest average membership\n",
    "sector_clusters = {}\n",
    "for idx, sector in enumerate(sectors):\n",
    "    max_cluster = np.argmax(avg_membership[idx])\n",
    "    max_membership = avg_membership[idx][max_cluster]\n",
    "    sector_clusters[sector] = (max_cluster, max_membership)\n",
    "\n",
    "# Identify sectors that consistently belong to the same cluster\n",
    "consistent_sectors = [sector for sector, (cluster, membership) in sector_clusters.items() if membership > 0.8]\n",
    "inconsistent_sectors = [sector for sector in sectors if sector not in consistent_sectors]\n",
    "\n",
    "# Step 6: Output results\n",
    "print(\"Sectors with high membership consistency in a single cluster:\")\n",
    "for sector in consistent_sectors:\n",
    "    cluster, membership = sector_clusters[sector]\n",
    "    print(f\"{sector} - Cluster: {cluster}, Membership: {membership:.2f}\")\n",
    "\n",
    "print(\"\\nSectors with mixed memberships across clusters (inconsistent):\")\n",
    "for sector in inconsistent_sectors:\n",
    "    memberships = avg_membership[sectors.tolist().index(sector)]\n",
    "    print(f\"{sector} - Cluster memberships: {[f'{m:.2f}' for m in memberships]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-15T20:19:17.157325Z",
     "start_time": "2024-11-11T17:45:54.417376Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from fcmeans import FCM\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")  # Ignore warnings for cleaner output\n",
    "\n",
    "def get_consistent_clusters(df, num_clusters=2, num_iterations=10, membership_threshold=0.8):\n",
    "    \"\"\"\n",
    "    Perform Fuzzy C-Means clustering on the data and identify consistent clusters.\n",
    "    \n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): The input data.\n",
    "    num_clusters (int): The number of clusters to identify.\n",
    "    num_iterations (int): The number of times to run the clustering algorithm.\n",
    "    membership_threshold (float): The minimum membership threshold for a data point to be considered part of a consistent cluster.\n",
    "    \n",
    "    Returns:\n",
    "    dict: A dictionary where the keys are the consistent cluster indices and the values are lists of the data points (rows) that belong to those clusters.\n",
    "    \"\"\"\n",
    "    # Preprocess the data\n",
    "    X = df.drop('Sector', axis=1)\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "    # Perform Fuzzy C-Means clustering multiple times\n",
    "    membership_results = []\n",
    "    for _ in range(num_iterations):\n",
    "        fcm = FCM(n_clusters=num_clusters, m=2)\n",
    "        fcm.fit(X_scaled)\n",
    "        membership_results.append(fcm.u)\n",
    "\n",
    "    # Calculate the average membership matrix\n",
    "    avg_membership = np.mean(np.array(membership_results), axis=0)\n",
    "\n",
    "    # Identify consistent clusters\n",
    "    consistent_clusters = {}\n",
    "    for i in range(num_clusters):\n",
    "        consistent_members = [idx for idx, membership in enumerate(avg_membership[:, i]) if membership >= membership_threshold]\n",
    "        if consistent_members:\n",
    "            consistent_clusters[i] = consistent_members\n",
    "\n",
    "    return consistent_clusters\n",
    "\n",
    "# Load and preprocess the data\n",
    "df = pd.read_csv('sector_rankings.csv')\n",
    "df = df.drop(columns=['Number_of_Companies', 'Date_Range'])\n",
    "\n",
    "# Get consistent clusters\n",
    "consistent_clusters = get_consistent_clusters(df, num_clusters=2, num_iterations=10, membership_threshold=0.8)\n",
    "\n",
    "# Print results\n",
    "print(\"Consistent Clusters:\")\n",
    "for cluster_idx, members in consistent_clusters.items():\n",
    "    print(f\"Cluster {cluster_idx}: {', '.join(df.iloc[members]['Sector'])}\")\n",
    "\n",
    "print(\"\\nInconsistent Sectors:\")\n",
    "all_members = [member for members in consistent_clusters.values() for member in members]\n",
    "inconsistent_sectors = [sector for idx, sector in enumerate(df['Sector']) if idx not in all_members]\n",
    "print(', '.join(inconsistent_sectors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-15T20:19:17.163524Z",
     "start_time": "2024-11-11T17:48:17.049014Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from fcmeans import FCM\n",
    "from collections import defaultdict\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")  # Ignore warnings for cleaner output\n",
    "\n",
    "# Step 1: Load and preprocess the data\n",
    "df = pd.read_csv('sector_rankings.csv')\n",
    "df = df.drop(columns=['Number_of_Companies', 'Date_Range'])  # Drop unnecessary columns\n",
    "\n",
    "# Separate 'Sector' column for labeling\n",
    "sectors = df['Sector'].values\n",
    "X = df.drop('Sector', axis=1)\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Step 2: Define the fuzzy clustering parameters\n",
    "num_clusters = 6  # Define the number of clusters you want\n",
    "num_iterations = 10\n",
    "cluster_assignments = []  # Store cluster assignments (company indices)\n",
    "\n",
    "# Step 3: Perform Fuzzy C-Means clustering multiple times\n",
    "for _ in range(num_iterations):\n",
    "    fcm = FCM(n_clusters=num_clusters, m=2)\n",
    "    fcm.fit(X_scaled)\n",
    "\n",
    "    # Get cluster assignments (indices of companies in each cluster)\n",
    "    labels = fcm.predict(X_scaled)\n",
    "    clusters = defaultdict(set)\n",
    "    for i, label in enumerate(labels):\n",
    "        clusters[label].add(i)\n",
    "    cluster_assignments.append(clusters)\n",
    "\n",
    "# Step 4: Calculate cluster similarity across iterations\n",
    "def jaccard_similarity(set1, set2):\n",
    "    intersection = len(set1.intersection(set2))\n",
    "    union = len(set1.union(set2))\n",
    "    return intersection / union if union != 0 else 0\n",
    "\n",
    "# Compare clusters across iterations to find consistent groupings\n",
    "consistent_clusters = []\n",
    "for i in range(num_iterations):\n",
    "    for j in range(i + 1, num_iterations):\n",
    "        for cluster1 in cluster_assignments[i]:\n",
    "            for cluster2 in cluster_assignments[j]:\n",
    "                similarity = jaccard_similarity(cluster_assignments[i][cluster1], cluster_assignments[j][cluster2])\n",
    "                if similarity > 0.8:  # Threshold for consistency\n",
    "                    consistent_clusters.append((i, cluster1, j, cluster2, similarity))\n",
    "\n",
    "# Step 5: Assign sectors to consistent clusters\n",
    "sector_clusters = {}\n",
    "for i, sector in enumerate(sectors):\n",
    "    cluster_memberships = []\n",
    "    for iteration, cluster1, _, cluster2, _ in consistent_clusters:\n",
    "        if i in cluster_assignments[iteration][cluster1]:\n",
    "            cluster_memberships.append(cluster1)\n",
    "        if i in cluster_assignments[iteration][cluster2]:\n",
    "            cluster_memberships.append(cluster2)\n",
    "\n",
    "    # Find the most frequent cluster for the sector\n",
    "    if cluster_memberships:\n",
    "        most_frequent_cluster = max(set(cluster_memberships), key=cluster_memberships.count)\n",
    "        sector_clusters[sector] = most_frequent_cluster\n",
    "\n",
    "# Step 6: Output results\n",
    "print(\"Sectors and their assigned consistent clusters:\")\n",
    "for sector, cluster in sector_clusters.items():\n",
    "    print(f\"{sector} - Cluster: {cluster}\")\n",
    "\n",
    "# (Optional) Further analysis of consistent_clusters to identify \n",
    "# the specific companies driving the consistent groupings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-15T20:19:17.164054Z",
     "start_time": "2024-11-11T18:02:24.401465Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from fcmeans import FCM\n",
    "from sklearn.decomposition import PCA\n",
    "from collections import defaultdict\n",
    "\n",
    "# Step 1: Load and preprocess the data\n",
    "df = pd.read_csv('sector_rankings.csv')\n",
    "df = df.drop(columns=['Number_of_Companies', 'Date_Range'])  # Drop unnecessary columns\n",
    "\n",
    "# Separate 'Sector' column for labeling\n",
    "sectors = df['Sector'].values\n",
    "X = df.drop('Sector', axis=1)\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Step 2: Define the fuzzy clustering parameters\n",
    "num_clusters = 6  # Define the number of clusters you want\n",
    "num_iterations = 1\n",
    "\n",
    "# Step 3: Perform Fuzzy C-Means clustering multiple times\n",
    "sector_pairs = defaultdict(int)  # Track frequency of sector pair clustering\n",
    "cluster_assignments = []         # Track individual cluster assignments\n",
    "\n",
    "for _ in range(num_iterations):\n",
    "    fcm = FCM(n_clusters=num_clusters, m=2)\n",
    "    fcm.fit(X_scaled)\n",
    "\n",
    "    # Get cluster assignments\n",
    "    labels = fcm.predict(X_scaled)\n",
    "    clusters = defaultdict(set)\n",
    "    for i, label in enumerate(labels):\n",
    "        clusters[label].add(i)\n",
    "\n",
    "    # Record pairwise sector clustering counts\n",
    "    for cluster in clusters.values():\n",
    "        for sector1 in cluster:\n",
    "            for sector2 in cluster:\n",
    "                if sector1 != sector2:\n",
    "                    sector_pairs[(sector1, sector2)] += 1\n",
    "\n",
    "# Step 4: Create the cluster strength matrix\n",
    "num_sectors = len(sectors)\n",
    "strength_matrix = np.zeros((num_sectors, num_sectors))\n",
    "for (sector1, sector2), count in sector_pairs.items():\n",
    "    strength_matrix[sector1, sector2] = count / num_iterations  # Normalize by number of iterations\n",
    "\n",
    "# Step 5: Plot the heatmap\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(strength_matrix, cmap=\"YlGnBu\", xticklabels=sectors, yticklabels=sectors)\n",
    "plt.title(\"Cluster Strength Heatmap for Sectors\")\n",
    "plt.xlabel(\"Sectors\")\n",
    "plt.ylabel(\"Sectors\")\n",
    "plt.show()\n",
    "\n",
    "# Step 6: Perform PCA for visualization of segregated sectors\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "# Use consistent cluster assignments (final most frequent cluster from previous code)\n",
    "sector_clusters = {}  # Mapping from sector name to its most consistent cluster\n",
    "\n",
    "for i, sector in enumerate(sectors):\n",
    "    cluster_memberships = [cluster for iteration, cluster1, _, cluster2, _ in consistent_clusters if i in cluster_assignments[iteration][cluster1]]\n",
    "    if cluster_memberships:\n",
    "        most_frequent_cluster = max(set(cluster_memberships), key=cluster_memberships.count)\n",
    "        sector_clusters[sector] = most_frequent_cluster\n",
    "\n",
    "# Plot the segregated sectors based on PCA results\n",
    "plt.figure(figsize=(12, 8))\n",
    "for cluster in set(sector_clusters.values()):\n",
    "    cluster_points = X_pca[[i for i, sec in enumerate(sectors) if sector_clusters[sec] == cluster]]\n",
    "    plt.scatter(cluster_points[:, 0], cluster_points[:, 1], label=f\"Cluster {cluster}\")\n",
    "\n",
    "plt.title(\"Segregated Sectors Based on Consistent Clustering\")\n",
    "plt.xlabel(\"PCA Component 1\")\n",
    "plt.ylabel(\"PCA Component 2\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-15T20:19:17.724635Z",
     "start_time": "2024-11-11T17:30:45.149549Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "# Sample data for inconsistent sectors with their cluster memberships.\n",
    "data = {\n",
    "    'Sectors': [\n",
    "        'Homebuilding', 'Packaged Foods & Meats', 'Movies & Entertainment', 'Health Care Supplies', \n",
    "        'Oil & Gas Equipment & Services', 'Semiconductors', 'Automobile Manufacturers', 'Consumer Finance',\n",
    "        'Health Care REITs', 'Aerospace & Defense', 'Systems Software', 'Apparel, Accessories & Luxury Goods'\n",
    "    ],\n",
    "    'Cluster 1': [0.19, 0.18, 0.20, 0.18, 0.19, 0.19, 0.20, 0.16, 0.27, 0.20, 0.19, 0.19],\n",
    "    'Cluster 2': [0.11, 0.17, 0.21, 0.11, 0.31, 0.20, 0.19, 0.11, 0.19, 0.39, 0.17, 0.23],\n",
    "    'Cluster 3': [0.21, 0.22, 0.20, 0.22, 0.27, 0.21, 0.20, 0.22, 0.05, 0.30, 0.22, 0.22],\n",
    "    'Cluster 4': [0.31, 0.16, 0.18, 0.26, 0.14, 0.19, 0.20, 0.16, 0.28, 0.10, 0.21, 0.17],\n",
    "    'Cluster 5': [0.18, 0.27, 0.21, 0.24, 0.09, 0.20, 0.20, 0.34, 0.21, 0.01, 0.21, 0.19]\n",
    "}\n",
    "\n",
    "# Convert to DataFrame for seaborn heatmap\n",
    "df = pd.DataFrame(data)\n",
    "df.set_index('Sectors', inplace=True)\n",
    "\n",
    "# Create a heatmap\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(df, annot=True, cmap='YlGnBu', linewidths=0.5, fmt=\".2f\", cbar_kws={'label': 'Membership Strength'})\n",
    "plt.title(\"Sector Membership Strengths Across Clusters\")\n",
    "plt.xlabel(\"Clusters\")\n",
    "plt.ylabel(\"Sectors\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-15T20:19:17.737331Z",
     "start_time": "2024-11-11T17:38:38.013046Z"
    }
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Step 1: Create a DataFrame with the average membership values\n",
    "membership_df = pd.DataFrame(avg_membership, columns=[f\"Cluster {i+1}\" for i in range(num_clusters)], index=sectors)\n",
    "\n",
    "# Step 2: Create the heatmap\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(membership_df, annot=True, cmap=\"YlGnBu\", cbar=True, fmt=\".2f\", linewidths=0.5)\n",
    "plt.title('Fuzzy C-Means Membership Heatmap')\n",
    "plt.xlabel('Clusters')\n",
    "plt.ylabel('Sectors')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
