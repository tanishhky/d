{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sector Analysis Pipeline Documentation\n",
    "\n",
    "## Overview\n",
    "This codebase implements a comprehensive sector analysis pipeline that processes market cap and revenue data, calculates various sector indices, performs clustering analysis, and builds a neural network classifier for sector categorization.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IMPORTING LIBRARIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-19T06:50:28.029304Z",
     "start_time": "2024-11-19T06:50:14.068698Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import os\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import pairwise_distances_argmin_min\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 1: Data Processing and Index Calculation\n",
    "\n",
    "### Core Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `calculate_yoy_growth(df)`\n",
    "- **Purpose**: Calculates Year-over-Year (YoY) growth for Market Cap\n",
    "- **Input**: DataFrame with MarketCap column\n",
    "- **Output**: DataFrame with added YoY_Growth column\n",
    "- **Key Operations**:\n",
    "  - Calculates percentage change over 4 periods (quarterly data)\n",
    "  - Removes rows with NaN growth values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_yoy_growth(df):\n",
    "    \"\"\"Calculate the Year-over-Year (YoY) growth for Market Cap.\"\"\"\n",
    "    df['YoY_Growth'] = df['MarketCap'].pct_change(periods=4) * 100  # YoY percentage change\n",
    "    df.dropna(subset=['YoY_Growth'], inplace=True)  # Drop rows with NaN YoY growth\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `apply_log10_transformation(df)`\n",
    "- **Purpose**: Applies logarithmic transformation to YoY growth\n",
    "- **Input**: DataFrame with YoY_Growth column\n",
    "- **Output**: DataFrame with added Log_YoY_Growth column\n",
    "- **Note**: Adds 100 to handle negative growth values before log transformation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_log10_transformation(df):\n",
    "    \"\"\"Apply log10 transformation to the YoY growth.\"\"\"\n",
    "    df['Log_YoY_Growth'] = np.log10(df['YoY_Growth'] + 100)  # log10(1 + YoY_Growth) to handle negative growth\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `calculate_sector_leader_and_rank(df, sector)`\n",
    "- **Purpose**: Identifies sector leaders and rankings based on performance\n",
    "- **Input**: \n",
    "  - DataFrame with sector data\n",
    "  - Sector name\n",
    "- **Output**: \n",
    "  - Sector leader ticker\n",
    "  - Sorted companies by performance\n",
    "  - Total overperformance metric\n",
    "- **Key Operations**:\n",
    "  - Calculates sector average performance\n",
    "  - Counts times each company outperforms sector average\n",
    "  - Ranks companies by overperformance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_sector_leader_and_rank(df, sector):\n",
    "    \"\"\"Calculate the leader and performance ranking for a sector.\"\"\"\n",
    "    sector_avg = df.groupby('Date')['YoY_Growth'].mean()\n",
    "    overperformance_counts = {}\n",
    "    \n",
    "    for ticker in df['Ticker'].unique():\n",
    "        company_data = df[df['Ticker'] == ticker]\n",
    "        company_data = company_data.set_index('Date')\n",
    "        company_data['Sector_Avg'] = sector_avg\n",
    "        \n",
    "        overperformance_count = (company_data['YoY_Growth'] > company_data['Sector_Avg']).sum()\n",
    "        overperformance_counts[ticker] = overperformance_count\n",
    "\n",
    "    sorted_companies = sorted(overperformance_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    sector_leader = sorted_companies[0][0]\n",
    "    \n",
    "    total_overperformance = sum([count**2 for _, count in sorted_companies])\n",
    "\n",
    "    print(f\"Leader for {sector} sector: {sector_leader}\")\n",
    "    print(f\"Descending order of companies by overperformance in {sector} sector:\")\n",
    "    for company, count in sorted_companies:\n",
    "        print(f\"{company}: {count} times overperformed\")\n",
    "    \n",
    "    return sector_leader, sorted_companies, total_overperformance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `calculate_sector_index(df, sorted_companies, total_overperformance)`\n",
    "- **Purpose**: Creates weighted sector index based on company performance\n",
    "- **Input**: \n",
    "  - Sector DataFrame\n",
    "  - Sorted companies list\n",
    "  - Total overperformance score\n",
    "- **Output**: DataFrame with Date and Sector_Index columns\n",
    "- **Key Operations**:\n",
    "  - Calculates fractional contributions\n",
    "  - Applies weights to company performance\n",
    "  - Aggregates weighted performances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-19T06:50:41.978321Z",
     "start_time": "2024-11-19T06:50:28.083924Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def calculate_sector_index(df, sorted_companies, total_overperformance):\n",
    "    \"\"\"Calculate the sector index based on fractional contribution of stocks.\"\"\"\n",
    "    fractional_contribution = {company: count**2 / total_overperformance for company, count in sorted_companies}\n",
    "    \n",
    "    sector_index = pd.DataFrame()\n",
    "    \n",
    "    for ticker in df['Ticker'].unique():\n",
    "        company_data = df[df['Ticker'] == ticker].copy()\n",
    "        \n",
    "        if ticker in fractional_contribution:\n",
    "            contribution = fractional_contribution[ticker]\n",
    "            company_data['Weighted_YoY_Growth'] = company_data['YoY_Growth'] * contribution\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "        if sector_index.empty:\n",
    "            sector_index = company_data[['Date', 'Weighted_YoY_Growth']].copy()\n",
    "        else:\n",
    "            sector_index = pd.merge(sector_index, company_data[['Date', 'Weighted_YoY_Growth']],\n",
    "                                    on='Date', how='outer', suffixes=('', f'_{ticker}'))\n",
    "\n",
    "    sector_index['Sector_Index'] = sector_index.filter(like='Weighted_YoY_Growth').sum(axis=1)\n",
    "\n",
    "    return sector_index[['Date', 'Sector_Index']]\n",
    "\n",
    "def calculate_simple_average_index(df):\n",
    "    \"\"\"Calculate sector index using simple average of YoY growth.\"\"\"\n",
    "    simple_avg_index = df.groupby('Date')['YoY_Growth'].mean().reset_index()\n",
    "    simple_avg_index.rename(columns={'YoY_Growth': 'Simple_Avg_Index'}, inplace=True)\n",
    "    return simple_avg_index\n",
    "\n",
    "def plot_sector_index(sector_index, simple_avg_index, df, sector):\n",
    "    \"\"\"Plot the sector index (weighted and simple average) and stock values.\"\"\"\n",
    "    if sector_index.empty:\n",
    "        print(f\"No data to plot for {sector}.\")\n",
    "        return\n",
    "\n",
    "    # Set up the plot\n",
    "    plt.figure(figsize=(12, 8))\n",
    "\n",
    "    # Plot contributing stock YoY values\n",
    "    unique_tickers = df['Ticker'].unique()\n",
    "    for ticker in unique_tickers:\n",
    "        company_data = df[df['Ticker'] == ticker]\n",
    "        plt.plot(company_data['Date'], company_data['YoY_Growth'], label=f\"{ticker} YoY Growth\", linestyle='--')\n",
    "\n",
    "    # Plot weighted sector index\n",
    "    plt.plot(sector_index['Date'], sector_index['Sector_Index'], label=f'{sector} Weighted Sector Index', color='blue', linewidth=2)\n",
    "\n",
    "    # Plot simple average sector index\n",
    "    plt.plot(simple_avg_index['Date'], simple_avg_index['Simple_Avg_Index'], label=f'{sector} Simple Average Index', color='red', linewidth=2)\n",
    "\n",
    "    plt.title(f'Sector Index for {sector} Sector (Weighted vs Simple Average)')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('YoY Growth / Index Value')\n",
    "    plt.legend(loc='best')\n",
    "    plt.grid(True)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EXECUTION OF STAGE-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Path to the directory where sector CSV files are saved\n",
    "    input_dir = \"sector_mkt_cap_results\"\n",
    "    output_dir = \"sector_wise_index\"  # Directory to save sector indices\n",
    "\n",
    "    # Create output directory if it doesn't exist\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    # Process each sector CSV\n",
    "    for sector_file in os.listdir(input_dir):\n",
    "        if sector_file.endswith(\".csv\"):\n",
    "            sector = sector_file.replace(\"_mkt_cap_quarter_end.csv\", \"\")\n",
    "            print(f\"Processing {sector} sector...\")\n",
    "\n",
    "            # Load the CSV file\n",
    "            file_path = os.path.join(input_dir, sector_file)\n",
    "            df = pd.read_csv(file_path, parse_dates=['Date'])\n",
    "\n",
    "            # Calculate YoY growth and apply log10 transformation\n",
    "            df = calculate_yoy_growth(df)\n",
    "            df = apply_log10_transformation(df)\n",
    "\n",
    "            # Calculate sector leader, ranking and total overperformance count\n",
    "            sector_leader, sorted_companies, total_overperformance = calculate_sector_leader_and_rank(df, sector)\n",
    "\n",
    "            # Calculate the sector index based on fractional contributions\n",
    "            sector_weighted_index = calculate_sector_index(df, sorted_companies, total_overperformance)\n",
    "\n",
    "            # Calculate the simple average index\n",
    "            simple_avg_index = calculate_simple_average_index(df)\n",
    "\n",
    "            # Merge sector_index and simple_avg_index for saving\n",
    "            combined_index = pd.merge(sector_weighted_index, simple_avg_index, on='Date', how='outer')\n",
    "\n",
    "            # Save the sector index data to a CSV file\n",
    "            output_file_path = os.path.join(output_dir, f\"{sector}_sector_index.csv\")\n",
    "            combined_index.to_csv(output_file_path, index=False)\n",
    "            print(f\"Saved sector index for {sector} to {output_file_path}\")\n",
    "\n",
    "            # Plot the sector index (both weighted and simple average) alongside stock values\n",
    "            plot_sector_index(sector_weighted_index, simple_avg_index, df, sector)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 2: Data Merging and Growth Analysis\n",
    "\n",
    "### Key Functions\n",
    "\n",
    "#### `merge_sector_data(mkt_cap_dir, revenue_dir, output_dir)`\n",
    "- **Purpose**: Merges market cap and revenue data for sectors\n",
    "- **Input**: \n",
    "  - Directory paths for market cap and revenue data\n",
    "  - Output directory path\n",
    "- **Output**: Dictionary of merged sector DataFrames\n",
    "- **Key Operations**:\n",
    "  - Identifies common sectors between datasets\n",
    "  - Aligns quarterly market cap with revenue data\n",
    "  - Handles date formatting and matching\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_sector_data(mkt_cap_dir, revenue_dir, output_dir=\"merged_sector_data\"):\n",
    "    \"\"\"\n",
    "    Merge market cap and revenue data for sectors where both datasets are available.\n",
    "    Handles specific CSV structures with quarterly market cap and yearly revenue data.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    \n",
    "    merged_sectors = {}\n",
    "    \n",
    "    # Get list of sectors from both directories\n",
    "    mkt_cap_sectors = {f.split('_mkt_cap')[0] for f in os.listdir(mkt_cap_dir) if f.endswith('.csv')}\n",
    "    revenue_sectors = {f.split('_revenue')[0] for f in os.listdir(revenue_dir) if f.endswith('.csv')}\n",
    "    \n",
    "    # Find common sectors\n",
    "    common_sectors = mkt_cap_sectors.intersection(revenue_sectors)\n",
    "    \n",
    "    for sector in common_sectors:\n",
    "        print(f\"Processing sector: {sector}\")\n",
    "        \n",
    "        # Read market cap data\n",
    "        mkt_cap_file = f\"{sector}_mkt_cap_quarter_end.csv\"\n",
    "        mkt_cap_path = os.path.join(mkt_cap_dir, mkt_cap_file)\n",
    "        mkt_cap_df = pd.read_csv(mkt_cap_path)\n",
    "        \n",
    "        # Read revenue data\n",
    "        revenue_file = f\"{sector}_revenue.csv\"\n",
    "        revenue_path = os.path.join(revenue_dir, revenue_file)\n",
    "        revenue_df = pd.read_csv(revenue_path)\n",
    "        \n",
    "        # Clean and convert dates\n",
    "        try:\n",
    "            # Handle the specific date format in market cap data\n",
    "            mkt_cap_df['Date'] = pd.to_datetime(mkt_cap_df['Date'].str.split(' ').str[0])\n",
    "        except AttributeError:\n",
    "            mkt_cap_df['Date'] = pd.to_datetime(mkt_cap_df['Date'])\n",
    "            \n",
    "        revenue_df['date'] = pd.to_datetime(revenue_df['date'])\n",
    "        \n",
    "        # Extract year and quarter\n",
    "        mkt_cap_df['year'] = pd.DatetimeIndex(mkt_cap_df['Date']).year\n",
    "        mkt_cap_df['quarter'] = pd.DatetimeIndex(mkt_cap_df['Date']).quarter\n",
    "        \n",
    "        # Create a list to store merged data for each ticker\n",
    "        merged_data = []\n",
    "        \n",
    "        # Get unique tickers from both datasets\n",
    "        mkt_cap_df['Ticker'] = mkt_cap_df['Ticker'].str.upper()\n",
    "        revenue_df['ticker'] = revenue_df['ticker'].str.upper()\n",
    "        \n",
    "        common_tickers = set(mkt_cap_df['Ticker']).intersection(set(revenue_df['ticker']))\n",
    "        \n",
    "        print(f\"Found {len(common_tickers)} common tickers for {sector}\")\n",
    "        \n",
    "        for ticker in common_tickers:\n",
    "            ticker_mkt_cap = mkt_cap_df[mkt_cap_df['Ticker'] == ticker].copy()\n",
    "            ticker_revenue = revenue_df[revenue_df['ticker'] == ticker].copy()\n",
    "            \n",
    "            for _, mkt_cap_row in ticker_mkt_cap.iterrows():\n",
    "                matching_revenue = ticker_revenue[\n",
    "                    (ticker_revenue['year'] == mkt_cap_row['year']) &\n",
    "                    (ticker_revenue['quarter'] == mkt_cap_row['quarter'])\n",
    "                ]\n",
    "                \n",
    "                if matching_revenue.empty:\n",
    "                    yearly_revenue = ticker_revenue[\n",
    "                        ticker_revenue['year'] == mkt_cap_row['year']\n",
    "                    ]\n",
    "                    if not yearly_revenue.empty:\n",
    "                        revenue_value = yearly_revenue.iloc[-1]['revenue']\n",
    "                        revenue_growth = yearly_revenue.iloc[-1].get('revenue_yoy_growth', np.nan)\n",
    "                    else:\n",
    "                        continue\n",
    "                else:\n",
    "                    revenue_value = matching_revenue.iloc[0]['revenue']\n",
    "                    revenue_growth = matching_revenue.iloc[0].get('revenue_yoy_growth', np.nan)\n",
    "                \n",
    "                merged_row = {\n",
    "                    'Date': mkt_cap_row['Date'],\n",
    "                    'Year': mkt_cap_row['year'],\n",
    "                    'Quarter': mkt_cap_row['quarter'],\n",
    "                    'Ticker': ticker,\n",
    "                    'MarketCap': mkt_cap_row['MarketCap'],\n",
    "                    'Revenue': revenue_value,\n",
    "                    'Revenue_YoY_Growth': revenue_growth,\n",
    "                    'Company_Name': ticker_revenue.iloc[0].get('company_name', ticker)\n",
    "                }\n",
    "                merged_data.append(merged_row)\n",
    "        \n",
    "        if merged_data:\n",
    "            merged_df = pd.DataFrame(merged_data)\n",
    "            merged_df = merged_df.sort_values(['Date', 'Ticker'])\n",
    "            merged_sectors[sector] = merged_df\n",
    "            \n",
    "            # Save merged data to a CSV file\n",
    "            output_file_path = os.path.join(output_dir, f\"{sector}_merged_data.csv\")\n",
    "            merged_df.to_csv(output_file_path, index=False)\n",
    "            print(f\"Successfully merged data for {sector} and saved to {output_file_path}\")\n",
    "        else:\n",
    "            print(f\"No matching data found for {sector}\")\n",
    "        \n",
    "    return merged_sectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `calculate_growth_indicator(value, mean, std_dev)`\n",
    "- **Purpose**: Determines growth status based on statistical thresholds\n",
    "- **Input**: \n",
    "  - Value to evaluate\n",
    "  - Mean of the distribution\n",
    "  - Standard deviation of the distribution\n",
    "- **Output**: Growth indicator (-1, 0, or 1)\n",
    "- **Thresholds**: Uses 0.07 standard deviations from mean\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_growth_indicator(value, mean, std_dev):\n",
    "    \"\"\"\n",
    "    Calculate the growth indicator based on sector-specific mean and standard deviation.\n",
    "    Returns 1 if value is more than 0.5 standard deviations above the mean,\n",
    "    -1 if it is more than 0.5 standard deviations below, and 0 if in between.\n",
    "    \"\"\"\n",
    "    if pd.isna(value) or pd.isna(mean) or pd.isna(std_dev):\n",
    "        return 0  # Return 0 if any inputs are missing (NaN)\n",
    "    \n",
    "    threshold_high = mean + 0.07 * std_dev\n",
    "    threshold_low = mean - 0.07 * std_dev\n",
    "    \n",
    "    if value > threshold_high:\n",
    "        return 1\n",
    "    elif value < threshold_low:\n",
    "        return -1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 3: Beta Analysis and Covariance\n",
    "\n",
    "### Core Function\n",
    "\n",
    "#### `calculate_beta_covariance(df, period_months)`\n",
    "- **Purpose**: Calculates beta covariance over specified time periods\n",
    "- **Input**: \n",
    "  - Sector DataFrame\n",
    "  - Analysis period in months\n",
    "- **Output**: Average absolute covariance value\n",
    "- **Key Operations**:\n",
    "  - Calculates returns and market weights\n",
    "  - Computes rolling betas\n",
    "  - Handles missing values and time series alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_beta_covariance(df, period_months):\n",
    "    \"\"\"\n",
    "    Calculate covariance of beta over a specified period with improved handling of time series.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = df.copy()\n",
    "        \n",
    "        # Ensure data is sorted by date\n",
    "        df = df.sort_values(['Date', 'Ticker'])\n",
    "        \n",
    "        # Calculate returns for each company\n",
    "        df['Returns'] = df.groupby('Ticker')['MarketCap'].pct_change()\n",
    "        \n",
    "        # Calculate market returns (using value-weighted market return)\n",
    "        df['Market_Value'] = df.groupby('Date')['MarketCap'].transform('sum')\n",
    "        df['Market_Weight'] = df['MarketCap'] / df['Market_Value']\n",
    "        df['Market_Returns'] = df.groupby('Date')['Returns'].transform(lambda x: (x * df.loc[x.index, 'Market_Weight']).sum())\n",
    "        \n",
    "        # Set minimum periods for rolling calculations\n",
    "        min_periods = max(2, period_months - 1)  # Ensure at least 2 periods for correlation\n",
    "        rolling_window = period_months * 3  # Convert months to quarters (assuming quarterly data)\n",
    "        \n",
    "        betas_by_date = []\n",
    "        \n",
    "        for ticker in df['Ticker'].unique():\n",
    "            ticker_data = df[df['Ticker'] == ticker].copy()\n",
    "            \n",
    "            if len(ticker_data) >= min_periods:\n",
    "                # Calculate rolling betas\n",
    "                rolling_cov = (\n",
    "                    ticker_data['Returns']\n",
    "                    .rolling(window=rolling_window, min_periods=min_periods)\n",
    "                    .cov(ticker_data['Market_Returns'])\n",
    "                )\n",
    "                \n",
    "                rolling_market_var = (\n",
    "                    ticker_data['Market_Returns']\n",
    "                    .rolling(window=rolling_window, min_periods=min_periods)\n",
    "                    .var()\n",
    "                )\n",
    "                \n",
    "                # To avoid dividing by zero, handle NaN or zero variance values\n",
    "                ticker_data['Beta'] = rolling_cov / rolling_market_var.replace(0, np.nan)\n",
    "                \n",
    "                # Store results\n",
    "                betas_by_date.append(ticker_data[['Date', 'Ticker', 'Beta']].dropna())\n",
    "        \n",
    "        if not betas_by_date:\n",
    "            return 0\n",
    "        \n",
    "        # Combine all beta calculations\n",
    "        all_betas = pd.concat(betas_by_date)\n",
    "        \n",
    "        # Create a pivot table of betas (companies x dates)\n",
    "        beta_matrix = all_betas.pivot_table(\n",
    "            index='Ticker',\n",
    "            columns='Date',\n",
    "            values='Beta',\n",
    "            aggfunc='first'\n",
    "        )\n",
    "        \n",
    "        # Remove companies with too many missing values\n",
    "        min_observations = beta_matrix.shape[1] * 0.5  # Require at least 50% of dates\n",
    "        beta_matrix = beta_matrix[beta_matrix.count(axis=1) >= min_observations]\n",
    "        \n",
    "        if beta_matrix.empty:\n",
    "            return 0\n",
    "        \n",
    "        # Fill remaining NaN values with forward fill then backward fill\n",
    "        beta_matrix = beta_matrix.fillna(method='ffill', axis=1).fillna(method='bfill', axis=1)\n",
    "        \n",
    "        # Calculate covariance between different dates\n",
    "        cov_matrix = beta_matrix.T.cov()\n",
    "        \n",
    "        # Calculate average absolute covariance (excluding diagonal)\n",
    "        mask = ~np.eye(cov_matrix.shape[0], dtype=bool)\n",
    "        avg_cov = np.abs(cov_matrix.where(mask)).mean().mean()\n",
    "        \n",
    "        return float(avg_cov) if not np.isnan(avg_cov) else 0\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in beta covariance calculation: {e}\")\n",
    "        return 0\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 4: Sector Rankings and Visualization\n",
    "\n",
    "### Key Functions\n",
    "\n",
    "#### `calculate_sector_rankings(merged_sectors, output_file)`\n",
    "- **Purpose**: Generates comprehensive sector rankings\n",
    "- **Input**: Dictionary of merged sector data\n",
    "- **Output**: DataFrame with sector rankings\n",
    "- **Metrics**:\n",
    "  - Market Cap Growth Score\n",
    "  - Revenue Growth Score\n",
    "  - Weighted-Simple Variance\n",
    "  - 6-month Beta Covariance\n",
    "  - 4-year Beta Covariance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_sector_rankings(merged_sectors, output_file='sector_rankings.csv'):\n",
    "    \"\"\"Calculate and rank sectors based on the five specified parameters\"\"\"\n",
    "    rankings = []\n",
    "    \n",
    "    for sector, df in merged_sectors.items():\n",
    "        print(f\"Processing sector: {sector}\")\n",
    "        try:\n",
    "            # Ensure data is sorted chronologically\n",
    "            df = df.sort_values('Date')\n",
    "            \n",
    "            # 1. Market Cap YoY Growth Indicator\n",
    "            df['MktCap_YoY_Change'] = df.groupby('Ticker')['MarketCap'].pct_change(periods=4) * 100\n",
    "            mkt_cap_mean = df['MktCap_YoY_Change'].mean()\n",
    "            mkt_cap_std = df['MktCap_YoY_Change'].std()\n",
    "            df['MktCap_Growth_Indicator'] = df['MktCap_YoY_Change'].apply(\n",
    "                lambda x: calculate_growth_indicator(x, mkt_cap_mean, mkt_cap_std)\n",
    "            )\n",
    "            \n",
    "            # 2. Revenue YoY Growth Indicator\n",
    "            revenue_mean = df['Revenue_YoY_Growth'].mean()\n",
    "            revenue_std = df['Revenue_YoY_Growth'].std()\n",
    "            df['Revenue_Growth_Indicator'] = df['Revenue_YoY_Growth'].apply(\n",
    "                lambda x: calculate_growth_indicator(x, revenue_mean, revenue_std)\n",
    "            )\n",
    "            \n",
    "            # 3. Variance between weighted and simple average\n",
    "            df['Weighted_MktCap_Change'] = (\n",
    "                df['MktCap_YoY_Change'] * \n",
    "                df['MarketCap'] / \n",
    "                df.groupby('Date')['MarketCap'].transform('sum')\n",
    "            )\n",
    "            \n",
    "            # Calculate averages only for non-NaN values\n",
    "            weighted_avg = df.groupby('Date')['Weighted_MktCap_Change'].sum().mean()\n",
    "            simple_avg = df['MktCap_YoY_Change'].mean()\n",
    "            variance_avg = abs(weighted_avg - simple_avg)\n",
    "            \n",
    "            # 4 & 5. Beta covariances\n",
    "            print(f\"Calculating 6-month beta covariance for {sector}\")\n",
    "            beta_6m_cov = calculate_beta_covariance(df, 2)\n",
    "            \n",
    "            print(f\"Calculating 5-year beta covariance for {sector}\")\n",
    "            beta_4y_cov = calculate_beta_covariance(df, 16)\n",
    "            \n",
    "            rankings.append({\n",
    "                'Sector': sector,\n",
    "                'MktCap_Growth_Score': df['MktCap_Growth_Indicator'].mean(),\n",
    "                'Revenue_Growth_Score': df['Revenue_Growth_Indicator'].mean(),\n",
    "                'Weighted_Simple_Variance': variance_avg,\n",
    "                'Beta_6M_Covariance': beta_6m_cov,\n",
    "                'Beta_4Y_Covariance': beta_4y_cov,\n",
    "                'Number_of_Companies': len(df['Ticker'].unique()),\n",
    "                'Date_Range': f\"{df['Date'].min().strftime('%Y-%m-%d')} to {df['Date'].max().strftime('%Y-%m-%d')}\"\n",
    "            })\n",
    "            \n",
    "            print(f\"Successfully processed {sector}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing sector {sector}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    # Create rankings DataFrame and save to CSV\n",
    "    rankings_df = pd.DataFrame(rankings)\n",
    "    rankings_df.to_csv(output_file, index=False)\n",
    "    return rankings_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `plot_covariance_heatmap(cov_matrix, title)`\n",
    "- **Purpose**: Visualizes sector covariances\n",
    "- **Input**: \n",
    "  - Covariance matrix\n",
    "  - Plot title\n",
    "- **Output**: Heatmap visualization\n",
    "- **Features**: Uses seaborn for enhanced visualization\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_covariance_heatmap(cov_matrix, title='Sector Covariance Heatmap'):\n",
    "    \"\"\"\n",
    "    Plot a heatmap based on the covariance matrix.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.heatmap(cov_matrix, annot=True, cmap=\"coolwarm\", fmt=\".2f\", cbar_kws={'label': 'Covariance'})\n",
    "    plt.title(title)\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.yticks(rotation=0)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EXECUTION\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-19T06:50:46.464685Z",
     "start_time": "2024-11-19T06:50:42.134796Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def main():\n",
    "    mkt_cap_dir = \"sector_mkt_cap_results\"\n",
    "    revenue_dir = \"sector_revenue_results\"\n",
    "    \n",
    "    # Merge sector data\n",
    "    merged_sectors = merge_sector_data(mkt_cap_dir, revenue_dir)\n",
    "    \n",
    "    # Calculate rankings with new parameters\n",
    "    rankings_df = calculate_sector_rankings(merged_sectors)\n",
    "    \n",
    "    # Print rankings for each parameter\n",
    "    parameters = ['MktCap_Growth_Score', 'Revenue_Growth_Score', 'Weighted_Simple_Variance', \n",
    "                 'Beta_6M_Covariance', 'Beta_4Y_Covariance']\n",
    "    \n",
    "    for param in parameters:\n",
    "        print(f\"\\nRanking of sectors based on {param}:\")\n",
    "        sorted_rankings = rankings_df.sort_values(param, ascending=False)\n",
    "        for _, row in sorted_rankings.iterrows():\n",
    "            print(f\"{row['Sector']} ({row['Number_of_Companies']} companies): {row[param]:.4f}\")\n",
    "            print(f\"Date Range: {row['Date_Range']}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 5: Machine Learning Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Performing Heirarchical Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-19T11:30:41.680666Z",
     "start_time": "2024-11-19T11:30:41.139102Z"
    }
   },
   "outputs": [],
   "source": [
    "# Step 1: Read the CSV file\n",
    "df = pd.read_csv('sector_rankings.csv')\n",
    "\n",
    "# Step 2: Normalize each column independently (excluding the 'Sector' column)\n",
    "scaler = StandardScaler()\n",
    "df = df.drop(columns=['Number_of_Companies', 'Date_Range'])\n",
    "X = df.drop('Sector', axis=1)  # Keep only numerical data for normalization\n",
    "print(df.shape)\n",
    "X_scaled = X.apply(lambda col: scaler.fit_transform(col.values.reshape(-1, 1)).flatten(), axis=0)\n",
    "\n",
    "# Step 3: Perform hierarchical clustering\n",
    "Z = linkage(X_scaled, method='complete')\n",
    "\n",
    "# Step 4: Plot the dendrogram\n",
    "plt.figure(figsize=(12, 20))\n",
    "dendrogram(Z, labels=df['Sector'].values, leaf_rotation=90, leaf_font_size=10)\n",
    "plt.title('Hierarchical Clustering of Sectors')\n",
    "plt.xlabel('Sectors')\n",
    "plt.ylabel('Euclidean Distance')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using K-means seeded with the sectors with minimum distance in each cluster, we initialize the clusters and obtain expected output for each sector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-19T11:45:37.636478Z",
     "start_time": "2024-11-19T11:45:37.296392Z"
    }
   },
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")  # Ignore warnings for cleaner output\n",
    "\n",
    "# Step 1: Load and preprocess the data\n",
    "df = pd.read_csv('sector_rankings.csv')\n",
    "df = df.drop(columns=['Number_of_Companies', 'Date_Range'])  # Drop unnecessary columns\n",
    "\n",
    "# Separate 'Sector' column for labeling\n",
    "sectors = df['Sector'].values\n",
    "X = df.drop('Sector', axis=1)\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Step 2: Define seed sectors and calculate initial centroids\n",
    "seed_sectors = {\n",
    "    0: \"Consumer Staples Merchandise Retail\",\n",
    "    1: \"Health Care Services\",\n",
    "    2: \"Insurance Brokers\",\n",
    "    3: \"Movies & Entertainment\",\n",
    "    4: \"Broadcasting\"\n",
    "}\n",
    "\n",
    "# Find the rows corresponding to the seed sectors\n",
    "seed_indices = [list(sectors).index(seed) for seed in seed_sectors.values()]\n",
    "initial_centroids = X_scaled[seed_indices]\n",
    "\n",
    "# Step 3: Iteratively perform KMeans until convergence\n",
    "num_clusters = 5\n",
    "centroids = initial_centroids\n",
    "tolerance = 1e-4  # Convergence threshold\n",
    "max_iterations = 100  # Safety limit on iterations\n",
    "iteration = 0\n",
    "\n",
    "while iteration < max_iterations:\n",
    "    # Step 3a: Assign each point to the nearest centroid\n",
    "    labels, _ = pairwise_distances_argmin_min(X_scaled, centroids)\n",
    "    \n",
    "    # Step 3b: Calculate new centroids as the mean of points in each cluster\n",
    "    new_centroids = np.array([X_scaled[labels == k].mean(axis=0) for k in range(num_clusters)])\n",
    "    \n",
    "    # Step 3c: Check for convergence (if centroids do not change significantly)\n",
    "    centroid_shift = np.linalg.norm(new_centroids - centroids, axis=1).max()\n",
    "    print(f\"Iteration {iteration + 1}, centroid shift: {centroid_shift:.6f}\")\n",
    "    \n",
    "    if centroid_shift < tolerance:\n",
    "        print(\"Convergence reached.\")\n",
    "        break\n",
    "    \n",
    "    centroids = new_centroids\n",
    "    iteration += 1\n",
    "\n",
    "# Final labels after convergence\n",
    "final_labels = labels\n",
    "\n",
    "# Step 4: Reduce dimensions for plotting using PCA\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "centroids_pca = pca.transform(centroids)\n",
    "\n",
    "# Step 5: Plot the final clusters\n",
    "plt.figure(figsize=(10, 7))\n",
    "for cluster in range(num_clusters):\n",
    "    # Plot points in each cluster\n",
    "    cluster_points = X_pca[final_labels == cluster]\n",
    "    plt.scatter(cluster_points[:, 0], cluster_points[:, 1], label=f\"Cluster {cluster}\")\n",
    "\n",
    "print(cluster)\n",
    "# Plot centroids\n",
    "plt.scatter(centroids_pca[:, 0], centroids_pca[:, 1], s=200, c='black', marker='X', label='Centroids')\n",
    "\n",
    "# Add labels and title\n",
    "plt.title(\"Final Clusters after KMeans Convergence\")\n",
    "plt.xlabel(\"Principal Component 1\")\n",
    "plt.ylabel(\"Principal Component 2\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Architecture\n",
    "```python\n",
    "Sequential([\n",
    "    Dense(128, activation='relu'),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(num_clusters, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-19T13:01:13.109359Z",
     "start_time": "2024-11-19T13:01:09.627921Z"
    }
   },
   "outputs": [],
   "source": [
    "# Step 1: Load and preprocess the data\n",
    "df = pd.read_csv('sector_rankings.csv')\n",
    "df = df.drop(columns=['Number_of_Companies', 'Date_Range'])  # Drop unnecessary columns\n",
    "\n",
    "# Separate 'Sector' column for labeling and features\n",
    "sectors = df['Sector'].values\n",
    "X = df.drop('Sector', axis=1)\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Step 2: Generate cluster labels with KMeans\n",
    "num_clusters = 5\n",
    "# seed_sectors = {\n",
    "#     0: \"Internet Services & Infrastructure\",\n",
    "#     1: \"Oil & Gas Exploration & Production\",\n",
    "#     2: \"Interactive Media & Services\",\n",
    "#     3: \"Broadcasting\",\n",
    "#     4: \"Rail Transportation\"\n",
    "# }\n",
    "seed_sectors = {\n",
    "    0: \"Consumer Staples Merchandise Retail\",\n",
    "    1: \"Health Care Services\",\n",
    "    2: \"Insurance Brokers\",\n",
    "    3: \"Movies & Entertainment\",\n",
    "    4: \"Broadcasting\"\n",
    "}\n",
    "\n",
    "# Find the rows corresponding to the seed sectors\n",
    "seed_indices = [list(sectors).index(seed) for seed in seed_sectors.values()]\n",
    "initial_centroids = X_scaled[seed_indices]\n",
    "\n",
    "# Initialize and fit KMeans\n",
    "kmeans = KMeans(n_clusters=num_clusters, init=initial_centroids, n_init=1)\n",
    "kmeans.fit(X_scaled)\n",
    "labels = kmeans.labels_  # Use these labels as the target for training\n",
    "\n",
    "# Step 3: Prepare data for the neural network\n",
    "# Convert labels to categorical (one-hot encoding)\n",
    "y = to_categorical(labels, num_clusters)\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.4, random_state=42)\n",
    "\n",
    "# Step 4: Build the neural network model\n",
    "model = Sequential([\n",
    "    Dense(128, input_shape=(X_train.shape[1],), activation='relu'),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(num_clusters, activation='softmax')  # Output layer with softmax for classification\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Step 5: Train the model\n",
    "history = model.fit(X_train, y_train, epochs=100, batch_size=8, validation_split=0.3, verbose=1)\n",
    "\n",
    "# Step 6: Evaluate the model\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"Test Accuracy: {test_accuracy:.2f}\")\n",
    "\n",
    "# Predict clusters for new data points\n",
    "predictions = model.predict(X_test)\n",
    "predicted_clusters = np.argmax(predictions, axis=1)\n",
    "\n",
    "# Output some test predictions\n",
    "for i in range(len(y_test)):  # Show first 5 predictions\n",
    "    print(f\"{i}.True cluster: {np.argmax(y_test[i])}, Predicted cluster: {predicted_clusters[i]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-19T06:50:50.456101Z",
     "start_time": "2024-11-19T06:50:50.430718Z"
    }
   },
   "outputs": [],
   "source": [
    "model.save(\"sector_classification_model.keras\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "americanmarketanalysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
